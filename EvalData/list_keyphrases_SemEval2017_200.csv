id,document,keyphrases
S0031920113000708,"Geomagnetic jerks are conspicuous yet poorly understood phenomena of Earth’s magnetic field, motivating investigations of their morphology and the theory behind their origins. Jerks are most commonly defined by their observed form at a single observatory as ‘V’ shapes in a single component of the geomagnetic secular variation (SV), the first time derivative of the main magnetic field (MF). The times of the gradient changes, which separate linear trends of several years, have associated step changes in the second time derivative of the MF (secular acceleration (SA)) and impulses in the third time derivative. The ‘V’ shape SV definition of jerks includes an implicit expectation of a ‘large’ magnitude step change in the gradient without definition of this scale or its threshold value other than the basic need for it to be observable in the data above the highly variable background noise. Jerks can be described by their amplitude, that is, the difference in the gradients of the two linear SV segments about a jerk, A=a2-a1, where a2 is the gradient after the jerk and a1 is the gradient before the jerk. This measure is essentially the best fit SA change across a jerk. Jerk amplitude is thus positive for a positive step in SA and negative for a negative step. Here we do not consider spatial extent in our definition and refer to individual features in one field component of a given observatory time series as a single jerk.","['Earth’s magnetic field', 'Geomagnetic jerks', 'geomagnetic secular variation', 'gradient changes', 'jerk', 'Jerk amplitude', 'jerks', 'Jerks', '‘large’ magnitude step change', 'magnetic field', 'main magnetic field', 'MF', 'SA', 'SA change', 'secular acceleration', 'secular variation', 'SV', 'SV segments']"
S1574119215000796,"The proposed multihop routing protocol, PHASeR, applies the technique of blind forwarding in a MWSN, which increases the reliability of data delivery through its inherent use of multiple routes. This approach requires a gradient metric to be continuously maintained, which is problematic in a dynamic topology. The literature commonly uses either flooding or location awareness, however flooding creates large amounts of overhead and location determination schemes can often be inaccurate, power hungry and create the issue of the dead end problem. PHASeR uses a novel method of gradient maintenance in a mobile network, which requires the proactive sharing of only local topology information. This is facilitated by a global TDMA (time division multiple access) MAC (medium access control) layer and further reduces the amount of overhead, which in turn will decrease packet latency. PHASeR is also set apart by its use of encapsulation, which allows data from multiple nodes to be transmitted in the same packet in order to handle high volumes of traffic. It utilises node cooperation to create a robust multipath routing solution. As such, the contribution of this paper is a cross-layer routing protocol for MWSNs that can handle the constant flow of data from sensors in highly mobile situations.","['cross-layer routing protocol', 'cross-layer routing protocol for MWSNs', 'encapsulation', 'gradient maintenance', 'gradient metric', 'MAC', 'medium access control', 'mobile network', 'multihop routing protocol', 'MWSN', 'MWSNs', 'node', 'node cooperation', 'nodes', 'packet latency', 'PHASeR', 'sensors', 'TDMA', 'time division multiple access']"
S1877750311000676,"One way to enforce this ratio is to use a probabilistic, ‘roulette wheel’ style lane selection policy. VISSIM, along with most simulation toolkits, offers methods to specify probabilistic routing whereby a defined percentage of vehicles are sent down unique routes. This is a piecewise technique that can be reapplied at various locations around a simulation. While these methods are attractive from a calibration perspective as exact representations of existing statistics can be ensured, the process is an unrealistic one as it assumes that drivers make probabilistic decisions at precise locations. So in this case when a vehicle arrives at a point prior to the weighbridges it is allocated one of the lanes based on the respective probabilities. It turns out that this method leads to significant variations in trip times depending on the initial random number seed, this can be seen in a graphic of the key areas of the simulation for the 2 different runs (Fig. 7). One of the benefits of graphical microsimulation is that the 2D and 3D simulations help the researcher to visualise a new scheme and its potential benefits but also to highlight unrealistic behaviour. Fig. 7 shows the congestion at the decision point for 2 different runs. Using probabilistic routing to enforce correct routing percentages is a clear case of overcalibration affecting simulation brittleness.","['2D and 3D simulations', 'drivers', 'graphical microsimulation', 'lane selection policy', 'piecewise technique', 'probabilistic, ‘roulette wheel’ style', 'probabilistic routing', 'simulation', 'simulation toolkits', 'vehicle', 'VISSIM', 'weighbridges']"
S0165212511000874,"The propagation of unsteady disturbances in ducts of slowly-varying geometry, such as those typical of an aeroengine, can be successfully modelled using a multiple scales approach. From the first application [1] of multiple-scales analysis to sound propagation in ducts of rectangular and circular cross section without mean flow, more recent developments have extended the method to cases with uniform mean flow [2], mean swirling flow [3], ducts of arbitrary cross section [4] (with uniform mean flow) and strongly curved ducts [5]. The multiple-scales approach has a number of distinct advantages over full numerical methods as it is ideally suited to handle higher frequencies and the computational complexity is only marginally more than calculating the eigenmodes inside a straight parallel duct. The accuracy and usefulness of the multiple scales approach has been validated against finite-element methods [6] for realistic aeroengine configurations and acoustic frequencies [7,8].","['acoustic frequencies', 'aeroengine', 'aeroengine configurations', 'calculating the eigenmodes', 'ducts', 'finite-element methods', 'higher frequencies', 'mean flow', 'mean swirling flow', 'multiple-scales analysis', 'multiple scales approach', 'multiple-scales approach', 'numerical methods', 'propagation of unsteady disturbances', 'sound propagation', 'straight parallel duct', 'strongly curved ducts', 'uniform mean flow']"
S0029549314001551,"An increase of neutron leakage from the core region can be achieved through modifications in the core geometry (usually by adopting a pan-cake geometry of the active core region at the expense of the general neutron economy). Extensive studies determined a set of core design modifications that optimised the total sodium void reactivity (becoming less positive). Among the most efficient design solutions identified is an enlarged sodium plenum above the active core region in combination with an absorber layer above the sodium plenum (to reduce neutron backscattering from the reflector region above the plenum). Fig. 19 shows the combined effect of different upper plenum thicknesses of the absorber and boron layers. It can be observed that the sequential increase of the layer's thickness converge to an asymptotic value of reactivity reduction slightly over 800pcm. The pair of values selected was 60cm for the sodium plenum and 30cm for the boron layer. These modifications implied a considerable increase in the sub-assembly length that was compensated by reducing the upper axial reflector width (Sun et al., 2013).","['absorber and boron layers', 'absorber layer', 'axial reflector', 'boron layer', 'core', 'core design modifications that optimised the total sodium void reactivity', 'core region', 'design solutions', 'enlarged sodium plenum', 'increase in the sub-assembly length', ""increase of the layer's thickness"", 'modifications in the core geometry', 'neutron', 'neutron leakage', 'pan-cake geometry of the active core region', 'plenum', 'reactivity reduction', 'reduce neutron backscattering', 'reduce neutron backscattering from the reflector region above the plenum', 'reducing the upper axial reflector width', 'sodium', 'sodium plenum', 'upper plenum']"
S0370269304008974,"Thus, the extension to the charmed analogue Θc(3099) provides an interesting test for the SDO sum rule and lattice calculations [17]. Here, the charm quark is quite heavy so that the constituent-quark picture may fit well and the JW prediction for the parity is expected to be reproduced from QCD. In fact, quenched lattice calculation finds the parity of Θc(3099) to be positive [28]. In the extension to the Θc(3099) sum rules, there are two important aspects, which make this sum rule different from the SDO sum rule. First of all, since the charm quark is too heavy to form quark condensate, it gives non-perturbative effects only by radiating gluons. The quark–gluon mixed condensate 〈s̄gsσ·Gs〉, which was the important contribution in the Θ+ sum rule, is replaced by gluonic operators in the heavy quark expansion that are normally suppressed. Secondly, the charm quark mass has to be kept finite in the OPE, which can be done by using the momentum space expression for the charm-quark propagator. This is different from the light-quark sum rule where the calculation is performed in the coordinate space and all the quark propagators are obtained based on the expansion with the small quark mass. Keeping these two aspects in mind, we construct QCD sum rules for Θc(3099) and see how they are different from the Θ+(1540) sum rule.","['charm quark', 'charm quark mass has to be kept finite in the OPE', 'charm-quark propagator', 'constituent-quark picture', 'extension to the charmed analogue Θc(3099)', 'gluonic operators', 'heavy quark', 'JW prediction', 'momentum space expression', 'OPE', 'QCD', 'QCD sum rules', 'quark condensate', 'quark–gluon mixed condensate', 'quark propagators', 'quenched lattice calculation', 'radiating gluons', 'replaced by gluonic operators in the heavy quark expansion', 'SDO sum rule', 'SDO sum rule and lattice calculations', 'see how they are different from the Θ+(1540) sum rule', 's̄gsσ·Gs', 'small quark', 'sum rule', 'the light-quark sum rule', 'Θ+ sum rule']"
S0370269304009141,"Longitudinal beam and target single-spin asymmetries have been at the center of the attention lately, since they have been measured by the HERMES and CLAS experimental Collaborations [1–4] and more measurements are planned. They were originally believed to be signals of the so-called T-odd fragmentation functions [5], in particular, of the Collins function [6–12]. However, both types of asymmetry can receive contributions also from T-odd distribution functions [13–16], a fact that has often been neglected in analyses. An exhaustive treatment of the contributions of T-odd distribution functions has not been carried out completely so far, especially up to subleading order in an expansion in 1/Q, Q2 being the virtuality of the incident photon and the only hard scale of the process, and including quark mass corrections. It is the purpose of the present work to describe the longitudinal beam and target spin asymmetries in a complete way in terms of leading and subleading twist distribution and fragmentation functions. We consider both single-particle inclusive DIS, e+p→e′+h+X, and single-jet inclusive DIS, e+p→e′+jet+X. We assume factorization holds for these processes, even though at present there is no factorization proof for observables containing subleading-twist transverse-momentum dependent functions (only recently proofs for the leading-twist case have been presented in Refs. [17,18]).","['Collins function', 'describe the longitudinal beam and target spin asymmetries', 'e+p→e′+h+X', 'e+p→e′+jet+X', 'exhaustive treatment of the contributions of T-odd distribution functions', 'factorization', 'factorization proof for observables containing subleading-twist transverse-momentum dependent functions', 'fragmentation functions', 'HERMES and CLAS experimental Collaborations\xa0', 'incident photon', 'Longitudinal beam and target single-spin asymmetries', 'proofs for the leading-twist case', 'quark', 'quark mass corrections', 'receive contributions also from T-odd distribution functions', 'single-jet inclusive DIS', 'single-particle inclusive DIS', 'subleading order in an expansion in\xa01/Q', 'subleading-twist transverse-momentum dependent functions', 'T-odd distribution functions', 'T-odd fragmentation functions', 'twist distribution']"
S0021999114007876,"In this work, light propagation in a scattering medium with piece-wise constant refractive index using the radiative transport equation was studied. Light propagation in each sub-domain with a constant refractive index was modeled using the RTE and the equations were coupled using boundary conditions describing Fresnel reflection and transmission phenomenas on the interfaces between the sub-domains. The resulting coupled system of RTEs was numerically solved using the FEM. The proposed model was tested using simulations and was compared with the solution of the Monte Carlo method. The results show that the coupled RTE model describes light propagation accurately in comparison with the Monte Carlo method. In addition, results show that neglecting internal refractive index changes can lead to erroneous boundary measurements of scattered light. This indicates that the quality of the DOT reconstructions could possible be increased by incorporating a model for internal refractive index changes in the image reconstruction procedure.","['coupled system of RTEs', 'DOT reconstructions', 'FEM', 'Fresnel reflection and transmission phenomenas', 'image reconstruction procedure', 'light propagation in a scattering medium with piece-wise constant refractive index', 'Monte Carlo method', 'radiative transport equation', 'refractive index changes', 'RTE', 'simulations', 'solution of the Monte Carlo method']"
S0301932213001985,"In the current CLSVOF method, the normal vector is calculated directly by discretising the LS gradient using a finite difference scheme. By appropriately choosing one of three finite difference schemes (central, forward, or backward differencing), it has been demonstrated that thin liquid ligaments can be well resolved see Xiao (2012). Although a high order discretisation scheme (e.g. 5th order WENO) has been found necessary for LS evolution in pure LS methods to reduce mass error, low order LS discretisation schemes (2nd order is used here) can produce accurate results when the LS equation is solved and constrained as indicated above in a CLSVOF method (see Xiao, 2012), since the VOF method maintains 2nd order accuracy. This is a further reason to adopt the CLSVOF method, which has been used for all the following simulations of liquid jet primary breakup.","['5th order WENO', 'central, forward, or backward differencing', 'CLSVOF method', 'finite difference scheme', 'finite difference schemes', 'high order discretisation scheme', 'liquid', 'low order LS discretisation schemes', 'LS', 'LS methods', 'reduce mass error', 'simulations of liquid jet primary breakup', 'thin liquid ligaments', 'VOF method']"
S2212671612002302,Monitoring the wear condition of the tramway superstructure is one of the key points to guarantee an adequate safety level of the light rail transport system. The purpose of this paper is to suggest a new non-conventionalprocedure for measuring the transverse profile of rails in operation by means of image-processing technique. This methodological approach is based on the “information” contained in high-resolution photographic images of tracks and on specific algorithms which allow to obtain the exact geometric profile of the rails and therefore to measure the state of the rail-head extrados wear.,"['high-resolution photographic images', 'image-processing technique', 'information”', 'light rail transport system', 'Monitoring the wear condition', 'new non-conventionalprocedure', 'rails', 'safety level', 'specific algorithms', 'tracks', 'tramway superstructure', 'transverse profile', 'wear condition']"
S0370269304009530,"If signals suggesting supersymmetry (SUSY) are discovered at the LHC then it will be vital to measure the spins of the new particles to demonstrate that they are indeed the predicted super-partners. A method is discussed by which the spins of some of the SUSY particles can be determined. Angular distributions in sparticle decays lead to charge asymmetry in lepton-jet invariant mass distributions. The size of the asymmetry is proportional to the primary production asymmetry between squarks and anti-squarks. Monte Carlo simulations are performed for a particular mSUGRA model point at the LHC. The resultant asymmetry distributions are consistent with a spin-0 slepton and a spin-12χ˜20, but are not consistent with both particles being scalars.","['Angular distributions', 'anti-squarks', 'asymmetry distributions', 'charge asymmetry', 'demonstrate that they are indeed the predicted super-partners', 'lepton-jet invariant mass distributions', 'measure the spins of the new particles', 'Monte Carlo simulations', 'mSUGRA model point', 'particles', 'primary production asymmetry', 'sparticle decays', 'spin-0 slepton', 'squarks', 'super-partners', 'supersymmetry', 'SUSY', 'SUSY particles']"
S2212671612002351,"In this paper, a novel position estimation method of prism was proposed for single-lens stereovision system. The prism with multi faces was considered as a single optical system composed of some refractive planes. A transformation matrix which can express the relationship between an object point and its image by the refraction of prism was derived based on geometrical optics, and a mathematical model was introduced which can denote the position of prism with arbitrary faces only by 7 parameters. This model can extend the application of single-lens stereovision system using prism to a more widely area. Experimentation results are presented to prove the effectiveness and robustness of our proposed model.","['effectiveness', 'express the relationship', 'extend the application of single-lens stereovision system', 'geometrical optics', 'image', 'mathematical model', 'object point', 'position estimation method', 'prism', 'refraction of prism', 'refractive planes', 'robustness', 'single-lens stereovision system', 'single optical system', 'transformation matrix']"
S0301010415300355,"Alternatively to H-atom photodetachment from the intermediate radicals, the latter may serve as reducing agents. Evidence has been reported in recent years that the pyridinyl radical (PyH) is an exceptionally strong reducing agent which can even reduce CO2 to formaldehyde, formic acid or methanol with suitable catalyzers [27–29], albeit the mechanisms of these reactions are currently poorly understood [30–32]. The theoretically predicted dissociation thresholds of the AcH, AOH and BAH radicals are about 2.7eV, 2.5eV and 3.0eV, respectively (see Fig. 4), while the predicted dissociation threshold of the pyridinyl radical is much lower, about 1.7eV [1]. Pyridinyl is thus a significantly stronger reductant than acridinyl and related radicals. It is therefore not expected that the latter will be able to reduce carbon dioxide in dark reactions.","['AcH, AOH and BAH radicals', 'acridinyl', 'carbon dioxide', 'catalyzers', 'CO2', 'formaldehyde', 'formic acid', 'H-atom photodetachment', 'intermediate radicals', 'methanol', 'PyH', 'Pyridinyl', 'pyridinyl radical', 'radicals', 'reduce carbon dioxide in dark reactions', 'reducing agent', 'reducing agents', 'reductant']"
S0927025612000249,"The need for power generation industry to improve the thermal efficiency of power plant has led to the development of 9–12% Cr martensitic steels. The development of and research on P91 steels started since late 1970s and early 1990s, respectively [1]. The work has focussed on their creep strengths due to its intended application at high temperature. Recently, the introduction of more cyclic operation of power plant has introduced the possibility of fatigue problems. Bore cracking due to the effects of varying steam warming has been reported [2]. The temperature cycling causes thermal gradients between the inside and outside of components and this can cause cyclic stress levels to be of concerns. Recently, research on thermal–mechanical analysis of P91 has been carried out including the characterisation of the cyclic behaviour of the material using the two-layer and unified visco-plasticity models [3,4].","['9–12% Cr martensitic steels', 'characterisation of the cyclic behaviour of the material', 'development of and research on P91 steels', 'fatigue problems', 'improve the thermal efficiency of power plant', 'P91', 'P91 steels', 'power generation', 'power plant', 'steam warming', 'temperature cycling', 'the material', 'thermal–mechanical analysis of P91', 'two-layer and unified visco-plasticity models']"
S0010938X12001163,"In situ oxidation, experiments were carried out using 3mm diameter discs with one surface ground and polished to a 1μm diamond finish. The 3mm discs were then oxidised in a Philips XL-30 FEG ESEM with a hot stage attachment. The oxidising atmosphere used was laboratory air at a pressure of 266Pa. During the experiment, the sample was observed and imaged using a primary beam energy of 20kV and an Everhart–Thornley secondary electron detector. The sample was heated at a rate of 100°C/min to a temperature of 700°C and held at this temperature for 8min to stabilise the stage and the microscope. The sample was then heated to a final temperature of 900°C at the same heating rate. The total time of exposure of the sample was 120min before cooling to room temperature by turning off the heating coils. The samples were then examined in the LEO 1530VP FEGSEM with chemical information gathered using EDS. Cross-sections and Transmission Electron Microscope (TEM) samples were produced using a dual beam FEI Nova Nanolab 600 for Focused Ion Beam (FIB) milling perpendicular to the phase boundaries to determine their influence on the oxide development and imaged using a Jeol 2000FX W-filament TEM. EDS maps of the TEM samples were collected using the Nanolab 600 with a Scanning TEM (STEM) detector and an EDAX Genesis EDS system at an accelerating voltage of 30kV.","['3mm diameter discs', 'accelerating voltage of 30kV', 'dual beam FEI Nova Nanolab 600', 'EDAX Genesis EDS system', 'EDS', 'FIB', 'Focused Ion Beam', 'ground and polished', 'heated', 'heated to a final temperature of 900°C', 'heating coils', 'hot stage attachment', 'In situ oxidation', 'Jeol 2000FX W-filament TEM', 'LEO 1530VP FEGSEM', 'Nanolab 600', 'oxide', 'oxide development', 'oxidised', 'Philips XL-30 FEG ESEM', 'Scanning TEM', 'Scanning TEM (STEM) detector', 'stabilise the stage and the microscope', 'STEM', 'TEM', 'TEM samples', 'Transmission Electron Microscope']"
S0021961413004321,"The thermodynamics of copper-zinc alloys (brass) was subject of numerous investigations. Brass is characterised by an excess enthalpy and excess entropy of mixing, both of which are negative. The enthalpic data were measured by solution calorimetry e.g., [1–3] and based on chemical potential data calculated from phase equilibrium experiments e.g., [4–6], the excess entropy of mixing could be evaluated e.g., [7–9]. This excess entropy contains both, the vibrational and the configurational parts. The excess vibrational entropy, defined as the deviation from the entropy of a mechanical mixture of the end members A and B (i.e., Smmechmix=XASmA+XBSmB), can be determined by measuring the low temperature heat capacity (5 to 300K) versus composition behaviour. The determination of the excess configurational entropy, i.e., the excess entropy coming from non-random atomic distributions and defects, is much more difficult. Here, neutron scattering investigations together with computer simulations are normally used. If, however, reliable data of the total excess entropy (from enthalpic and chemical potential data) are available, the measurement of the excess vibrational entropy enables the determination of the excess configurational entropy simply by subtraction. Since configurational and vibrational entropies may have different temperature dependencies, it is worthwhile to separate the entropic effects. This is one aim of this study. Another aim is to deliver experimental data so that first principles studies can test their models on a disordered alloy, whose structural details (short-range order) depend on temperature.","['brass', 'Brass', 'chemical potential data', 'computer simulations', 'copper-zinc alloys', 'deliver experimental data', 'determination of the excess configurational entropy', 'disordered alloy', 'enthalpic and chemical potential data', 'enthalpic data', 'excess configurational entropy', 'excess enthalpy and excess entropy of mixing', 'excess entropy of mixing', 'experimental data', 'measurement of the excess vibrational entropy', 'measuring the low temperature heat capacity (5 to 300K) versus composition behaviour', 'neutron scattering investigations', 'phase equilibrium experiments', 'reliable data of the total excess entropy', 'separate the entropic effects', 'Smmechmix=XASmA+XBSmB', 'solution calorimetry', 'subtraction', 'thermodynamics of copper-zinc alloys (brass)', 'vibrational and the configurational parts']"
S002231151500032X,"Zirconium alloys are used as cladding to encapsulate fuel pellets in pressurised and boiling water nuclear reactors. Research into oxidation of these alloys has been significant since the introduction of the material. However, the microstructure and electro-chemical processes during oxidation are complex and many questions still remain unanswered. One such issue is the formation of lateral cracks near the metal-oxide interface. Small cracks have been seen to form continuously during oxidation, with large scale networks of lateral cracks forming cyclically every ∼2μm of oxide growth. These networks of cracks can be correlated with acceleration in the corrosion kinetics [1–7]. These lateral cracks might enable the link up of nano pores along grain boundaries perpendicular to the metal/oxide interface as reported in [8,9]. Experiments using Synchrotron X-Ray Diffraction (S-XRD) by both Polatidis et al. and Petigny et al., have separately shown that oxides formed on Zircaloy-4 are composed of monoclinic and stabilised tetragonal phases, with an ∼7% reduction in the tetragonal phase fraction from 1 to 3μm oxide growth [4,10]. One theory is that the lateral cracks may destabilise the tetragonal phase close to the metal-oxide interface. The phase transformation has an ∼6% expansion associated with it, which could lead to fracture perpendicular to the metal-oxide interface, thereby generating fast ingress routes for oxygen containing species [11,12].","['∼7% reduction in the tetragonal phase', 'alloys', 'cladding', 'corrosion kinetics', 'fracture', 'fuel pellets', 'grain', 'ingress', 'link up of nano pores', 'metal-oxide interface', 'metal/oxide interface', 'microstructure and electro-chemical processes', 'monoclinic and stabilised tetragonal phases', 'oxidation', 'oxidation of these alloys', 'oxides', 'oxygen containing species', 'phase transformation', 'S-XRD', 'Synchrotron X-Ray Diffraction', 'tetragonal phase', 'water nuclear reactors', 'Zircaloy-4', 'Zirconium alloys']"
S0003491615000433,"Nuclear theory devoted major efforts since 4 decades to describe thermalization in nuclear reactions, predominantly using semi-classical methods  [13,14,10], in line with similar problems in quantum liquids  [15,16]. There were attempts to develop improved molecular dynamics methods combining quantum features with a semi classical treatment of dynamical correlations  [17,18]. Still, no clear-cut quantum approach is readily available yet, in spite of numerous formal attempts [19,20,10]. The field of clusters and nano structures is far younger but fast developing in relation to the ongoing developments of lasers and imaging techniques. Semiclassical approaches were also considered in the field to include some dynamical corrections  [21,22] and could qualitatively describe dynamical processes. But such approaches are bound to simple metals with sufficiently delocalized wave functions, and thus smooth potentials justifying semiclassical approximations. The case of organic systems, in particular the much celebrated C60   [4,23], cannot be treated this way. Semi classical, and even classical approaches, can be used at very high excitations such as delivered by very intense laser pulses  [2]. In such cases the system is blown up and details of its quantum mechanical features do not matter anymore. But for less violent scenarios, quantum shell effects cannot be ignored.","['C60', 'combining quantum features', 'field of clusters and nano structures', 'imaging techniques', 'improved molecular dynamics methods', 'lasers', 'nuclear reactions', 'Nuclear theory', 'organic systems', 'qualitatively describe dynamical processes.', 'quantum approach', 'quantum liquids', 'quantum mechanical features', 'quantum shell effects', 'semi-classical methods', 'semi classical treatment of dynamical correlations', 'simple metals', 'simple metals with sufficiently delocalized wave functions', 'thermalization', 'very intense laser pulses']"
S0301010415002256,"For decades, vibronic coupling models [1–4] have served as bridges connecting nuclear dynamics studies with the static studies of electronic structure calculations [5]. The vibronic coupling model is a simple polynomial expansion of diabatic potential energy surfaces and couplings. The expansion coefficients are chosen so that the eigenvalues of the potential operator map on to the adiabatic potential surfaces. This diabatisation by ansatz circumvents many of the problems of describing non-adiabatic systems. It is also the inspiration for a diabatisation scheme that is used in modern, direct-dynamic methods that include non-adiabatic effects [6]. For a model Hamiltonian to correctly approximate the eigenvectors of the true Hamiltonian it has to span the totally symmetric irreducible representation (IrRep) of the point groups the molecule belongs to, at the appropriate symmetric geometries [7]. In recent times, many articles have demonstrated the advantages of using symmetry when constructing analytic model potentials [8–12], most often in the context of permutation-inversion groups [13].","['analytic model potentials', 'diabatic potential energy surfaces and couplings', 'diabatisation', 'diabatisation scheme', 'direct-dynamic methods', 'electronic structure calculations', 'expansion coefficients', 'irreducible representation', 'IrRep', 'model Hamiltonian', 'non-adiabatic systems', 'nuclear dynamics studies', 'permutation-inversion groups', 'polynomial expansion', 'symmetric geometries', 'true Hamiltonian', 'vibronic coupling model', 'vibronic coupling models']"
S0009261415000974,"Within the range of temperatures chosen, alanine dipeptide exhibits very simple behaviour. This result is due to the relatively small number of physically relevant minima (seven were characterised using this force field and solvent model) and the larger potential energy spacing between the global minimum and higher energy minima. Indeed, cross-overs in the approximate global free energy minimum for this system (where the free energy of the second-lowest potential energy minimum becomes lower than that of the global potential energy minimum) in the harmonic approximation would occur at 1170K. In general, the harmonic prediction for the crossover temperature between two minima is(4)kBTxo=V1−V2ln((o2ν¯2κ)/(o1ν¯1κ)),from Eq. (3), which clearly illustrates the balance between potential energy and well entropy.","['alanine dipeptide', 'balance between potential energy and well entropy', 'cross-overs in the approximate global free energy', 'force field and solvent model', 'larger potential energy spacing', 'physically relevant minima', 'range of temperatures chosen']"
S1687850714000405,"Xylanases have potential applications in various fields. Some of the important applications are as fallows. Xylanases are used as bleaching agent in the pulp and paper industry. Mostly they are used to hydrolyzed the xylan component from wood which facilitate in removal of lignin (Viikari, Kantelinen, Buchert, & Puls, 1994). It also helps in brightening of the pulp to avoid the chlorine free bleaching operations (Paice, Jurasek, Ho, Bourbonnais, & Archibald, 1989). In bakeries the xylanase act on the gluten fraction of the dough and help in the even redistribution of the water content of the bread (Wong & Saddler, 1992). Xylanases also have potential application in animal feed industry. They are used for the hydrolysis of non-starchy polysaccharides such as arabinoxylan in monogastric diets (Walsh, Power, & Headon, 1993). Xylanases also play a key role in the maceration of vegetable matter (Beck & Scoot, 1974), protoplastation of plant cells, clarification of juices and wine (Biely, 1985) liquefaction of coffee mucilage for making liquid coffee, recovery of oil from subterranian mines, extraction of flavors and pigments, plant oils and starch (McCleary, 1986) and to improve the efficiency of agricultural silage production (Wong & Saddler, 1992).","['animal feed industry', 'arabinoxylan', 'bleaching agent', 'brightening of the pulp', 'chlorine free bleaching operations', 'clarification of juices and wine', 'efficiency of agricultural silage production', 'even redistribution of the water content of the bread', 'extraction of flavors and pigments, plant oils and starch', 'hydrolysis of non-starchy polysaccharides', 'hydrolyzed the xylan component', 'lignin', 'liquefaction of coffee mucilage', 'maceration of vegetable matte', 'making liquid coffee', 'non-starchy polysaccharides', 'protoplastation of plant cells', 'pulp', 'pulp and paper industry', 'recovery of oil from subterranian mines', 'removal of lignin', 'wood', 'xylanase', 'Xylanases', 'xylan component']"
S0021999113002945,"Inequality (22) indicates that the maximum-norm is the loosest among all p-norms. Fortunately, this loosest constraint would not seriously affect the accuracy since the value of ||y||∞ is comparable to that of the 2-norm and 1-norm. The maximum-norm provides us with the largest number of possible solutions under a given error limitation [24]. This would greatly enhance the possibility of finding a group of optimized coefficients when scanning a vast solution set. On the other hand, checking the maximum deviation sounds more reasonable than checking the “distance” between the accurate and approximated wave numbers since it is not working in the space domain. Therefore, we chose the maximum-norm as our criterion for designing the objective functions to extend the accurate wave number coverage as widely as possible.","['1-norm', '2-norm', 'extend the accurate wave number coverage', 'Inequality', 'maximum deviation', 'maximum-norm', 'objective functions', 'optimized coefficients', 'p-norms', 'vast solution set', '||y||∞']"
S0167931713002438,"There have been suggestions that electrons can be trapped in the bulk and at surfaces of silica [15] but new models of electron trapping centres started to appear only recently. It has been suggested by Bersuker et al., who used molecular models, that electrons can be trapped by Si–O bonds in a-SiO2 leading to their weakening and thus facilitating Si–O bond dissociation [16]. Further calculations by Camellone et al. have shown that electrons can spontaneously trap in non-defective continuum random network model of a-SiO2 [17]. Recent calculations have also demonstrated that the two dominant neutral paramagnetic defects at surfaces of a-SiO2, the non-bridging oxygen centre and the silicon dangling bond, are deep electron traps and can form the corresponding negatively charged defects [18]. However, these theoretical predictions have not yet been confirmed experimentally, emphasising the challenges for identifying defect centres.","['a-SiO2', 'a-SiO2 leading', 'bulk', 'deep electron traps', 'electrons', 'electron trapping centres', 'identifying defect centres', 'molecular models', 'negatively charged defects', 'neutral paramagnetic defects', 'non-bridging oxygen centre', 'non-defective continuum random network model', 'silica', 'silicon dangling bond', 'Si–O bond', 'Si–O bond dissociation', 'Si–O bonds', 'spontaneously trap', 'surfaces of a-SiO2', 'trapped', 'trapped in the bulk and at surfaces of silica', 'weakening']"
S0957417416302561,"•More efforts should be directed towards advancing the methods of feature extraction to overcome the influence of dynamic factors that limit the performance. The use of advanced machine learning methods such as deep neural networks and muscles synergies extraction should also be investigated on problems under the influence of multiple dynamic factors as such methods may provide substantial improvements upon the utilized time-and-frequency EMG feature extraction methods (Diener, Janke, & Schultz, 2015; Ison, Vujaklija, Whitsell, Farina, & Artemiadis, 2016; Park & Lee, 2016). Meanwhile, we showed that the performance of the learning algorithms can be improved by using feature extraction methods that rely on the angular information of muscle activation patterns. Features such as the TD-PSD and the DFT proved more successful than others in reducing the impact of the two dynamic factors that we considered in this paper. Such features can be readily implemented into a prosthesis controller for real-time control, especially that the EMG pattern recognition systems are nowadays becoming available for clinical testing, e.g. the COAPT complete control system (Kuiken et al., 2014)11https://www.coaptengineering.com/.","['advanced machine learning methods', 'clinical testing', 'COAPT complete control system', 'deep neural networks', 'DFT', 'EMG pattern recognition systems', 'feature extraction', 'feature extraction methods', 'Features', 'muscles synergies extraction', 'performance', 'prosthesis controller', 'real-time control', 'reducing the impact of the two dynamic factors', 'TD-PSD', 'utilized time-and-frequency EMG feature extraction methods']"
S0098300413002720,"Artificial Neural Networks (ANN) have been widely used in science and engineering problems. They attempt to model the ability of biological nervous systems to recognize patterns and objects. ANN basic architecture consists of networks of primitive functions capable of receiving multiple weighted inputs that are evaluated in terms of their success at discriminating the classes in Τa. Different types of primitive functions and network configurations result in varying models (Hastie et al., 2009; Rojas, 1996). During training network connection weights are adjusted if the separation of inputs and predefined classes incurs an error. Convergence proceeds until the reduction in error between iterations reaches a decay threshold (Kotsiantis, 2007; Rojas, 1996). We use feed-forward networks with a single hidden layer of nodes, a so called Multi-Layer Perceptron (MLP) (Venables and Ripley, 2002), and select one of two possible parameters: size, the number nodes in the hidden layer.","['adjusted if the separation of inputs and predefined classes incurs an error', 'ANN', 'ANN basic architecture', 'Artificial Neural Networks', 'Convergence', 'Different types of primitive functions and network configurations', 'feed-forward networks with a single hidden layer of nodes', 'MLP', 'model the ability of biological nervous systems to recognize patterns and objects', 'Multi-Layer Perceptron', 'networks of primitive functions', 'receiving multiple weighted inputs', 'science and engineering problems', 'select one of two possible parameters', 'the reduction in error between iterations reaches a decay threshold', 'training network connection', 'weights']"
S0370269304009049,"One of the challenges in quantum chromodynamics (QCD) is the relativistic bound state problem. In the light-cone Hamiltonian approach [1] light-cone wave functions can be constructed in a boost invariant way. It is necessary to have reliable light-cone wave functions if one wants to calculate high energy scattering, especially exclusive reactions. Many parametrizations assume separability of the dependence on the longitudinal momentum fraction and transverse momentum which is very unlikely since the two momenta are coupled in the kinetic energy operator. Various approaches have been tried to compute such wave functions. One can use the usual equal time Hamiltonian [2] and transform the resulting wave functions into light-cone form with the help of kinematical on-shell equations. The light-cone Hamiltonian in a string picture is formulated in Ref. [3]. More ambitious is the construction of an effective Hamiltonian including the gauge degrees of freedom explicitly and then solving the bound state problem. For mesons this approach [4,5] still needs many parameters to be fixed. Attempts have been made to solve the valence quark wave function for mesons in a simple Hamiltonian with a two-body potential [6].","['calculate high energy scattering', 'construction of an effective Hamiltonian including the gauge degrees of freedom explicitly', 'equal time Hamiltonian', 'Hamiltonian', 'kinematical on-shell equations', 'light-cone Hamiltonian approach', 'light-cone wave functions', 'mesons', 'QCD', 'quantum chromodynamics', 'reactions', 'relativistic bound state problem', 'reliable light-cone wave functions', 'simple Hamiltonian with a two-body potential\xa0', 'solve the valence quark wave function for mesons', 'solving the bound state problem', 'transform the resulting wave functions into light-cone form', 'valence quark wave function']"
S1361841516300342,"Probabilistic and stochastic approaches can facilitate the search for local and global optima. Evolutionary algorithms, such as genetic population (Jomier et al., 2006; Rivest-Henault et al., 2012; Ruijters et al., 2009), are considered as a strategy that is “less likely to get stuck in a local optimum” (Ruijters et al., 2009). A cost function consisting of the “sum of the Gaussian-blurred intensity values in the [DSA] at the projected model points” (Jomier et al., 2006) is optimized using a genetic algorithm optimizer. Other authors “use the Condensation form of sequential Monte Carlo sampling to estimate a cost function gradient” (Florin et al., 2005) for finding the global minimum. Besides, the Kalman filter is successfully adopted (Curwen et al., 1994; Feldmar et al., 1997; Toledo et al., 1998).","['Condensation form of sequential Monte Carlo sampling', 'cost function', 'estimate a cost function gradien', 'Evolutionary algorithms', 'genetic algorithm optimizer', 'genetic population', 'Kalman filter', 'Probabilistic and stochastic approaches', 'search for local and global optima']"
S0079642514000887,"The exquisite manipulation and exact measurement of properties of individual nanomaterials, compared with notable progress in their preparation, have not been thoroughly addressed albeit being of prime importance for the sustained development of new devices [58–61]. To date, several instruments have been designed for such goals, namely, scanning electron microscopes (SEM), atomic force microscopes (AFM) and transmission electron microscopes (TEM) [62,63]. Compared with the first two setups, which have no direct access to the material internal structure and atomic bonding information [64–67], the state-of-the-art in situ high-resolution TEM technique allows one to not only manipulate with an individual object at the nano-scale precision but to also get deep insights into its physical, chemical, and microstructural statuses [68–71]. Combining the capabilities of a conventional high-resolution TEM and AFM or STM probes produces advanced and dedicated TEM holders, which are becoming the powerful tools in nanomaterials manipulation and properties analysis. Such holders have been commercialized, for instance, by “Nanofactory Instruments AB’’, Goteborg, Sweden [72]. The full usefulness of these advanced in-situ TEM techniques is apparent with respect to mechanical and thermal property analysis of individual nanostructures, e.g., elasticity, plasticity and strength data while employing direct bent or tensile tests [73–75], probing electrical characteristics, e.g., field emission [27,76,77], electrical transport tracing [78–80], soldering [81,82], and doping [83], etc.","['AFM', 'atomic bonding information', 'atomic force microscopes', 'direct bent or tensile tests', 'doping', 'electrical transport tracing', 'exact measurement of properties', 'field emission', 'holders', 'instruments', 'manipulate with an individual object', 'material internal structure', 'mechanical and thermal property analysis', 'nanomaterials', 'nanomaterials manipulation', 'nanostructures', 'probing electrical characteristics', 'properties analysis', 'scanning electron microscopes', 'SEM', 'soldering', 'STM probes', 'sustained development of new devices', 'TEM', 'TEM holders', 'TEM techniques', 'transmission electron microscopes']"
S2212667814000951,"Design semantics is an integration of human mode of existence and view on culture and art, which means it is a unity of art and science. Design semantics is the annotation of form and the reflection of its symbolic meaning, which means it is an explanation of the deposited human cultural spirit. Chinese art stresses Expression, Force and Qi. In China, people advocate “to learn from nature”, “to look up to observe the sun, the moon and stars, and look down to observe the surroundings”, and take “Nature and Man in One” as the highest state of spirit. Design semantics is expressed in space environment design through a symbiotic philosophical view that natural and artificial forms are complementary and interactive. This form of design leads humans back to a better state of living, i.e. Nature and Man in One.","['annotation of form and the reflection of its symbolic meaning', 'Design semantics', 'explanation of the deposited human cultural spirit', 'integration of human mode of existence and view on culture and art', 'natural and artificial forms', 'observe the surroundings', 'space environment design', 'to learn from nature', 'unity of art and science']"
S0370269302013412,"We consider cosmological consequences of a conformal-invariant formulation of Einstein's General Relativity where instead of the scale factor of the spatial metrics in the action functional a massless scalar (dilaton) field occurs which scales all masses including the Planck mass. Instead of the expansion of the universe we obtain the Hoyle–Narlikar type of mass evolution, where the temperature history of the universe is replaced by the mass history. We show that this conformal-invariant cosmological model gives a satisfactory description of the new supernova Ia data for the effective magnitude–redshift relation without a cosmological constant and make a prediction for the high-redshift behavior which deviates from that of standard cosmology for z>1.7.We consider cosmological consequences of a conformal-invariant formulation of Einstein's General Relativity where instead of the scale factor of the spatial metrics in the action functional a massless scalar (dilaton) field occurs which scales all masses including the Planck mass. Instead of the expansion of the universe we obtain the Hoyle–Narlikar type of mass evolution, where the temperature history of the universe is replaced by the mass history. We show that this conformal-invariant cosmological model gives a satisfactory description of the new supernova Ia data for the effective magnitude–redshift relation without a cosmological constant and make a prediction for the high-redshift behavior which deviates from that of standard cosmology for z>1.7.","['conformal-invariant cosmological model', ""conformal-invariant formulation of Einstein's General Relativity"", ""consider cosmological consequences of a conformal-invariant formulation of Einstein's General Relativity"", 'cosmological constant', 'dilaton', 'effective magnitude–redshift relation', ""Einstein's General Relativity"", 'expansion of the universe', 'high-redshift behavior', 'Hoyle–Narlikar type of mass evolution', 'masses', 'mass history', 'massless scalar', 'massless scalar (dilaton) field', 'Planck mass', 'scale factor', 'standard cosmology', 'supernova Ia data', 'temperature history of the universe']"
S0927025614006181,"The Discrete Element Method applied to spheres is well established as a reasonably realistic tool, in a wide range of engineering disciplines, for modelling packing and flow of granular materials; Asmar et al. [8] describes the fundamentals of this method as applied by code developed in-house at Nottingham; since these are widely documented the details are not reproduced here, simply a summary. It applies an explicit time stepping approach to numerically integrate the translational and rotational motion of each particle from the resulting forces and moments acting on them at each timestep. The inter-particle and particle wall contacts are modelled using the linear spring–dashpot–slider analogy. Contact forces are modelled in the normal and tangential directions with respect to the line connecting the particles centres. Particle elastic stiffness is set so sphere “overlap” is not significant and moderate contact damping is applied. Particle cohesion can also be modelled but is assumed to be negligible in the current study. The translational and rotational motion of each particle is modelled using a half step leap-frog Verlet numerical integration scheme to update particle positions and velocities. Near-neighbour lists are used to increase the computational efficiency of determining particle contacts and a zoning method is used each time the list is composed; that is the system is divided into cubic regions, each particle centre is within one zone, and potential contacting particles are within the same or next-door neighbour zones. Full details are given in Asmar et al. [8].","['contact damping', 'Contact forces', 'contacting particles', 'Discrete Element Method', 'explicit time stepping approach', 'forces', 'granular materials', 'half step leap-frog Verlet numerical integration scheme', 'inter-particle and particle wall contacts', 'linear spring–dashpot–slider analogy', 'modelling packing and flow of granular materials', 'moments', 'Near-neighbour lists', 'numerically integrate', 'particle', 'particle centre', 'Particle cohesion', 'particle contacts', 'Particle elastic', 'Particle elastic stiffness', 'particle positions', 'particles centres', 'sphere', 'sphere “overlap”', 'spheres', 'translational and rotational motion', 'velocities', 'zoning method']"
S136481521630305X,"Using measured data from two arable sites in the UK we have shown that lags can have significant impact on model evaluation and can affect both the level of correlation between measured and simulated data and the magnitude of the sums of the residuals. Also, we used the division of MSE to three constituent statistics (SB, SDSD and LCS) to show how the level of correlation can affect the sum of residuals. By dividing the algorithm-predicted series of lag values into monthly sets and examining the frequency distribution of the lags, certain patterns in these temporally patchy series have been identified. A challenging task in relation to time lags between observed and simulated daily data, is to determine their cause. This task becomes more difficult for model outputs such as soil N2O emissions that are driven by various interacting variables. Even more so, because the measured N2O datasets and the measured datasets of their drivers (e.g. soil moisture, soil N content) cover small time periods, they are not continuous and can vary widely in size. In this study we implemented the algorithm using measured and simulated data for soil moisture (first and second example) and soil mineral N (second example), and compared its results with the respective results for N2O. In our first example, we showed that the estimated lags in N2O prediction are related to the lags in soil moisture prediction in a way that changes gradually through time. In our second example, the lags in N2O prediction were explained by the lags in soil moisture and soil mineral N prediction, with which they had a positive relationship.","['constituent statistics', 'LCS', 'measured and simulated data for soil moisture', 'model evaluation', 'MSE', 'N2O', 'N2O datasets', 'N2O emissions', 'N2O prediction', 'SB', 'SDSD', 'soil mineral N', 'soil mineral N prediction', 'soil moisture', 'soil moisture prediction']"
S0009261415008362,"For any quantum dynamical method, existing or emerging, reliable benchmarks are required to assess their accuracy. A model Hamiltonian exhibiting tunnelling dynamics through a multidimensional asymmetric double well potential has been used as a test by the MP/SOFT [18] and CCS methods [19] mentioned above, and also more recently by a configuration interaction (CI) expansion method [20] and two-layer version of CCS (2L-CCS). [21] The Hamiltonian consists of a 1-dimensional tunnelling mode coupled to an (M−1)-dimensional harmonic bath, hence it is a system-bath problem which bears some similarity to the Caldeira-Leggett model of tunnelling in a dissipative system [22,23]. This Hamiltonian is non-dissipative, however and the harmonic modes all have the same frequency. System-bath models play an important role in physics, being used to describe superconductivity at a Josephson junction in a superconducting quantum interface device (SQUID) [24], for which the Caldeira-Leggett model provides a theoretical basis, and magnetic and conductance phenomena in the spin-bath regime [25].","['1-dimensional tunnelling mode', '2L-CCS', 'A model Hamiltonian', 'benchmarks', 'Caldeira-Leggett model', 'Caldeira-Leggett model of tunnelling', 'CCS methods', 'CI', 'configuration interaction', 'configuration interaction (CI) expansion method', 'Hamiltonian', 'harmonic modes', '(M−1)-dimensional harmonic bath', 'magnetic and conductance phenomena', 'MP/SOFT', 'multidimensional asymmetric double well potential', 'physics', 'quantum dynamical method', 'spin-bath regime', 'SQUID', 'superconducting quantum interface device', 'superconductivity at a Josephson junction', 'System-bath models', 'system-bath problem', 'The Hamiltonian', 'tunnelling dynamics', 'two-layer version of CCS']"
S0370269304009335,"In these frameworks, however, the physical spacetime dimension is an input rather than a prediction of the theory. In fact, in standard theories whose gravitational sector is described by the Einstein–Hilbert action, there is no obstruction to perform dimensional reductions to spacetimes of dimensions d≠4. Then the question arises, since eleven-dimensional Minkowski space is a maximally (super)symmetric state, and the theory is well-behaved around it, why the theory does not select this configuration as the vacuum, but instead, it chooses a particular compactified space with less symmetry. An ideal situation, instead, would be that the eleven-dimensional theory dynamically predicted a low energy regime which could only be a four-dimensional effective theory. In such a scenario, a background solution with an effective spacetime dimension d>4 should be expected to be a false vacuum where the propagators for the dynamical fields are ill-defined, lest a low energy effective theory could exist in dimensions higher than four.","['background solution', 'compactified space', 'd>4', 'dimensional reductions', 'effective spacetime dimension', 'Einstein–Hilbert action', 'eleven-dimensional theory', 'false vacuum', 'four-dimensional effective theory', 'low energy effective theory', 'low energy regime', 'Minkowski space', 'physical spacetime dimension', 'prediction of the theory', 'propagators for the dynamical fields', 'select this configuration', 'standard theories', '(super)symmetric state']"
S0021999115003939,"The extrapolation of the upwind value required for TVD differencing is a particular hurdle for the application on unstructured meshes. As discussed in Section 3.2, two methods to extrapolate the value at the virtual upwind node, using data readily available on unstructured meshes, are considered. Given how the virtual upwind node is incorporated in the gradient ratio rf, the extrapolation method of Darwish and Moukalled [13] is referred to as implicit extrapolation and the method introduced by Ubbink and Issa [12] as explicit extrapolation. Both methods precisely reconstruct the upwind value for equidistant, rectilinear meshes but fail to do so on non-equidistant or non-rectilinear meshes, as discussed in Section 3.2. Using the explicit extrapolation method this issue can be rectified by imposing appropriate limits on the extrapolated upwind value.","['Darwish and Moukalled', 'data readily available on unstructured meshes', 'equidistant, rectilinear meshes', 'explicit extrapolation', 'extrapolation of the upwind value', 'implicit extrapolation', 'method introduced by Ubbink and Issa', 'non-equidistant', 'non-rectilinear meshes', 'TVD differencing', 'unstructured meshes', 'virtual upwind node is incorporated in the gradient ratio rf']"
S0167931714004456,"PDMS (Polydimethylsiloxane) has become by far the most popular material in the academic microfluidics community because it is inexpensive, easy to fabricate by replication of molds made using rapid prototyping or other techniques, flexible, optically transparent, biocompatible and its fabrication does not require high capital investment and cleanroom conditions. Various techniques have been adapted to fabricate microfluidic structures in PDMS, including wet and dry etching [20–22], photolithographic patterning of a photosensitive PDMS [23], and laser ablation [24]. But, it was the “soft-lithography” techniques [25] introduced by Whitesides et al. that enabled the widespread use of PDMS and opened up the era of PDMS-based microfluidics in the late 1990s. Replica molding, which is the casting of prepolymer against a master and generating a replica of the master in PDMS, has become a standard fabrication technique available in almost every research laboratory. Detailed overviews of soft-lithography techniques and their applications can be found from the reviews by McDonald et al. [26] and Sia et al. [27]. Nowadays, many tools dedicated for this purpose are available and can be purchased as a complete set (e.g. SoftLithoBox® provided by Elveflow (USA) [28]). Moreover, companies, such as FlowJEM (Canada) [29], Microfluidic Innovations (USA) [30], and Scientific Device Laboratory (USA) [31] provide rapid prototyping service for PDMS-based LOC devices.","['casting of prepolymer against a master and generating a replica of the master in PDMS', 'fabricate', 'fabricate microfluidic structures', 'fabrication technique', 'laser ablation', 'microfluidic', 'microfluidics', 'PDMS', 'PDMS-based LOC devices', 'PDMS-based microfluidics', 'photolithographic patterning', 'Polydimethylsiloxane', 'prepolymer', 'rapid prototyping', 'rapid prototyping service', 'Replica molding', 'replication of molds', 'SoftLithoBox®', '“soft-lithography” techniques', 'soft-lithography techniques', 'wet and dry etching']"
S002199911300346X,"Contact methods have been developed and used in Lagrangian staggered-grid hydrodynamic (SGH) calculations for many years. Early examples of contact methods are discussed in Wilkins [37] and Cherry et al. [7]. Hallquist et al. [17] provides an overview of multiple contact algorithms used in various Lagrangian SGH codes dating back to HEMP [37]. Of particular interest, Hallquist et al. [17] describes the contact surface scheme used in TOODY [31] and later implemented in DYNA2D [36]. The contact method of TOODY uses a master–slave approach. The goal of this approach is to treat the nodes on the contact surface in a manner similar to an internal node. The physical properties of the slave surface are interpolated to a ghost mesh (termed phony elements in [17]) that overlays the slave zones. The physical properties are interpolated from the slave surface to the ghost zones using surface area weights. The surface area weights are equal to the ratio of the ghost zone surface area to the surface area of the master surface. The contact surface method for nodal-based Lagrangian cell-centered hydrodynamics (CCH) presented in this paper will use surface area weights similar in concept to those in TOODY. Following the area fraction approach of TOODY may seem retrospective; however, using surface area weights naturally extends to the new CCH methods that solve a Riemann-like problem at the node of a zone [10,24,25,3].","['area fraction approach', 'CCH', 'CCH methods', 'cell-centered hydrodynamics', 'contact algorithms', 'contact method', 'contact methods', 'Contact methods', 'contact surface', 'contact surface method', 'contact surface scheme', 'DYNA2D', 'ghost mesh', 'ghost zones', 'ghost zone surface area', 'HEMP', 'internal node', 'Lagrangian SGH', 'master–slave approach', 'master surface', 'nodal-based Lagrangian cell-centered hydrodynamics', 'nodes', 'phony elements', 'SGH', 'slave surface', 'slave zones', 'solve a Riemann-like problem at the node of a zone', 'staggered-grid hydrodynamic', 'surface area', 'TOODY']"
S0168365913008766,"Immunopotentiators activate innate immunity directly (for example, cytokines) or through pattern-recognition receptors (PRRs, such as those for bacterial components). The Toll-like receptors (TLRs) are a family of PRRs that are an important link between innate and adaptive immunity. Some studies have shown that TLR ligands have adjuvant activity and enhance antigen-specific antibody and cell-mediated immune responses, especially when they are combined with delivery systems that promote their uptake and delivery into antigen-presenting cells [22–24]. For clinical studies, TLR9 is generally stimulated with synthetic oligodeoxynucleotides containing one or more unmethylated CpG dinucleotides. In humans, CpG has been used as an adjuvant for infectious disease vaccination [25,26] and in the development of cancer therapy [27]. In a mouse model, CpG has also been shown to induce T helper 1 (Th1) immune responses, which are characterized by the production of IFN-γ and the generation of IgG2a [28,29]. Moreover, a previous study had demonstrated that different liposomes with CpG ODN significantly increased Th1-biased cytokines and augmented cell mediated immune response [30].","['activate innate immunity', 'adjuvant', 'antigen-presenting cells', 'augmented cell mediated immune response', 'clinical studies', 'CpG', 'e development of cancer therapy', 'Immunopotentiators', 'induce T helper 1', 'infectious disease vaccination', 'liposomes', 'mouse model', 'pattern-recognition receptors', 'production of IFN-γ and the generation of IgG2a', 'PRRs', 'stimulated', 'synthetic oligodeoxynucleotides', 'Th1-biased cytokines', 'TLR9', 'TLR ligands', 'TLRs', 'Toll-like receptors', 'unmethylated CpG dinucleotides']"
S088523081530036X,"Note that the presented architecture works at the frame level, meaning that each single frame (plus its corresponding context) is fed-forward through the network, obtaining a class posterior probability for all of the target languages. This fact makes the DNNs particularly suitable for real-time applications because, unlike other approaches (i.e. i-vectors), we can potentially make a decision about the language at each new frame. Indeed, at each frame, we can combine the evidence from past frames to get a single similarity score between the test utterance and the targetlanguages. A simple way of doing this combination is to assume that frames are independent and multiply the posterior estimates of the last layer. The score sl for language l of a given test utterance is computed by multiplying the output probabilities pl obtained for all of its frames; or equivalently, accumulating the logs as:(6)sl=1N∑t=1Nlogp(Ll|xt​, θ)where p(Ll|xt​, θ) represents the class probability output for the language l corresponding to the input example at time t, xt by using the DNN defined by parameters θ.","['accumulating the logs as:(6)sl=1N∑t=1Nlogp(Ll|xt\u200b,\u2009θ)', 'assume that frames are independent and multiply the posterior estimates of the last layer', 'combine the evidence from past frames', 'DNN', 'DNNs', 'evidence from past frames', 'i-vectors', 'multiplying the output probabilities pl obtained for all of its frames', 'other approaches', 'target languages', 'test utterance']"
S0003491615001955,"A fluctuating vacuum is a general feature of quantum fields, of which the free Maxwell field considered in  [1–12] is but one example. Fermionic fields such as that describing the electron, also undergo vacuum fluctuations, consequently one expects to find Casimir effects associated with such fields whenever they are confined in some way. Such effects were first investigated in the context of nuclear physics, within the so-called “MIT bag model” of the nucleon  [13]. In the bag-model one envisages the nucleon as a collection of fermionic fields describing confined quarks. These quarks are subject to a boundary condition at the surface of the ‘bag’ that represents the nucleon’s surface. Just as in the electromagnetic case, the bag boundary condition modifies the vacuum fluctuations of the field, which results in the appearance of a Casimir force  [14–18]. This force, although very weak at a macroscopic scale, can be significant on the small length scales encountered in nuclear physics. It therefore has important consequences for the physics of the bag-model nucleon  [19].","['a collection of fermionic fields describing confined quarks', 'bag-model nucleon', 'Casimir effects', 'Casimir force', 'Fermionic fields', 'fluctuating vacuum', 'free Maxwell field', '“MIT bag model” of the nucleon', 'nuclear physics', 'nucleon', 'quantum fields', 'such fields', 'the bag boundary condition modifies the vacuum fluctuations of the field', 'undergo vacuum fluctuations']"
S0021999115007238,"An inherent problem of the phase-space discretisation is the spurious separation of energy into the discretised bins. This is called the “Garden Sprinkler Effect” and has been extensively studied in [48,49,20]. (In the Boltzmann transport community this is known as the ray effect.) To showcase this effect in the angular dimension, a large spatial domain (4000km×4000km) is simulated, with a monochromatic wave propagating over a long distance in deep water (d=10000m). For the spatial discretisation a structured triangle mesh is used, with an element edge length of 67km (Fig. 11(a)). The initial wave field, located 500km from the lower and left side has a Gaussian distribution in space, with a significant wave height of Hs=2.5m and a standard deviation of 150km (Fig. 11(b)). Its mean direction is 30° with an angular distribution of cos2⁡(θ) and a frequency of 0.1Hz. The simulation is time-dependent and runs for 5 days with a time-step of 600s.","['deep water', 'Garden Sprinkler Effect', 'Gaussian distribution in space', 'monochromatic wave', 'phase-space discretisation', 'ray effect', 'simulation', 'spatial discretisation', 'spatial domain (4000km×4000km) is simulated', 'spurious separation of energy into the discretised bins', 'structured triangle mesh', 'wave', 'wave field']"
S0305440314001927,"Traditionally, archaeologists have recorded sites and artefacts via a combination of ordinary still photographs, 2D line drawings and occasional cross-sections. Given these constraints, the attractions of 3D models have been obvious for some time, with digital photogrammetry and laser scanners offering two well-known methods for data capture at close range (e.g. Bates et al., 2010; Hess and Robson, 2010). The highest specification laser scanners still boast better positional accuracy and greater true colour fidelity than SfM–MVS methods (James and Robson, 2012), but the latter produce very good quality models nonetheless and have many unique selling points. Unlike traditional digital photogrammetry, little or no prior control of camera position is necessary, and unlike laser scanning, no major equipment costs or setup are involved. However, the key attraction of SfM–MVS is that the required input can be taken by anyone with a digital camera and modest prior training about the required number and overlap of photographs. A whole series of traditional bottlenecks are thereby removed from the recording process and large numbers of archaeological landscapes, sites or artefacts can now be captured rapidly, in the field, in the laboratory or in the museum. Fig. 2a–c shows examples of terracotta warrior models for which the level of surface detail is considerable.","['2D line drawings', '3D models', 'archaeological landscapes', 'artefacts', 'control of camera position', 'data capture at close range', 'digital camera', 'digital photogrammetry', 'laser scanners', 'laser scanning', 'occasional cross-sections', 'ordinary still photographs', 'prior training about the required number and overlap of photographs', 'recorded sites and artefacts', 'recording process', 'SfM–MVS', 'SfM–MVS methods', 'sites', 'terracotta warrior models', 'traditional digital photogrammetry']"
S0038092X15001024,"The wind speed and cloud height Markov chains are produced accounting for seasonal variations. A Markov chain is used for each variable representing each of the four seasons, capturing the variability at different times of the year, totalling four chains each. The okta number Markov chains also consider the effect of season, with the inclusion of impacts from pressure and diurnal variation. Eight okta Markov chains are produced that are split by above and below average pressure for each season, and four additional morning okta Markov chains are produced to capture the diurnal variation for okta transitions between 00:00 and 05:00am for each season. The intent is to capture the variation in transition probability that occurs as a result of weather changes due to the presence of solar energy. 5am is considered the cut-off because it is a typical sunrise in the summer for the applied study locations. 5h represents 5 okta transitions and is considered an appropriate duration for the slight propensity to shift towards an increased okta to be represented, Fig. 8 demonstrates the diurnal transition differences. Fig. 2 visually demonstrates the mean okta Markov chain for the entire year, whilst the effect of season can be seen in Fig. 11.","['capture the variation in transition probability', 'Markov chain', 'okta Markov chain', 'okta Markov chains', 'okta number Markov chains', 'wind speed and cloud height Markov chains']"
S0927025614003322,"A principle of high-throughput materials science is that one does not know a priori where the value of the data lies for any specific application. Trends and insights are deduced a posteriori. This requires efficient interfaces to interrogate available data on various levels. We have developed a simple WEB-based API to greatly improve the accessibility and utility of the AFLOWLIB database [14] to the scientific community. Through it, the client can access calculated physical properties (thermodynamic, crystallographic, or mechanical properties), as well as simulation provenance and runtime properties of the included systems. The data may be used directly (e.g., to browse a class of materials with a desired property) or integrated into higher level work-flows. The interface also allows for the sharing of updates of data used in previous published works, e.g., previously calculated alloy phase diagrams [19–31], thus the database can be expanded systematically.","['AFLOWLIB database', 'alloy phase diagrams', 'browse a class of materials with a desired property', 'efficient interfaces to interrogate available data on various levels', 'higher level work-flows', 'high-throughput materials science', 'improve the accessibility and utility of the AFLOWLIB database', 'interface', 'physical properties', 'runtime properties', 'sharing of updates of data', 'simulation provenance', 'thermodynamic, crystallographic, or mechanical properties', 'WEB-based API']"
S2212667814000069,"In the present paper, a hypergraph model for the structural system modeling and reconfigurability analysis has been presented. At first, we represent each system equation by a hyperedge, and then we extend the modeling hypergraph with others colored hyperedges (red and blue) which allows us to perform the analysis task. Based on the bottom up analysis hypergraph model, it's very easy to check the system reconfigurability in the presence of fault by verifying the existence of paths from the affected hyperedge to specifics blue hyperedges passing through specifics red hyperedges. The method is illustrated through a pedagogical example.","['analysis task', 'bottom up analysis hypergraph model', 'hypergraph model', 'modeling hypergraph', 'pedagogical example', 'reconfigurability analysis', 'structural system modeling', 'system reconfigurability', 'verifying the existence of paths from the affected hyperedge']"
S0010938X14000420,"One surface was then polished and cleaned using a protocol designed to eliminate as much preparation-related contamination as possible. This is as follows: The lead surface was polished by hand using a damp abrasive disc (BuehlerMet II ®) to remove visible surface defects and to expose a fresh metal surface. Coupons were then polished using a sequence of diamond polishes with decreasing particle sizes (6μm, 3μm, 1μm Buehler MetaDi ® polycrystalline diamond suspension). A polishing cloth (Buehler MicroCloth ®) was saturated with the appropriate diamond suspension. A custom-made jig fitted to an automatic polisher (Buehler Minimet ® 1000) was used to hold the coupons in place during automated polishing. Coupons were polished for 15min using each diamond suspension followed by rinsing with 2-propanol (99.5%, reagent grade) and cleaning in 2-propanol for 5min in an ultrasonic bath. After polishing with the 1μm diamond suspension, the coupons were ultrasonically cleaned in 2-propanol for 3×5min, with fresh propanol for each cleaning cycle. Polished coupons were stored in 2-propanol until required.","['2-propanol', 'automated polishing', 'cleaning', 'coupons', 'Coupons', 'custom-made jig', 'damp abrasive disc', 'diamond', 'diamond polishes', 'diamond suspension', 'metal surface', 'polished', 'polisher', 'polishing', 'polishing cloth', 'polycrystalline diamond', 'propanol', 'surface', 'ultrasonic bath']"
S2212667812000524,"Digital libraries promise new societal benefits, especially for e-learning in digital or mobile times, starting with the elimination of the time and space constraints of traditional bricks-and-mortar libraries. The library and information professionals are required to acquire such knowledge and skills as the library is one of the highly IT influenced service profession. This paper gives an overview of current trends in digital library research consists of digital library characteristic, advantage, disadvantages and function. This paper also highlights on the impact of information technology on the traditional library.","['acquire such knowledge and skills', 'bricks-and-mortar libraries', 'Digital libraries', 'digital library', 'e-learning', 'elimination of the time and space constraints', 'highlights on the impact of information technology', 'library', 'overview of current trends in digital library research']"
S2212667812000822,"It has been more than a century since the emergence of the lettered words. After that, with the development of economy and culture, the increase of international contacts and communication between China and foreign countries, lettered words have been appearing more frequently. Lettered words have become an indispensable part of Chinese vocabulary, such as WTO, Ka la OK and MP3. As a new phenomenon in the vocabulary system of the modern Chinese, the lettered words draws a lot of academic attention. Ecolinguistics is a new branch of linguistic, which combine the linguistic with the ecology. This paper is trying to analyze the lettered words from the perspective of Ecolinguistics. This paper will discuss the reasons of appearing the lettered words and the influence may give to modern Chinese form the ecolinguistic view.","['analyze the lettered words from the perspective of Ecolinguistics', 'branch of linguistic', 'Chinese vocabulary', 'combine the linguistic with the ecology', 'communication', 'development of economy and culture', 'discuss the reasons of appearing the lettered words', 'Ecolinguistics', 'Ka la OK', 'lettered words', 'Lettered words', 'MP3', 'vocabulary system', 'WTO']"
S0045782515001231,"Isogeometric analysis. The central idea of isogeometric analysis is to use the same ansatz functions for the discretization of the partial differential equation at hand, as are used for the representation of the problem geometry. Usually, the problem geometry Ω is represented in computer aided design (CAD) by means of NURBS or T-splines. This concept, originally invented in  [1] for finite element methods (IGAFEM) has proved very fruitful in applications  [1,2]; see also the monograph  [3]. Since CAD directly provides a parametrization of the boundary ∂Ω, this makes the boundary element method (BEM) the most attractive numerical scheme, if applicable (i.e., provided that the fundamental solution of the differential operator is explicitly known). Isogeometric BEM (IGABEM) has first been considered for 2D BEM in  [4] and for 3D BEM in  [5]. Unlike standard BEM with piecewise polynomials which is well-studied in the literature, cf. the monographs  [6,7] and the references therein, the numerical analysis of IGABEM is essentially open. We only refer to  [2,8–10] for numerical experiments and to  [11] for some quadrature analysis. In particular, a posteriori error estimation has been well-studied for standard BEM, e.g.,  [12–18] as well as the recent overview article  [19], but has not been treated for IGABEM so far. The purpose of the present work is to shed some first light on a posteriori error analysis for IGABEM which provides some mathematical foundation of a corresponding adaptive algorithm.","['2D BEM', '3D BEM', 'adaptive algorithm', 'ansatz functions', 'a\xa0posteriori error analysis', 'a\xa0posteriori error estimation', 'BEM', 'boundary element method', 'CAD', 'computer aided design', 'finite element methods', 'IGABEM', 'IGAFEM', 'isogeometric analysis', 'Isogeometric analysis', 'Isogeometric BEM', 'mathematical foundation of a corresponding adaptive algorithm', 'numerical analysis of IGABEM', 'numerical scheme', 'NURBS', 'problem geometry', 'quadrature analysis', 'shed some first light on a\xa0posteriori error analysis for IGABEM', 'standard BEM', 'T-splines']"
S0167931712002699,"As the progression towards smaller lithographic nodes continues it has become necessary to adopt thinner resist films to mitigate problems such as pattern collapse. To address the issue of reduced etch resistance of thin photoresist films the semiconductor industry has begun to develop multilayer processes where the pattern is first transferred into an intermediate organic hardmask with higher etch selectivity before final silicon pattern transfer [25–27]. In this paper we demonstrate how the introduction of such a multilayer process can also benefit nanosphere lithography by increasing achievable aspect ratios of silicon nanopillars without the need for complex etch processes requiring specialised and expensive equipment, but instead needing only a standard SF6/C4F8 inductively coupled plasma (ICP) mixed mode etch process at room temperature [28]. As intermediate layer material we used polyimide, which finds widespread use as encapsulation material for IC production. It is readily patterned in oxygen plasma and has a lower etch rate than silicon in SF6 gas. Its flexibility can also be used for the fabrication of soft polymer pillars by the same process as we will show. The multilayer process slightly increases the complexity of sample preparation but allows basic ICP etching to achieve high aspect ratio structures at smaller feature sizes that previously reported without the need for complex etching equipment.","['complex etch processes', 'coupled plasma', 'encapsulation material', 'etching equipment', 'fabrication of soft polymer pillars', 'ICP', 'ICP etching', 'IC production', 'intermediate layer material', 'intermediate organic hardmask', 'multilayer process', 'multilayer processes', 'nanosphere lithography', 'oxygen plasma', 'pattern collapse', 'polyimide', 'reduced etch resistance of thin photoresist films', 'sample preparation', 'SF6 gas', 'silicon', 'silicon nanopillars', 'silicon pattern transfer', 'smaller lithographic nodes', 'thinner resist films', 'thin photoresist films']"
S0301679X14003272,"The lateral force, Q, is measured and recorded throughout the entire test by a piezoelectric load cell which is connected to the quasi-stationary LSMB. The LSMB is mounted on flexures which provide flexibility in the horizontal direction so that the majority of the lateral force is transmitted though the much stiffer load path which contains the load cell as shown in Fig. 2. Both displacement and load sensors have been calibrated (both externally and in-situ) in static conditions. The load and displacement signals are sampled at a rate of two hundred measurements per fretting cycle at all fretting frequencies, with these data being used to generate fretting loops. The loops were used to derive the contact slip amplitude and the energy coefficient of friction in each cycle according to the method suggested by Fouvry et al. [17]. Average values for these were calculated for each test (the average coefficient of friction included values associated with the initial transients in the tests as suggested by Hirsch and Neu [18]).","['displacement and load sensors', 'flexures', 'fretting loops.', 'lateral force, Q, is measured and recorded', 'load cell', 'LSMB', 'piezoelectric load cell']"
S0375960115005630,"The systems in which the Stern–Gerlach force is most prominent are those with a high electromagnetic field gradient. Section 2 considers the implications of the coupling between the spin of a classical electron and the rapidly varying electromagnetic field produced by a laser-driven plasma wave. Sufficiently short, high-intensity laser pulses can form longitudinal waves within the electron density of a plasma. These density waves propagate with speed comparable to the group speed of the laser pulse. Not all plasma electrons form this wave, however; some of the electrons are caught up in the wave and accelerated by its high fields. The wave eventually collapses as these electrons damp the wave (the wave ‘breaks’). The extremely high electric field gradient of a plasma wave near wavebreaking provides an excellent theoretical testing ground for the effects of Stern–Gerlach-type contributions to the trajectory of a test electron.","['density waves', 'electromagnetic field', 'electromagnetic field gradient', 'electron', 'electrons', 'electrons damp the wave', 'high electric field gradient of a plasma wave', 'high-intensity laser pulses', 'laser', 'laser-driven plasma wave', 'laser pulse', 'longitudinal waves', 'plasma', 'plasma electrons', 'Stern–Gerlach force', 'Stern–Gerlach-type contributions', 'wave ‘breaks’']"
S2212671612000698,In Obstacle detection is based on inverse perspective mapping and homography. Obstacle classification is based on fuzzy neural network. The estimation of the vanishing point relies on feature extraction strategy. The method exploits the geometrical relations between the elements in the scene so that obstacle can be detected. The estimated homography of the road plane between successive images is used for image alignment. A new fuzzy decision fusion method with fuzzy attribution for obstacle detection and classification application is described The fuzzy decision function modifies parameters with auto-adapted algorithm to get better classification probability It is shown that the method can achieve better classification result.,"['auto-adapted algorithm', 'classification', 'detected', 'elements', 'estimated homography', 'estimation of the vanishing point', 'feature extraction strategy', 'fuzzy attribution for obstacle detection and classification application', 'fuzzy decision fusion method', 'fuzzy neural network', 'geometrical relations', 'image alignment', 'images', 'inverse perspective mapping and homography', 'obstacle', 'Obstacle classification', 'Obstacle detection', 'obstacle detection and classification application', 'road plane', 'scene', 'The fuzzy decision function']"
S0263822312001468,"Recently together with structural efficiency, passenger safety is also an important issue in application of material to transportation industries. Hence, the crashworthiness parameters are introducing to predict the capability of structure to prevent the massive damage and protect the passenger in the event of a crash. Crashworthiness parameters for various thin-walled tubes made from metal or fibre/resin composites in different geometries have been studied. A critical difference of tubular composites failure modes compared with metallic is the brittle collapse. In addition, in composites, tubular failure modes are involved with micro-cracking development, delamination, fibre breakage, etc., instead of plastic deformation. Implementation of composite materials in the field of crashworthiness is attributed to Hull, who in 80s and 90s of the last century studied extensively the crushing behaviour of fibre reinforced composite material. He found that the composite materials absorbed high energy in the face of the fracture surface energy mechanism rather than plastic deformation as observed for metals [1]. This observation has inspired others to further investigation about crashworthiness characteristics of composite materials. Studies have examined the axial crushing behaviour of fibre-reinforced tubes [2], fibreglass tubes [3,4], PVC tubes [5] and carbon fibre reinforced plastic (CFRP) tubes [6].","['axial crushing behaviour', 'brittle collapse', 'carbon fibre reinforced plastic', 'carbon fibre reinforced plastic (CFRP) tubes', 'CFRP', 'composite materials', 'composites', 'crashworthiness characteristics', 'crashworthiness parameters', 'Crashworthiness parameters', 'crushing behaviour', 'delamination', 'fibre breakage', 'fibreglass tubes', 'fibre reinforced composite material', 'fibre-reinforced tubes', 'fibre/resin composites', 'fracture surface energy', 'Implementation of composite materials', 'metal', 'metals', 'micro-cracking development', 'passenger safety', 'plastic deformation', 'prevent the massive damage', 'protect the passenger', 'PVC tubes', 'structural efficiency', 'transportation industries', 'tubular composites failure', 'tubular failure modes']"
S0370269304009037,"Within a coalescence approach as successfully applied earlier in the light-quark sector, we have evaluated transverse-momentum dependencies of charmed hadrons in central heavy-ion reactions at RHIC. For the charm-quark distributions at hadronization we have considered two limiting scenarios, i.e., no reinteractions (using spectra from PYTHIA) and complete thermalization with transverse flow of the bulk matter. The resulting J/ψ (mT-)spectra differ in slope by up to a factor of 2 (harder for pQCD c-quarks), and the integrated yield is about a factor of 3 larger in the thermal case. For D-mesons, we found that the difference in the slope parameters of the pT-spectra in the two scenarios is less pronounced, but their elliptic flow is about a factor of 2 larger for pT⩾1.5 GeV in the thermalized case. The elliptic flow pattern of D-mesons was found to be essentially preserved in the single-electron decay spectra, rendering the latter a very promising observable to address the strength of charm reinteractions in the QGP. The present study can be straightforwardly generalized to charmed baryons (Λc), which may serve as a complimentary probe for charm-quark reinteractions in the QGP.","['baryons', 'central heavy-ion reactions', 'charmed hadrons', 'charm-quark distributions', 'charm-quark reinteractions', 'charm reinteractions', 'coalescence approach', 'complete thermalization', 'complimentary probe', 'difference in the slope parameters', 'D-mesons', 'elliptic flow', 'elliptic flow pattern', 'evaluated transverse-momentum dependencies', 'hadronization', 'integrated yield', 'light-quark sector', 'pQCD c-quarks', 'pT-spectra', 'QGP', 'quark', 'single-electron decay spectra', 'spectra', 'transverse flow']"
S0370269302011838,"First results from RHIC on charged multiplicities, evolution of multiplicities with centrality, particle ratios and transverse momentum distributions in central and minimum bias collisions, are analyzed in a string model which includes hard collisions, collectivity in the initial state considered as string fusion, and rescattering of the produced secondaries. Multiplicities and their evolution with centrality are successfully reproduced. Transverse momentum distributions in the model show a larger pT-tail than experimental data, disagreement which grows with increasing centrality. Discrepancies with particle ratios appear and are examined comparing with previous features of the model at SPS.First results from RHIC on charged multiplicities, evolution of multiplicities with centrality, particle ratios and transverse momentum distributions in central and minimum bias collisions, are analyzed in a string model which includes hard collisions, collectivity in the initial state considered as string fusion, and rescattering of the produced secondaries. Multiplicities and their evolution with centrality are successfully reproduced. Transverse momentum distributions in the model show a larger pT-tail than experimental data, disagreement which grows with increasing centrality. Discrepancies with particle ratios appear and are examined comparing with previous features of the model at SPS.","['charged multiplicities', 'collectivity in the initial state', 'Discrepancies with particle ratios', 'evolution of multiplicities with centrality', 'hard collisions', 'Multiplicities', 'particle', 'particle ratios', 'rescattering of the produced secondaries', 'string model', 'their evolution', 'transverse momentum distributions', 'Transverse momentum distributions']"
S0167273813006735,"While impedance spectroscopy is a quite common method to investigate mixed conducting thin film electrodes, [6,10–12] oxygen tracer experiments are often performed on bulk samples [13–16]. Recently, several IEDP measurements of mixed conducting cathode materials were published with the oxide films being deposited on insulating substrates [17–19]. However, to the best of the authors' knowledge no study so far reported experiments with both techniques being applied on the same films at the same temperature. This contribution reports the results of a study applying EIS and IEDP to one and the same La0.6Sr0.4CoO3−δ (LSC) thin film in order to get complementary results on the resistive contributions of the oxygen reduction kinetics on such films. As electrical measurements require an oxygen ion conductor, yttria stabilized zirconia (YSZ) was used as substrate for LSC films with two different grain sizes. Quantitative material parameters are deduced from both types of experiments and comparison of the data allowed testing the appropriateness of analysis models.","['analysis models', 'applying EIS and IEDP to one and the same La0.6Sr0.4CoO3−δ (LSC) thin film', 'appropriateness of analysis models', 'bulk samples', 'comparison of the data', 'EIS', 'electrical measurements', 'films', 'grain', 'IEDP', 'IEDP measurements', 'impedance spectroscopy', 'insulating substrates', 'La0.6Sr0.4CoO3−δ', 'LSC', 'LSC films', 'mixed conducting cathode materials', 'mixed conducting thin film electrodes', 'oxide films', 'oxygen', 'oxygen ion conductor', 'oxygen reduction kinetics', 'oxygen tracer experiments', 'Quantitative material parameters', 'resistive contributions of the oxygen reduction kinetics', 'thin film', 'YSZ', 'yttria stabilized zirconia']"
S187775031300077X,"Although mean-field models have been used in all these settings, little analysis has been done on their behaviour as spatially extended dynamical systems. In part, this is due to their staggering complexity. The Liley model [15] considered here, for instance, consists of fourteen coupled Partial Differential Equations (PDEs) with strong nonlinearities, imposed by coupling between the mean membrane potentials and the mean synaptic inputs. The model can be reduced to a system of Ordinary Differential Equations (ODEs) by considering only spatially homogeneous solutions, and the resulting system has been examined in detail using numerical bifurcation analysis (see [16] and references therein). In order to compute equilibria, periodic orbits and such objects for the PDE model, we need a flexible, stable simulation code for the model and its linearization that can run in parallel to scale up to a domain size of about 2500cm2, the size of a full-grown human cortex. We also need efficient, iterative solvers for linear problems with large, sparse matrices. In this paper, we will show that all this can be accomplished in the open-source software package PETSc [17]. Our implementation consists of a number of functions in C that are available publicly [18].","['efficient, iterative solvers', 'efficient, iterative solvers for linear problems with large, sparse matrices', 'large, sparse matrices', 'Liley model', 'mean-field models', 'membrane potentials', 'numerical bifurcation analysis', 'ODEs', 'Ordinary Differential Equations', 'Partial Differential Equations', 'PDE model', 'PDEs', 'PETSc', 'spatially extended dynamical systems', 'synaptic inputs']"
S0022311515002664,"The primary benefit of using a 3D model is that it allows the application of anisotropic material properties. As a hexagonal close packed lattice structure, a single zirconium grain is plastically anisotropic due to the difficulty of activating slip with a 〈c〉 component [23–26]. Abaqus allows this to be represented by setting plasticity potential ratios. The anisotropic elastic and plastic constants are shown in Table 1. Zirconium alloys can often have a bimodal basal pole distribution, with a tilt on the basal normal or c direction of ±30° in the normal direction being quoted for recrystallized Zircaloy-4 [27,28]. However, for simplicity the basal normal or c direction has been taken as being parallel to the normal direction. As such the 1, 2 and 3 directions in Table 1 correlate with the X, Y and Z global coordinate system for the 3D simulations, with the 3 direction correlating to the c direction of a zirconium unit lattice. Table 1 also shows the elastic properties incorporated into the simulations. The oxide layer has been simulated as a purely elastic material. Although it is known that the oxide is strongly textured [29], it is still simulated as a homogenous solid therefore isotropic material properties have been used for the oxide in all simulations.","['3D model', '3D simulations', 'Abaqus', 'anisotropic elastic and plastic constants', 'anisotropic material properties', 'bimodal basal pole distribution', 'isotropic material properties', 'oxide', 'oxide layer', 'plasticity potential ratios', 'purely elastic material', 'recrystallized Zircaloy-4', 'simulations', 'Zirconium', 'zirconium grain', 'zirconium unit lattice']"
S0377221716301904,"We propose an equilibrium model that allows to analyze the long-run impact of the electricity market design on transmission line expansion by the regulator and investment in generation capacity by private firms in liberalized electricity markets. The model incorporates investment decisions of the transmission system operator and private firms in expectation of an energy-only market and cost-based redispatch. In different specifications we consider the cases of one vs. multiple price zones (market splitting) and analyze different approaches to recover network cost—in particular lump sum, generation capacity based, and energy based fees. In order to compare the outcomes of our multilevel market model with a first best benchmark, we also solve the corresponding integrated planner problem. Using two test networks we illustrate that energy-only markets can lead to suboptimal locational decisions for generation capacity and thus imply excessive network expansion. Market splitting heals these problems only partially. These results are valid for all considered types of network tariffs, although investment slightly differs across those regimes.","['analyze different approaches to recover network cost', 'analyze the long-run impact of the electricity market design on transmission line expansion', 'energy based fees', 'equilibrium model', 'excessive network expansion', 'first best benchmark', 'generation capacity based', 'integrated planner problem', 'lump sum', 'market splitting', 'Market splitting', 'multilevel market model', 'recover network cost', 'solve the corresponding integrated planner problem']"
S0370269304009177,"In the brane system appearing in string/D-brane theory, the stableness is the most important requirement. We find some stable brane configurations in the SUSY bulk-boundary theory. We systematically solve the singular field equation using a general mathematical result about the free-wave solution in S1/Z2-space. The two scalars, the extra-component of the bulk-vector (A5) and the bulk-scalar (Φ), constitute the solutions. Their different roles are clarified. The importance of the “parallel” configuration is disclosed. The boundary condition (of A5) and the boundary matter fields are two important elements for making the localized configuration. Among all solutions, the solution (c1=−1, c2=−1) is expected to be the thin-wall limit of a kink solution. We present a bulk Higgs model corresponding to the non-singular solution. The model is expected to give a non-singular and stable brane solution in the SUSY bulk-boundary theory.","['A5', 'boundary condition', 'brane system', 'bulk Higgs model', 'bulk-scalar', 'c1=−1, c2=−1', 'different roles are clarified', 'extra-component of the bulk-vector', 'general mathematical result about the free-wave solution', 'kink solution', 'non-singular and stable brane solution', 'non-singular solution', 'solution', 'solve the singular field equation', 'stable brane configurations', 'stableness', 'string/D-brane theory', 'SUSY bulk-boundary theory', 'thin-wall limit', 'Φ']"
S0032386108009397,"The viscoelastic behavior of elastomers containing small amounts of unattached chains has been investigated to characterize the dynamics of the polymer chains trapped in fixed networks [66–68]. Polymer chains trapped in fixed networks constitute a simpler system for the study of the polymer chain dynamics than the corresponding uncrosslinked polymer melts. This is because the complicated effect of the motion of the surrounding chains on the dynamics of the probe chain – called “constraint release” [69] – is absent in the fixed network systems. Most of the earlier studies employed randomly crosslinked elastomers as host networks. In this case, precise control of the mesh size of the host networks is not possible, and the mesh size has a broad distribution. The end-linking systems give the host networks a more uniform mesh size, and they can control the mesh size by the size of the precursor chains. We investigated the dynamic viscoelasticity of end-linked PDMS elastomers containing unattached linear PDMS as functions of the size of the unattached chains (Mg) and the network mesh (Mx) (Fig. 9a) [70]. We employed two types of host networks with Mx>Me and Mx<Me where Me (≈10,000 for PDMS) is the molecular mass between adjacent entanglements in the molten state. The Mx>Me and Mx<Me networks (designated as NL and NS, respectively) were designed by end-linking the long (Mn=84,000) and short precursor chains (Mn=4,550), respectively. The mesh of the NL networks is dominated by trapped entanglements, while that of the NS network is governed by chemical cross-links.","['characterize the dynamics of the polymer chains', 'chemical cross-links.', 'constraint release', 'control of the mesh size', 'crosslinked elastomers', 'elastomers', 'mesh', 'Mg', 'Mx', 'Mx<Me', 'Mx>Me', 'network mesh', 'NL', 'NS', 'PDMS', 'PDMS elastomers', 'polymer chain', 'polymer chains', 'Polymer chains', 'polymer melts', 'precursor chains', 'probe chain', 'unattached chains', 'viscoelastic behavior']"
S0377025714002213,"The first of these systems, a biopolymer gel, involves the thermoreversible gelation of aqueous gelatin solutions to form a physical gel, whereas the other systems considered herein involve the formation of chemical gels featuring permanent cross-linked branching networks. The second system is a commercial silicone dielectric gel (SDG) which is used in the production of electronic products created by industrial printing processes. The third experimental system is a fibrin gel formed by the thrombin-induced polymerisation of fibrinogen molecules. The gel network product in the latter case forms the principal microstructural component of a blood clot [8]. The latter case is particularly interesting as the critical-gel which is established at the GP serves as a ‘template’ for the ensuing development of microstructure and associated rheological properties in the post-GP phase of fibrin clot evolution [9].","['biopolymer gel', 'blood clot', 'commercial silicone dielectric gel', 'critical-gel', 'cross-linked branching networks', 'fibrin clot', 'fibrin gel', 'fibrinogen molecules', 'gel network product', 'GP', 'microstructural component', 'production of electronic products', 'SDG', '‘template’ for the ensuing development of microstructure and associated rheological properties', 'thermoreversible gelation', 'thrombin-induced polymerisation']"
S0021999115004301,"A popular choice is to couple a set of quadrature points with an equal number of nodal Lagrange polynomials defined at the same points, leading to a collocation method. There are many examples of this throughout the literature, both in terms of the more traditionally utilised continuous Galerkin (CG) and discontinuous Galerkin (DG) formulations, as well as newer extensions such as the flux reconstruction (FR) technique as presented by Huynh [23]. In collocation methods, while most linear operators can be exactly integrated in this setting depending on the choice of quadrature, integrals of nonlinear terms typically incur numerical error. However, the computational efficiencies that can be attained through the use of a collocation formulation, especially given the presence of a diagonal mass matrix, often outweigh the numerical error that is incurred.","['CG', 'collocation formulation, especially given the presence of a diagonal mass matrix', 'collocation method', 'collocation methods', 'couple a set of quadrature points with an equal number of nodal Lagrange polynomials', 'DG', 'discontinuous Galerkin', 'flux reconstruction', 'FR', 'Galerkin', 'linear operators', 'newer extensions']"
S0010482516301810,"Three-dimensional digital subtraction angiographic (3D-DSA) images from diagnostic cerebral angiography were obtained at least one day prior to embolization in all patients. The raw data of 3D-DSA in a DICOM file were used for creating a 3D model of the target vessel segment. These data were converted to standard triangulation language (STL) surface data as an aggregation of fine triangular meshes using 3D visualization and measurement software (Amira version X, FEI, Burlington, MA, USA). An unstructured computational volumetric mesh was constructed from the triangulated surface. Smoothing and remeshing followed as next steps. The STL file was then transferred to a 3D printer (OBJET30 Pro; Stratasys Ltd., Eden Prairie, MN, USA). The resolution of the build layer was 0.028mm, and the 3D printed vessel model was produced using acrylic resin (Vero). Following immersion in water for a few hours, the surface of the 3D printed model was smoothed by manually removing spicule.","['3D-DSA', '3D model', '3D printed model', '3D printer', '3D visualization', '3D visualization and measurement software', 'acrylic resin', 'Amira version X', 'cerebral angiography', 'computational volumetric mesh', 'creating a 3D model of the target vessel segment', 'DICOM file', 'embolization', 'immersion in water', 'measurement', 'OBJET30 Pro', 'remeshing', 'removing spicule.', 'smoothed', 'Smoothing', 'standard triangulation language', 'STL', 'STL file', 'Three-dimensional digital subtraction angiographic', 'triangular meshes', 'triangulated surface', 'Vero', 'water']"
S0009261412006513,"Water is the most important liquid, and the nature of its structure remains a topic of keen debate and an active area of research [1–9]. Much of this debate centers around whether water has a mainly tetrahedral structure with a continuum of distorted hydrogen bonds, or if it contains a mixture of two distinct components. One major development in recent years is the application of inner-shell spectroscopic techniques, such as X-ray absorption spectroscopy (XAS) and X-ray emission spectroscopy (XES) at the oxygen K-edge to investigate the structure of water [2,10–12]. These methods can provide a direct structural probe of water, providing insight into the nature of its hydrogen bonding network. Theoretical studies play a critical role in these studies, since the analysis of the experimental data requires calculations to provide a link between the observed spectral features and the underlying structure. However, the simulation of the XAS or XES for liquid water presents a difficult challenge because it requires accurate molecular dynamics simulations to provide a correct description of the molecular structure coupled with accurate calculations of the spectral properties, i.e. excitation energies and line intensities. Furthermore, adequate sampling over molecular configurations also needs to be accounted for.","['accurate molecular dynamics simulations', 'adequate sampling', 'calculations', 'direct structural probe of water', 'distorted hydrogen bonds', 'excitation energies', 'experimental data', 'hydrogen bonding network', 'inner-shell spectroscopic techniques', 'line intensities', 'liquid', 'liquid water', 'molecular configurations', 'molecular structure', 'observed spectral features', 'oxygen K-edge', 'spectral properties', 'underlying structure', 'water', 'Water', 'XAS', 'XES', 'X-ray absorption spectroscopy', 'X-ray emission spectroscopy']"
S1071581916300854,"We have developed a systematic, quantified understanding of a specific problem: the design of mobile-friendly unique identifiers. But our results also apply to the design of other text-based services. There has been a trend toward bespoke and adaptive keyboards (e.g., Dunlop and Levine, 2012; Karrenbauer and Oulasvirta, 2014; Leiva et al., 2015; Wiseman et al., 2013). More often than not, though, input devices are a fixed constraint in the design of a service. Most users are typing on the keyboard that came with their phone. Those keyboards have advantages, limitations and quirks. The mode-switching that most touchscreen keyboards require to reach numbers and capital letters is at the root of design improvements we propose in this paper. When designing services, it is vital to be aware of the fixed constraints of a system and to then focus on the aspects of a service's design that can be controlled. Making changes to input data in this way is a cheap, quick and easy way to improve user experience.","['bespoke and adaptive keyboards', 'designing services', 'design of a service', 'design of mobile-friendly unique identifiers', 'design of other text-based services', 'input devices', 'mode-switching']"
S0032386107010518,"Copper-catalyzed Huisgen cycloadditions have been recently extensively studied by polymer chemists for the synthesis of functional polymers (either end-functional or side-functional). The post-functionalization of synthetic polymers is an important feature of macromolecular engineering as many polymerization mechanisms are rather sensitive to the presence of bulky or functional groups. For example, a wide variety of telechelic polymers (i.e. polymers with defined chain-ends) can be efficiently prepared using a combination of atom transfer radical polymerization (ATRP) and CuAAC. This strategy was independently reported in early 2005 by van Hest and Opsteen [31], Lutz et al. [32], and Matyjaszewski et al. [33]. Such step was important since ATRP is a very popular polymerization method in modern materials science [34,35]. Indeed, ATRP is a facile technique, which allows the preparation of well-defined polymers with narrow molecular weight distribution, predictable chain length, controlled microstructure, defined chain-ends and controlled architecture [36–41]. However, the range of possibilities of ATRP can be further broadened by CuAAC. For instance, the ω-bromine chain-ends of polymers prepared by ATRP can be transformed into azides by nucleophilic substitution and subsequently reacted with functional alkynes (Scheme 3) [32]. Due to the very high chemoselectivity of CuAAC, this method is highly modular and may be used to synthesize a wide range of ω-functional polymers. Moreover, the formed triazole rings are not “passive” spacers but interesting functions exhibiting H-bonds capability, aromaticity and rigidity.","['alkynes', 'atom transfer radical polymerization', 'ATRP', 'azides', 'Copper-catalyzed Huisgen cycloadditions', 'CuAAC', 'functional polymers', 'H-bonds', 'macromolecular engineering', 'nucleophilic substitution', 'polymerization', 'polymers', 'polymers with defined chain-ends', 'preparation of well-defined polymers', 'synthesis of functional polymers', 'synthetic polymers', 'telechelic polymers', 'triazole rings', 'ω-bromine chain-ends of polymers', 'ω-functional polymers']"
S000926141301539X,"In this Letter we revisit the Chesnavich model Hamiltonian [37] in the light of recent developments in TST. For barrierless systems such as ion–molecule reactions, the concepts of OTS and TTS can be clearly formulated in terms of well defined phase space geometrical objects. (For work on the phase space description of OTS, see Refs. [38–40].) The first goal of the present article is the identification of these notions with well defined phase space dividing surfaces attached to NHIMs. The second and main goal is an elucidation of the roaming phenomenon in the context of the Chesnavich model Hamiltonian. The associated potential function, possessing many features associated with a realistic molecular PES, leads to dynamics which clearly reveal the origins of the roaming effect. Based on our trajectory simulations, we show how the identification of the TTS and OTS DSs with periodic orbit dividing surfaces (PODS) provides the natural framework for analysis of the roaming mechanism.","['associated potential function', 'Chesnavich model Hamiltonian', 'dynamics which clearly reveal the origins of the roaming effect', 'identification', 'ion–molecule reactions', 'NHIMs', 'OTS', 'periodic orbit dividing surfaces', 'PODS', 'space geometrical objects', 'trajectory simulations', 'TST', 'TTS', 'well defined phase space dividing surfaces attached to NHIMs']"
S1364815216303061,"In representing wetland-river interactions involving GIWs, many models assume that the wetland can discharge into a river but cannot receive overbank flows from it. In such models, the volume of water (or water level elevation) in a wetland and its corresponding threshold value (predominantly controlled by outlet elevation) are the prime determinants of wetland outflow (Feng et al., 2012; Hammer and Kadlec, 1986; Johnson et al., 2010; Kadlec and Wallace, 2009; Powell et al., 2008; Voldseth et al., 2007; Wen et al., 2013; Zhang and Mitsch, 2005). However, in regions characterised by widespread riparian wetlands that are hydraulically connected with adjacent rivers, wetland-river interaction is likely to be bidirectional. Such interactions should be quantified according to hydraulic principles involving relative river and wetland water level elevations as well as the properties of the connection between the two (Kouwen, 2013; Liu et al., 2008; Min et al., 2010; Nyarko, 2007; Restrepo et al., 1998). In the WATFLOOD model, for instance, riparian wetland-river interaction is modelled using the principle of Dupuit-Forchheimer lateral/radial groundwater flow (Kouwen, 2013). Since exchange between riparian wetlands and rivers can occur over the surface and/or through the subsurface, Restrepo et al. (1998) incorporated an equivalent transmissivity expression, obtained for wetland vegetation and the subsurface soil, into the Darcy flow equation of the MODFLOW model.","['Darcy flow equation', 'equivalent transmissivity expression', 'interactions should be quantified according to hydraulic principles', 'MODFLOW', 'outlet elevation', 'principle of Dupuit-Forchheimer lateral/radial groundwater flow', 'properties of the connection between the two', 'relative river and wetland water level elevations', 'representing wetland-river interactions involving GIWs', 'riparian wetland-river interaction is modelled', 'subsurface soil', 'WATFLOOD', 'wetland vegetation']"
S0021999115001412,"Inspired by energy-fueled phenomena such as cortical cytoskeleton flows [46,45,32] during biological morphogenesis, the theory of active polar viscous gels has been developed [37,33]. The theory models the continuum, macroscopic mechanics of a collection of uniaxial active agents, embedded in a viscous bulk medium, in which internal stresses are induced due to dissipation of energy [41,58]. The energy-consuming uniaxial polar agents constituting the gel are modeled as unit vectors. The average of unit vectors in a small local volume at each point defines the macroscopic directionality of the agents and is described by a polarization field. The polarization field is governed by an equation of motion accounting for energy consumption and for the strain rate in the fluid. The relationship between the strain rate and the stress in the fluid is provided by a constitutive equation that accounts for anisotropic, polar agents and consumption of energy. These equations, along with conservation of momentum, provide a continuum hydrodynamic description modeling active polar viscous gels as an energy consuming, anisotropic, non-Newtonian fluid [37,33,32,41]. The resulting partial differential equations governing the hydrodynamics of active polar viscous gels are, however, in general analytically intractable.","['active polar viscous gels', 'biological morphogenesis', 'constitutive equation', 'continuum hydrodynamic description', 'cortical cytoskeleton flows', 'energy consuming, anisotropic, non-Newtonian fluid', 'equation of motion', 'fluid', 'gel', 'models the continuum, macroscopic mechanics', 'polarization field', 'polar viscous gels', 'theory of active polar viscous gels', 'These equations, along with conservation of momentum', 'uniaxial active agents', 'uniaxial polar agents', 'viscous bulk medium']"
S0022311514008691,"The class of steels known as oxide dispersion strengthened (ODS) ferritic alloys (also known as nanostructured ferritic alloys) consist of a dispersion of ultra-fine oxide particles throughout the matrix. These oxide particles serve to improve the mechanical properties of the system, particularly at high temperatures, of the system through inhibiting dislocation motion and grain boundary sliding. In nuclear applications the oxide particles have been suggested to act as point defect sinks [10,11] to improve radiation tolerance, and as preferential sites for the formation of nano-scale He bubbles therefore reducing swelling compared to non-ODS steels [12–15]. The ability of the oxide particles to improve these properties depends on the structure and composition of the particles [10,11,16,17] and their stability under irradiation. Typical compositions of ODS steels include between 9 and 14at.% Cr for oxidation resistance (most commonly 14at.%); W for solid solution hardening; Y2O3 that is put into solid solution during the initial, mechanical alloying, process but then during consolidation at high temperatures forms precipitates; and Ti to inhibit significant growth of the oxide particles; the balance being made up of Fe and impurities [18]. For this reason these steels are often referred to as 14YWT, reflecting the constituent elements.","['14YWT', '9 and 14at.% Cr', 'balance', 'class of steels', 'consolidation', 'constituent elements', 'dispersion of ultra-fine oxide particles throughout the matrix', 'Fe and impurities', 'ferritic alloys', 'improve radiation tolerance', 'improve the mechanical properties of the system', 'irradiation', 'mechanical alloying', 'nano-scale He bubbles', 'nanostructured', 'non-ODS steels', 'ODS', 'ODS steels', 'oxidation', 'oxide dispersion strengthened', 'oxide particles', 'precipitates', 'radiation', 'reducing swelling', 'solid solution', 'steels', 'structure and composition of the particles', 'swelling', 'Ti', 'W', 'Y2O3']"
S2212667814000690,"The paper presents the results of studies of the effect of multiwalled carbon nanotubes 18-20nm in concentrations of 1 and 10mg / ml for diatoms Pseudo-nitzschia pungens (clone PP-07) and golden alga Isochrysis galbana (clone TISO). The toxic effects of multiwalled nanotubes on both types of algae is revealed, which results in a decrease of the linear dimensions of cells, chloroplasts, and a reduced number of cells when incubated over 24h (Pseudo-nitzschia pungens) and 36hours (Isochrysis galbana).","['algae', 'carbon nanotubes', 'cells', 'chloroplasts', 'clone PP-07', 'clone TISO', 'golden alga Isochrysis galbana', 'incubated', 'Isochrysis galbana', 'nanotubes', 'presents the results of studies of the effect of multiwalled carbon nanotubes', 'Pseudo-nitzschia pungens', 'studies', 'toxic effects']"
S0167844214000652,"A bond failure is thought of as a micro-crack nucleation, specifically as a separation between the adjacent cells in the cellular structure along their common face. Initially, the micro-cracks may be dispersed in the model reflecting the random distribution of pore sizes and the low level of interaction due to force redistribution. Interaction and coalescence may follow as the population of micro-cracks increases. These situations are illustrated in Fig. 3. The structure of the failed surface can be represented with a mathematical graph, where graph nodes represent failed faces and graph edges exist between failed faces with common triple line in the cellular structure, i.e. where two micro-cracks formed a continuous larger crack. With reference to Fig. 3, each failed face is a graph node and each pair of neighbouring failed faces is a graph edge.","['bond failure', 'cellular structure', 'crack', 'failed face', 'failed faces', 'failed surface', 'mathematical graph', 'micro-crack nucleation', 'micro-cracks', 'pore', 'separation between the adjacent cells in the cellular structure along their common face']"
S0022311515301963,"Following fission, noble gas atoms will be distributed in the fuel matrix initially accommodated at point defects trap sites, generally thought to be Schottky trivacancy defects [4,5,31]. Diffusion to either bubbles or grain boundaries is then facilitated by associating a further uranium vacancy defect for the gas atom to ‘hop’ into, with the original vacancy then able to loop around to ensure continued diffusion. The rate determining step in the process is not the migration of the Xe itself but rather the rearrangement of the VU defect to facilitate net Xe diffusion [6–8]. Activation energies for the overall process depend on the availability of the defect trap sites, which in turn depends on the crystal stoichiometry. For Xe diffusion in UO2−x, UO2 and UO2+x the activation energies calculated using DFT are 7.04–12.92 eV, 4.15–7.88 eV and 1.38–4.07 eV with the ranges reflecting the way the calculations were performed depending on the charge states of the defects involved and the presence of a Jahn–Teller distortion [7]. Activation energies calculated using empirical pair potentials can vary strongly depending on the choice of potential. Govers et al. examined three different potentials for UO2 (those of Basak [9], Jackson [10] and Morelon [11]) coupled with different parameterisations for the U–Xe and O–Xe interactions from Geng [12] and Nicoll [13] and recommend values of 6.5 eV, 4.5 eV and 2.4 eV [6] for the different stoichiometric regimes in very good agreement with the experimental values of 6.0 eV, 3.9 eV and 1.7 eV respectively [14].","['Activation', 'bubbles', 'charge', 'crystal', 'crystal stoichiometry', 'defect trap sites', 'DFT', 'diffusion', 'Diffusion', 'empirical pair potentials', 'fission', 'fuel matrix', 'gas atom', 'grain boundaries', '‘hop’ into', 'Jahn–Teller distortion', 'loop around', 'migration', 'noble gas atoms', 'O–Xe', 'point defects trap sites', 'potential', 'potentials', 'rearrangement', 'Schottky trivacancy defects', 'UO2', 'UO2+x', 'UO2−x', 'uranium', 'U–Xe', 'VU', 'VU defect', 'Xe', 'Xe diffusion']"
S0010938X13003818,"Based on the theoretical analysis, the value of the measuring resistor, Rm, has no effect on the corrosion process and on the estimated value of noise resistance. In order to validate this conclusion, the experiment of Fig. 9 was performed. Specifically, a pair of nominally identical specimens was initially coupled by a 4.7kΩ resistor and their potential with respect to a saturated calomel electrode was recorded by using a NI-USB 6009 analog-to-digital converter. The electrochemical noise signal was recorded using in-house developed software, acquiring at 1023Hz segments of 1000 points at each iteration. Between iterations, the 1000 values acquired were averaged to obtain a single value of potential, subsequently saved to the file used for later processing. The final dataset comprised potential values spaced 1±0.05s in time. Under the assumption that the noise present above 1023Hz is negligible compared with the noise present below 0.5Hz, this procedure enables an accurate recording of the potential noise in the frequencies of interest, avoiding aliasing of frequencies between 0.5 and 1023Hz and minimizing the 50Hz interference from the mains supply.","['4.7kΩ resistor', 'accurate recording of the potential noise in the frequencies of interest', 'corrosion process', 'dataset', 'electrochemical noise signal', 'in-house developed software', 'NI-USB 6009 analog-to-digital converter', 'obtain a single value of potential', 'pair of nominally identical specimens', 'Rm', 'saturated calomel electrode', 'theoretical analysis', 'validate this conclusion', 'value of noise resistance', 'value of the measuring resistor']"
S0168874X1630049X,"The crack band approach for producing mesh independent load–displacement curves for fracture in plain concrete is based on the idea that the crack opening is transformed into inelastic strain by distributing it over an element length dependent zone [5]. This approach will only produce mesh independent load–displacement curves, if the inelastic strain profiles in the finite element analysis are mesh size dependent. This requirement is an important difference to the nonlocal model which is designed to produce both mesh size independent load–displacement curves and strain profiles. In CDPM2, the crack band approach is applied only to the tensile part of the damage algorithm by replacing the stress–inelastic strain law shown in Fig. 2(b) by a stress–inelastic displacement law of the form(13)σ=ftexp(−ϵinhwft)if(ϵin>0)Here, wft is a crack opening threshold used to control the slope of the softening curve and h is the width of the crack-band, which in the present study is equal to the maximum dimension of the element along the principal direction of the strain tensor corresponding to the maximum tensile principal strain at the onset of damage. For the compressive part, a stress–inelastic strain law was used to determine the compressive damage parameter, since it was reported in [14] for columns subjected to eccentric compression that inelastic strain profiles in compression do not exhibit a mesh dependence which would satisfy the assumptions of the crack-band approach. This approach of applying the crack-band approach only to the tensile part has already been successfully used in Grassl et al. [16].","['(13)σ=ftexp(−ϵinhwft)if(ϵin>0)', 'crack band approach', 'crack-band approach', 'inelastic strain profiles', 'stress–inelastic displacement law', 'stress–inelastic strain law', 'tensile part']"
S2212667813000774,"Evolutionary Algorithms are the stochastic optimization methods, simulating the behavior of natural evolution. These algorithms are basically population based search procedures efficiently dealing with complex search spaces having robust and powerful search mechanism. EAs are highly applicable in multiobjective optimization problem which are having conflicting objectives. This paper reviews the work carried out for diversity and convergence issues in EMO.","['EAs', 'EMO', 'Evolutionary Algorithms', 'natural evolution', 'optimization problem', 'population based search procedures', 'reviews the work carried out for diversity and convergence issues in EMO', 'search mechanism', 'stochastic optimization methods']"
S0022311511010014,"Discovering that both the vacancy and interstitial defect migration pathways are confined to Ga-free regions suggests changes in recombination rates of isolated vacancy-interstitial pairs in comparison to pure Pu. The degree to which the rates are effected depends on the distribution of residual defects post a cascade event, in addition to the concentration and ordering of the Ga atoms. If vacancies and interstitials become greatly separated after the collision cascade, then pathways to recombination are likely to become restricted and recovery times will be extended. This is viable for cascades that created a vacancy rich core surrounded by dispersed interstitials, as found for the low energy cascades in Pu and PuGa [11,12]. This may also be the case for channelling events, where energetic atoms travel deep into the lattice through channels of low atomic density.","['cascade event', 'channelling events', 'collision cascade', 'defect migration pathways', 'energetic atoms travel deep into the lattice through channels of low atomic density', 'Ga', 'Ga atoms', 'low energy cascades in Pu and PuGa']"
S2352179114200056,"Power and particle exhaust are crucial for the viability of any future fusion power plant concept. Heat in fusion reactors must be extracted through a wall and cannot be exhausted volumetrically, which limits the allowed power density in fusion reactors [1] and is a severe technical challenge in itself [2]. In addition, structural material changes resulting from neutron irradiation cause degradation in the heat exhaust capabilities of existing designs [3] and static surfaces can suffer severely from erosion due to impinging plasma particles [4,5]. It is concluded that conventional concepts and materials for plasma facing components (PFCs) reach their limits in terms of material lifetime and power exhaust at approximately 20MW/m2, which is presumably dramatically reduced to <10MW/m2 due to neutron damage in a D-T reactor [6] or even only half that value [7].","['D-T reactor', 'fusion power plant', 'fusion reactors', 'impinging plasma particles', 'neutron', 'neutron damage', 'neutron irradiation', 'PFCs', 'plasma facing components', 'Power and particle exhaust', 'severe technical challenge', 'static surfaces', 'structural material', 'structural material changes']"
S1010603013001809,"The other methods for enhancement of photocatalytic activity are grafting co-catalysts. There are two kinds of co-catalysts in terms of its function: one is for separation of electrons and the other is for separation of holes. The former representative co-catalysts are Pt, Fe3+, and Cu2+ [9–12]. It was reported that Fe3+ and Cu2+ were grafted as amorphous oxide cluster [9,10], and reduced into Fe2+ and Cu+ by receiving one electron, respectively [11,12]. The reduced metal oxide cluster with reduced ions could return into the original state by giving more than one electron to molecular oxygen. The latter ones are CoOx, CoPi (CoPOx), IrOx, and RuOx which are used for water oxidation, among which CoPi is reported to be the most effective co-catalyst for water oxidation [13]. However, there were few reports concerning co-grafting effects on photocatalytic activity especially in gaseous phase. We expected that by co-grafting of both co-catalysts for separations of electrons and holes, photocatalytic activity in gaseous phase would be further enhanced. Moreover, complex of BiVO4 with the other materials of p-type semiconductor is also effective for enhancing photocatalytic activity.","['amorphous oxide cluster', 'BiVO4', 'co-catalyst', 'co-catalysts', 'co-grafting', 'CoOx', 'CoPi', 'CoPOx', 'Cu+', 'Cu2+', 'electron', 'electrons', 'enhancement of photocatalytic activity', 'Fe2+', 'Fe3+', 'gaseous phase', 'holes', 'IrOx', 'oxygen', 'photocatalytic activity', 'Pt', 'p-type semiconductor', 'reduced ions', 'reduced metal oxide cluster', 'RuOx', 'separation of electrons', 'separation of holes', 'water oxidation']"
S0370269304009104,"Though, in this Letter we have constructed the Born–Infeld black holes in the presence of a cosmological constant and discussed their thermodynamical properties, many issues however still remain to be investigated. We know that Reissner–Nordström AdS black holes undergo Hawking–Page phase transition. This transition gets modified as we include Born–Infeld corrections into account. We hope to carry out a detail study on this issue in the future. Furthermore, in the context of brane world cosmology, it was found that a brane moving in a Reissner–Nordström AdS background generates non-singular cosmology [14]. However, as shown in [15], the brane always crosses the inner horizon of the bulk geometry, creating an instability. It would be interesting to study cosmology on the brane when it is moving in the charged black hole backgrounds that we have constructed. Note that since these charged holes does not have inner horizon for certain range of parameters, we may generate non-singular cosmology without creating the instabilities that we have just mentioned.","['Born–Infeld black holes', 'Born–Infeld corrections', 'brane', 'brane world cosmology', 'charged black hole', 'charged holes', 'constructed the Born–Infeld black holes in the presence of a cosmological constant', 'cosmology', 'crosses the inner horizon of the bulk geometry', 'discussed their thermodynamical properties', 'generate non-singular cosmology without creating the instabilities', 'Hawking–Page phase transition', 'instability', 'non-singular cosmology', 'Reissner–Nordström AdS background', 'Reissner–Nordström AdS black holes', 'study cosmology on the brane when it is moving in the charged black hole backgrounds']"
S107158191630074X,"An obvious metric to measure the monitoring performance between the different conditions would be to compare how many clicks the users made in average for each condition. Furthermore of interest are the buffer values of the respective buffers at the time of the user's interaction with the simulation (e.g., the input buffer of a certain machine at the time of refilling it). A relatively high average buffer value can e.g. signify that the users do not trust that the respective mode of process monitoring conveys the need for interaction in time, leading the users to switching their attention to the process simulation in regular intervals, and performing interactions just in case. A low average buffer can, on the other hand, signify that the users rely on the respective conditions’ ability to signal interaction needs. On the other hand, if e.g. an input buffer had already been completely depleted at the time of intervention, this may signify that the respective condition has failed to inform the users in time. In many cases, participants used double clicks for their interactions, while a single click would have been sufficient, a fact that was perhaps not communicated clearly enough to the participants. Therefore, if several clicks were performed directly one after another, only the first click was taken into account.","['buffer', 'buffers', 'compare how many clicks the users made in average for each condition', 'input buffer', 'simulation']"
S0166218X14003011,"We study sequences of optimal walks of a growing length in weighted digraphs, or equivalently, sequences of entries of max-algebraic matrix powers with growing exponents. It is known that these sequences are eventually periodic when the digraphs are strongly connected. The transient of such periodicity depends, in general, both on the size of digraph and on the magnitude of the weights. In this paper, we show that some bounds on the indices of periodicity of (unweighted) digraphs, such as the bounds of Wielandt, Dulmage–Mendelsohn, Schwarz, Kim and Gregory–Kirkland–Pullman, apply to the weights of optimal walks when one of their ends is a critical node.","['digraph', 'digraphs are strongly connected', 'sequences of entries of max-algebraic matrix powers with growing exponents', 'sequences of optimal walks of a growing length in weighted digraphs', '(unweighted) digraphs']"
S037026930400930X,"Recent publications [31] employ a variety of methods for calculating upper limits and there is no universally accepted procedure [27,32,33]. We choose an approach similar to that first advocated by Feldman and Cousins [27]. This method has been since extended by Conrad et al. [34] to incorporate uncertainties in detector sensitivity and the background estimate based on an approach described by Cousins and Highland [35]. A further refinement of the Conrad et al. method by Hill [36] results in more appropriate behavior of the upper limit when the observed number of events is less than the estimated background, as is the case for the present measurement. We have adopted this method but note that Table 2 contains all of the numbers needed to calculate an upper limit using any of the methods in the papers cited above. We assume that the probability density functions of Fsens and background estimates are Gaussian-distributed.","['background estimate', 'background estimates', 'calculate an upper limit', 'calculating upper limits', 'detector sensitivity', 'further refinement of the Conrad\xa0et\xa0al. method', 'Gaussian-distributed', 'incorporate uncertainties', 'measurement', 'more appropriate behavior', 'probability density functions']"
S221266781300083X,"In this paper, coordination problem of agricultural products supply chain with stochastic yield is studied based on prices compensation strategy. The agricultural producing is influenced by the natural conditions, and the yield is uncertain. While agricultural products is rigid demand goods, the fluctuations of yield cause greater volatility of prices. The two- echelon supply chain with one supplier and one retailor is studied, and the mathematical model is constructed. The model showed that prices compensation strategy is Pareto improvement for agricultural products supply chain with stochastic yield, and it also incentive agricultural products supplier to rise the production plan and balance the profit allocation of supply chain.","['agricultural producing', 'agricultural products', 'agricultural products supply chain', 'coordination problem', 'coordination problem of agricultural products supply chain', 'fluctuations of yield', 'mathematical model', 'mathematical model is constructed', 'prices compensation strategy', 'production plan', 'profit allocation', 'stochastic yield', 'supply chain', 'two- echelon supply chain', 'yield']"
S2212667814000124,"Based on expectation-maximization algorithm, parameter estimation was proposed for data-driven nonlinear models in this work. On this basis, particle filters were used to approximately calculate integrals, deriving EM algorithm based on particle filter. And the effectiveness of using the proposed algorithm for the soft sensor of COx content in tail gas of PX oxidation side reactions was verified through simulation results.","['COx', 'data-driven nonlinear models', 'EM algorithm', 'expectation-maximization algorithm', 'parameter estimation', 'particle filter', 'particle filters', 'PX', 'PX oxidation', 'simulation', 'soft sensor', 'tail gas']"
S0032386109007290,"ELRs are particularly attractive for the synthesis of block copolymers that self-assemble into polymer nanostructures such as micelles. The first work in this area involved an elastin-mimetic di-block copolymer containing VPGEG–(IPGAG)4 and VPGFG–(IPGVG)4 as the hydrophilic and hydrophobic blocks, respectively [49]. The resulting micelles were studied by dynamic light scattering (DLS) and DSC was used to measure the enthalpy of self-assembly. A tri-block copolymer was subsequently synthesized and the TEM images of this polymer showed that it formed spherical aggregates [50]. Other multivalent spherical micelles have been obtained from linear elastin-like AB di-block copolymers in the temperature range 37–42°C with the aim of targeting cancer cells [51]. Bidwell et al. have also exploited the ELRs for its ability to serve as macromolecular carriers for thermally targeted delivery of drugs. Attachment of doxorubicin to ELR-based system showed enhanced cytotoxicity in uterine sarcoma cells when aggregation was induced with hyperthermia [52].","['aggregation', 'block copolymers', 'cytotoxicity', 'DLS', 'doxorubicin', 'DSC', 'dynamic light scattering', 'elastin-mimetic di-block copolymer', 'ELR-based system', 'ELRs', 'hydrophilic and hydrophobic blocks', 'hyperthermia', 'linear elastin-like AB di-block copolymers', 'macromolecular carriers', 'micelles', 'multivalent spherical micelles', 'polymer', 'polymer nanostructures', 'self-assembly', 'spherical aggregates', 'synthesis of block copolymers', 'targeting cancer cells', 'TEM images', 'thermally targeted delivery of drugs', 'tri-block copolymer', 'uterine sarcoma cells', 'VPGEG–(IPGAG)4', 'VPGFG–(IPGVG)4']"
S2212667812000536,"According to the situation that the IT students can not meet the software industry demand for qualified personnel, a “triple-driven” three-dimensional software development practical teaching system was proposed, aiming to improve the software development capabilities and innovation sense of students. This system can effectively improve students the interest of software development and the practical skills and sense of innovation, laying a solid foundation for student after graduation to rapidly integrate into the software development process, meeting the needs of software industry.","['improve the software development capabilities and innovation sense of students', 'practical skills', 'software development', 'software development process', 'software industry', '“triple-driven” three-dimensional software development practical teaching system']"
S0370269304007439,"In contrast to the H particle, the situation for the Θ+ baryon is very promising. Thus, in this Letter we explore the formation of the Θ+-baryon within a new approach called parton-based Gribov–Regge theory. It is realized in the Monte Carlo program NEXUS 3.97 [22,23]. In this model high energy hadronic and nuclear collisions are treated within a self-consistent quantum mechanical multiple scattering formalism. Elementary interactions, happening in parallel, correspond to underlying microscopic (predominantly soft) parton cascades and are described effectively as phenomenological soft pomeron exchanges. A pomeron can be seen as layers of a (soft) parton ladder, which is attached to projectile and target nucleons via leg partons. At high energies one accounts also for the contribution of perturbative (high pt) partons described by a so-called “semihard pomeron”—a piece of the QCD parton ladder sandwiched between two soft pomerons which are connected to the projectile and to the target in the usual way. The spectator partons of both projectile and target nucleons, left after pomeron emissions, form nucleon remnants. The legs of the pomerons form color singlets, such as q–q̄, q–qq or q̄–q̄q̄. The probability of q–qq and q̄–q̄q̄ is controlled by the parameter Pqq and is fixed by the experimental yields on (multi-)strange baryons [23].","['color singlets', 'Elementary interactions', 'fixed by the experimental yields on (multi-)strange baryons', 'high energy hadronic and nuclear collisions', 'leg partons', 'microscopic (predominantly soft) parton cascades', 'Monte Carlo program', '(multi-)strange baryons', 'NEXUS 3.97', 'nucleon remnants', 'parton-based Gribov–Regge theory', 'perturbative (high pt) partons', 'phenomenological soft pomeron exchanges', 'piece of the QCD parton ladder sandwiched between two soft pomerons which are connected to the projectile and to the target in the usual way', 'pomeron', 'pomeron emissions', 'pomerons', 'projectile and target nucleons', 'q–q̄', 'q–qq', 'q̄–q̄q̄', 'self-consistent quantum mechanical multiple scattering formalism', '“semihard pomeron”', '(soft) parton ladder', 'spectator partons', 'the formation of the Θ+-baryon', 'Θ+ baryon']"
S0021999113005652,"This study proposes a new framework of a numerical modelling of the gas exchange between air and water across their interface, and subsequent chemical reaction in water based on an extended two-compartment model. The major purpose of this study is to provide a fundamental concept for modelling physicochemical processes of the gas exchange, followed by the chemical reaction in water. Demonstrating fundamental data and knowledge on the important environmental transport phenomena, especially the effects of the Schmidt number and the chemical reaction rate on the gas exchange mechanisms across the interface have also been attempted. The gas exchange processes are separated into two physicochemical substeps, the first is the gas–liquid equilibrium between the two phases, and the second is the chemical reaction in the water phase. A first-order, irreversible chemical reaction of the gaseous material after its uptake into the water phase is assumed here to simplify interactions of the chemical reactions and turbulent transport phenomena in water. While a traditional two-compartment model assumes uniform concentration of a material in each compartment, the present two-compartment model uses a computational fluid dynamics (CFD) technique in the water compartment to evaluate temporal development of three-dimensional profiles of the velocity and concentration fields. A direct numerical simulation (DNS) approach is used to evaluate profiles of fluid velocities and concentrations in water, and several important turbulence statistics have been evaluated without using turbulent closures, and subgrid-scale models. We assume that a fluid flow in the water phase is a well-developed turbulent water layer of a low Reynolds number, and the Schmidt number is varied from 1 to 8 to observe the effects of the molecular diffusion of the gas in sub-interface water on the gas exchange rate at the interface. Six degrees of the nondimensional chemical reaction rate are used to find the effect of the chemical reaction rate on the gas exchange mechanisms. Extrapolations of the gas exchange rates and the related transport phenomena toward larger Schmidt number and the faster chemical reaction rate will also be examined to predict the gas exchange processes of the actual gases of Sc∼O(102) based on results from the present numerical experiments.","['air', 'CFD', 'chemical reaction', 'chemical reaction in the water phase', 'chemical reaction in water', 'chemical reaction rate', 'chemical reactions', 'computational fluid dynamics', 'concentrations in water', 'direct numerical simulation', 'DNS', 'effect of the chemical reaction rate on the gas exchange mechanisms', 'effects of the Schmidt number', 'evaluate profiles of fluid velocities and concentrations in water', 'evaluate temporal development of three-dimensional profiles of the velocity and concentration fields', 'fluid flow in the water phase', 'fluid velocities', 'framework', 'fundamental concept', 'gas', 'gaseous material', 'gases', 'gas exchange', 'gas exchange mechanisms', 'gas exchange mechanisms across the interface', 'gas exchange processes', 'gas exchange rate at the interface', 'gas exchange rates', 'gas–liquid equilibrium', 'irreversible chemical reaction', 'liquid', 'modelling physicochemical processes', 'molecular diffusion', 'new framework of a numerical modelling', 'nondimensional chemical reaction rate', 'numerical experiments', 'observe the effects of the molecular diffusion', 'predict the gas exchange processes', 'Sc∼O(102)', 'subgrid-scale models', 'transport phenomena', 'turbulent transport phenomena in water', 'two-compartment model', 'water', 'water phase']"
S0370269304006070,"The purpose of this Letter is to answer the above question and to confront those six-zero textures of lepton mass matrices with the latest experimental data. First, we shall present a concise analysis of the lepton mass matrices in Table 1 and reveal their isomeric features, namely, they have the same phenomenological consequences, although their structures are apparently different. Second, we shall examine the predictions of these lepton mass matrices by comparing them with the 2σ and 3σ intervals of two neutrino mass-squared differences and three lepton flavor mixing angles,22To be specific, we make use of the 2σ and 3σ intervals of two neutrino mass-squared differences and three lepton flavor mixing angles given by M. Maltoni et al. in Ref. [5]. which are obtained from a global analysis of the latest solar, atmospheric, reactor (KamLAND and CHOOZ [10]) and accelerator (K2K) neutrino data. We find no parameter space allowed for six isomeric lepton mass matrices at the 2σ level. At the 3σ level, however, their results for neutrino masses and lepton flavor mixing angles can be compatible with current data. Third, we incorporate the seesaw mechanism and the Fukugita–Tanimoto–Yanagida hypothesis [9] in the charged lepton and Dirac neutrino mass matrices with six texture zeros. It turns out that their predictions, including θ23≈45°, are in good agreement with the present experimental data even at the 2σ level.","['charged lepton', 'confront those six-zero textures of lepton mass matrices', 'current data', 'Dirac neutrino mass matrices', 'examine the predictions of these lepton mass matrices', 'experimental data', 'Fukugita–Tanimoto–Yanagida hypothesis', 'global analysis', 'isomeric lepton mass matrices', 'lepton flavor mixing angles', 'lepton mass matrices', 'neutrino', 'neutrino mass-squared differences', 'reveal their isomeric features', 'seesaw mechanism', 'solar, atmospheric, reactor (KamLAND and CHOOZ [10]) and accelerator (K2K) neutrino data']"
S0375960113004908,"Topological insulators (TIs) are promising candidates of spintronics materials because of their robust helical surface states and the extremely strong spin–orbit interaction [1–3]. Initially, binary chalcogenides Bi2Te3, Sb2Te3 and Bi2Se3 have been identified as three-dimensional TIs by surface sensitive probes such as angle resolved photoemission spectroscopy and scanning tunneling microscopy/spectroscopy. Later, ternary chalcogenide (BixSb1−x)2Te3 [4,5], which has similar tetradymite structure to the parent compounds Bi2Te3 and Sb2Te3, was predicted by ab initio calculations and confirmed by ARPES measurements as a tunable topological insulator whose Fermi energy and carrier density can be adjusted via changing the Bi/Sb composition ratio with stable topological surface state for the entire composition range. Combined with magnetism or superconductivity, TIs have attracted great attention due to the rich variety of new physics and applications. The ferromagnetism in several transition metal (TM) doped TIs, which breaks the time-reversal symmetry, has been reported [6–13]. Ferromagnetism in TIs is important because the combination of magnetism with TIs makes a good platform to study fundamental physical phenomena, such as the quantum anomalous Hall effect [14–17], Majorana fermions [18], image magnetic monopole effect [19], and topological contributions to the Faraday and Kerr magneto-optical effect [20].","['ab initio calculations', 'angle resolved photoemission spectroscopy', 'ARPES measurements', 'Bi2Se3', 'Bi2Te3', 'binary chalcogenides', '(BixSb1−x)2Te3', 'changing the Bi/Sb composition ratio', 'Faraday and Kerr magneto-optical effect', 'ferromagnetism', 'Ferromagnetism', 'fundamental physical phenomena', 'Hall effect', 'image magnetic monopole effect', 'magnetism', 'Majorana fermions', 'Sb2Te3', 'scanning tunneling microscopy/spectroscopy', 'spin–orbit interaction', 'spintronics materials', 'superconductivity', 'surface sensitive probes', 'ternary chalcogenide', 'TIs', 'TM', 'topological insulator', 'Topological insulators', 'transition metal']"
S0168365913003295,"A limitation of the pharmacyte approach is the one-time nature of the intervention: ACT T-cells can only be loaded once with a cargo of adjuvant drug prior to transfer, and the duration of stimulation is inherently limited by expansion of the cell population in vivo, since cell-bound particles are diluted with each cell division. We hypothesized that a strategy to target supporting drugs to T-cells with nanoparticle drug carriers directly in vivo would enable transferred lymphocytes to be repeatedly stimulated with supporting adjuvant drugs, and thereby provide continuous supporting signals over the prolonged durations that might be necessary for elimination of large tumor burdens. Such “re-arming” of T-cells with supporting drugs could be achieved by repeated administration of targeted particles, allowing adoptively-transferred T-cells to be restimulated multiple times directly in vivo, while the use of internalizing targeting ligands would minimize the likelihood of immune responses against the nanoparticle carrier. To our knowledge, only two prior studies have attempted to target nanoparticles to T-cells in vivo [17,18]. In both of these studies, particles were targeted to T-cells via peptide-MHC ligands that bind to specific T-cell receptors. However, peptide-MHC-functionalized nanoparticles have recently been shown to deliver an anergizing/tolerizing signal to T-cells [18,19] — which is ideal for treating graft rejection or autoimmunity, but runs counter to the goals of cancer immunotherapy.","['ACT T-cells', 'adjuvant drug', 'administration of targeted particles', 'adoptively-transferred T-cells', 'cancer immunotherapy', 'cell', 'cell-bound particles', 'cell division', 'elimination of large tumor burdens', 'expansion of the cell population in vivo', 'internalizing targeting ligands', 'nanoparticle carrier', 'nanoparticle drug carriers', 'nanoparticles', 'peptide-MHC-functionalized nanoparticles', 'peptide-MHC ligands', 'pharmacyte approach', '“re-arming” of T-cells with supporting drugs', 'stimulation', 'supporting adjuvant drugs', 'supporting drugs', 'targeted particles', 'T-cell receptors', 'T-cells', 'transferred lymphocytes', 'treating graft rejection or autoimmunity']"
S0370269304008305,Correlation of charm-quark–charm-antiquark in γp scattering are calculated in the kt-factorization approach. We apply different unintegrated gluon distributions (uGDF) used in the literature. The results of our calculations are compared with very recent experimental results from the FOCUS Collaboration. The CCFM uGDF developed recently by Kwieciński et al. gives a good description of the data. New observables are suggested for future studies. Predictions and perspectives for the HERA energies are presented.,"['antiquark', 'CCFM uGDF', 'charm', 'Correlation of charm-quark–charm-antiquark in γp scattering', 'HERA energies', 'kt-factorization approach', 'quark', 'uGDF', 'unintegrated gluon distributions', 'γp scattering']"
S221267161200176X,"Modeling or approximating high dimensional, computationally-expensive problems faces an exponentially increasing difficulty, the “curse of dimensionality”. This paper proposes a new form of high dimensional model representation (HDMR) by utilizing the support vector regression (SVR), termed as adaptive SVR-HMDR, to conquer this dilemma. The proposed model could reveal explicit correlations among different input variables of the underlying function which is unknown or expensive for computation. Taking advantage of HDMR's hierarchical structure, it could alleviate the exponential increasing difficulty, and gain satisfying accuracy with small set of samples by SVR. Numerical examples of different dimensionality are given to illustrate the principle, procedure and performance of SVR-HDMR.","['adaptive SVR-HMDR', 'alleviate the exponential increasing difficulty', 'approximating', 'computation', 'conquer this dilemma', 'curse of dimensionality', 'explicit correlations', 'HDMR', 'high dimensional model representation', 'illustrate the principle, procedure and performance', 'input variables', 'Modeling', 'Modeling or approximating high dimensional, computationally-expensive problems', 'new form of high dimensional model representation', 'support vector regression', 'SVR', 'SVR-HDMR']"
S2214657115000179,"Aeroengine turbine disks often consist of paramagnetic, that means non-ferromagnetic Nickel based alloys. Sometimes, parasitic small ferromagnetic particles can be included in these disks that may decrease the mechanical stability. For this reason, in case of a suspicion disks are to be analysed with respect to ferromagnetic inclusions. These inclusions generate a magnetic density which can be measured by a flux gate magnetometer using the magnetic remanence method [1]. The detection principle of ferromagnetic impurities in non-magnetic metallic materials is based on their remanence. Before such a measurement can be carried out, the aeroengine turbine disks are premagnetised in axial direction. As ferromagnetic materials show the well-known hysteresis behaviour, those materials can be magnetised by a strong magnetic field which drives the magnetic material into saturation. When removing the magnetic field, the remanence is left. This remaining flux density is used to detect them in non-magnetic materials.","['Aeroengine turbine disks', 'decrease the mechanical stability', 'detection principle of ferromagnetic impurities', 'ferromagnetic inclusions', 'flux gate magnetometer', 'hysteresis behaviour', 'magnetic density', 'magnetic remanence method', 'measurement', 'non-ferromagnetic Nickel based alloys', 'paramagnetic', 'parasitic small ferromagnetic particles', 'premagnetised', 'remaining flux density', 'removing the magnetic field', 'strong magnetic field']"
S0003491615001839,"This work shows how our approach based on the combination of Statistical Mechanics and nonlinear PDEs theory provides us with a novel and powerful tool to tackle phase transitions. This method leads to solution of perhaps the most known test-case that exhibits a first order phase transition (semi-heuristically described) such as the van der Waals model. In particular we have obtained the first global mean field partition function (Eq. (9)), for a system of finite number of particles. The partition function is a solution to the Klein–Gordon equation, reproduces the van der Waals isotherms away from the critical region and, in the thermodynamic limit N→∞ automatically encodes the Maxwell equal areas rule. The approach hereby presented is of remarkable simplicity, has been successfully applied to spin  [17–19,14,16] and macroscopic thermodynamic systems  [20,15] and can be further extended to include the larger class of models admitting partition functions of the form (4) to be used to extend to the critical region general equations of state of the form (7) including a class virial expansions.","['class virial expansions', 'field partition function', 'first order phase transition', 'general equations of state of the form', 'Klein–Gordon equation', 'macroscopic thermodynamic systems', 'Maxwell equal areas rule', 'nonlinear PDEs theory', 'partition function', 'partition functions of the form', 'phase transitions', 'spin', 'Statistical Mechanics', 'thermodynamic limit N→∞', 'van der Waals isotherms', 'van der Waals model']"
S1574119211001544,"As future work on the protocol, we would promote two items. Firstly, the two mobility models that we have considered in this work propose possible way to capture social context in the way nodes move in the physical space, yet still potentially allowing nodes to explore the geographical regions considered in its entirety. Further insights to the performance potential could be given through the assessment of the protocol with other mobilities that can extend the physical region of movement as well as impose potential restrictions on the nodes mobility, for example by forcing similar nodes to move within specifically defined areas. Secondly, the different forwarding modes introduced in Section  3.3 express different levels of cooperation across the network. The push-community mode, for example, is a form of interest-community selfishness and assumes reciprocation in the nodes’ behaviour. The vulnerability (resp. resilience) of the protocol to different instances of node misbehaviours is a research item worth exploring.","['extend the physical region of movement', 'forcing similar nodes to move within specifically defined areas', 'forwarding modes', 'geographical regions', 'impose potential restrictions', 'mobility models', 'network', 'node', 'nodes', 'nodes mobility', 'performance potential', 'physical space', 'protocol', 'push-community mode']"
S0045782511003823,"In the present work we use the mortar finite element method for the coupling of nonconforming discretized sub-domains in the framework of nonlinear elasticity. The mortar method has been shown to preserve optimal convergence rates (see Laursen (2002) [25] for details) and is variationally consistent. We show that the method can be applied to isogeometric analysis with little effort, once the framework of NURBS based shape functions has been implemented. Furthermore, a specific coordinate augmentation technique allows the design of an energy–momentum scheme for the constrained mechanical system under consideration. The excellent performance of the redesigned mortar method as well as the energy–momentum scheme is illustrated in representative numerical examples.In the present work we use the mortar finite element method for the coupling of nonconforming discretized sub-domains in the framework of nonlinear elasticity. The mortar method has been shown to preserve optimal convergence rates (see Laursen (2002) [25] for details) and is variationally consistent. We show that the method can be applied to isogeometric analysis with little effort, once the framework of NURBS based shape functions has been implemented. Furthermore, a specific coordinate augmentation technique allows the design of an energy–momentum scheme for the constrained mechanical system under consideration. The excellent performance of the redesigned mortar method as well as the energy–momentum scheme is illustrated in representative numerical examples.","['constrained mechanical system', 'convergence rates', 'coordinate augmentation technique', 'coupling of nonconforming discretized sub-domains in the framework of nonlinear elasticity', 'energy–momentum scheme', 'framework of NURBS based shape functions', 'isogeometric analysis', 'mechanical system', 'mortar finite element', 'mortar finite element method', 'mortar method', 'optimal convergence rates']"
S1877750313001269,"While virtualization technologies certainly reduce the complexity of using a system, and especially when working across multiple heterogeneous computing environments, they are not widely deployed in high performance computing scenarios. As its name suggest, HPC seeks to obtain maximum performance from computing platforms. Extra software layers impact detrimentally on performance, meaning that in HPC scenarios users typically run the applications as close to the ‘bare metal’ as possible. In addition to the performance degradation introduced by virtualization technologies, choosing what details to abstract in a virtualized interface is itself very important. Grid and cloud computing support different interaction models. In grid computing, the user interacts with an individual resource (or sometimes a broker) in order to launch jobs into a queuing system. In cloud computing, users interact with a virtual server, in effect putting them in control of their own complete operating system. Both of these interaction models put the onus on the user to understand very specific details of the system that they are dealing with, making life difficult for the end user, typically a scientist who wants to progress his or her scientific investigations without any specific usability hurdles obstructing the pathway.","['applications', 'broker', 'cloud computing', 'Grid and cloud computing', 'grid computing', 'high performance computing', 'high performance computing scenarios', 'HPC', 'interaction models', 'queuing system', 'virtualization technologies', 'virtualized interface', 'virtual server']"
S221266781400080X,"Several inorganic flocculating agents, including FeSO4, Al2(SO4)3, FeCl3 and an organic coagulant aid PAM, were used to treat the wastewater from domestic anima and poultry breeding in this paper. The ideal operating conditions were attained by single factor experiment and orthogonal design experiment. And the ideal operating conditions are follows: the dose of FeSO4 and PAM is 135.2mg/L and 0.384mg/L respectively when keeping the pH 10; and the corresponding removal rate is 55% and 60% for COD and turbidity. Based on the experimental results, this paper analyzes the main factors that affect wastewater flocculation treatment.","['Al2(SO4)3,', 'analyzes the main factors', 'COD', 'FeCl3', 'FeSO4', 'inorganic flocculating agents', 'organic coagulant aid', 'orthogonal design experiment', 'PAM', 'removal rate', 'single factor experiment', 'treat the wastewater', 'wastewater flocculation', 'wastewater flocculation treatment']"
S1877750313000240,"A few studies within the physiological domain are of special relevance to this work. These include a performance analysis of a blood-flow LB solver using a range of sparse and non-sparse geometries [21] and a performance prediction model for lattice-Boltzmann solvers [22,23]. This performance prediction model can be applied largely to our HemeLB application, although HemeLB uses a different decomposition technique and performs real-time rendering and visualisation tasks during the LB simulations. Mazzeo and Coveney [1] studied the scalability of an earlier version of HemeLB. However, the current performance characteristics of HemeLB are substantially enhanced due to numerous subsequent advances in the code, amongst others: an improved hierarchical, compressed file format; the use of ParMETIS to ensure good load-balance; the coalesced communication patterns to reduce the overhead of rendering; use of compile-time polymorphism to avoid virtual function calls in inner loops.","['blood-flow LB solver', 'coalesced communication patterns', 'compile-time polymorphism', 'HemeLB', 'HemeLB application', 'lattice-Boltzmann solvers', 'LB simulations', 'ParMETIS', 'performance prediction model', 'rendering']"
S0045782512003428,"The choice of the interpolation functions and support point coordinates for the gradient field is crucial to ensure stability and accuracy of the formulation. For example, nodal integration and NS-FEM are unstable involving the appearance of spurious low-energy modes. They need non-physical penalty energy functions that stabilize them. The articles [2,28] numerically verify the stability, convergence and accuracy of several W2 variants including new elements which can be constructed based on the idea of assumed continuous deformation gradients. For first order hexahedral elements, [2,28] found good results for the element types C3D_8N_27C and C3D_8N_8I. The first is defined by 27 support points and a second order tensor-product interpolation of the deformation gradient by Lagrange polynomials. The latter element type is defined by 16 support points with 8 points being coincident with the nodes and 8 additional points in the element interior. Among the tested first order tetrahedra, the nodally integrated tetrahedron with an additional bubble mode in the gradients was found to be most accurate. It turned out to be even the most efficient with respect to computing time in explicit analysis [28] because the enlarged critical time step compensates the slightly increased numerical cost per restoring force assembly. Fig. 1 illustrates the positions of support points for various CAG and SFEM formulations.","['C3D_8N_27C', 'C3D_8N_8I', 'CAG and SFEM formulations', 'computing time in explicit analysis', 'continuous deformation gradients', 'element types', 'first order hexahedral elements', 'first order tetrahedra', 'interpolation functions', 'nodal integration and NS-FEM', 'nodally integrated tetrahedron', 'non-physical penalty energy functions', 'numerically verify the stability, convergence and accuracy of several W2 variants', 'second order tensor-product interpolation', 'stability and accuracy of the formulation', 'stabilize them', 'support point coordinates for the gradient field', 'W2 variants']"
S0045782513001448,"Traditionally, the simulation of incompressible fluid flow by the SPH method has been through a weakly compressible SPH formulation (WCSPH). In this approach, the pressure is treated as a thermodynamic variable and is calculated using an artificial equation of state. The sound speed is set to be sufficiently high to limit density variations to within a small fraction of the actual fluid density. In practice, this high sound speed places a limitation on the maximum permissible time-step size through the Courant–Friedrichs–Lewy (CFL) constraint. A particular weakness relates to noise in the pressure field since a small perturbation in the local density will yield a large variation in the local pressure. This can make WCSPH formulations ineffective for accurate force and pressure prediction, although recent developments which create more uniform particle distributions have improved this [1,2]. A review of the SPH method can be found in [3] while a review of the classical WCSPH formulation applied to free-surface flows can be found in [4].","['artificial equation of state', 'create more uniform particle distributions', 'fluid', 'fluid flow', 'free-surface flows', 'particle', 'simulation of incompressible fluid flow', 'SPH method', 'WCSPH', 'weakly compressible SPH formulation']"
S0301010413002139,"The vibrational spectra of l-cysteine have been recorded and assigned in both solution [8,9] and the solid state [10–14]. Spectral assignments have been made using empirical force fields [15], Hartree–Fock calculations [10,16,17] based on the isolated molecule approximation. For systems that exhibit strong intermolecular interactions, this approximation often leads to poor agreement between experiment and theory. A striking example is purine [18], where a study of the solid state vibrational spectra by isolated molecule and periodic calculations, gave almost quantitative agreement between theory and experiment for the latter, whereas the former gave only modest agreement and was unable to distinguish between the tautomers. In the present case, where the structure consists of ions linked by hydrogen bonds, periodic calculations based on the complete primitive cell are essential [19]. The only work [20] that includes some solid state effects used molecular dynamics but from which it is difficult to extract assignments. The aim of this paper is to provide a complete assignment of the vibrational spectra of l-cysteine in both the orthorhombic and monoclinic forms by the use of a combination of computational and experimental methods.","['complete primitive cell', 'computational and experimental methods', 'empirical force fields', 'Hartree–Fock calculations', 'hydrogen', 'ions', 'isolated molecule and periodic calculations', 'isolated molecule approximation', 'l-cysteine', 'molecular dynamics', 'periodic calculations', 'provide a complete assignment of the vibrational spectra of l-cysteine', 'purine', 'Spectral assignments', 'study of the solid state vibrational spectra', 'tautomers']"
S0032386109005485,"Inverse miniemulsion polymerization is a water-in-oil (W/O) heterogeneous polymerization process that forms kinetically stable macroemulsions at, below, or around the critical micellar concentration (CMC). This process contains aqueous droplets (including water-soluble monomers) stably dispersed, with the aid of oil-soluble surfactants, in a continuous organic medium. Stable inverse miniemulsions are formed under high shear by either a homogenizer or a high speed mechanical stirrer. Oil-soluble nonionic surfactants with hydrophilic-lipophilic balance (HLB) value around 4 are used to implement colloidal stability of the resulting inverse emulsion. Upon addition of radical initiators, polymerization occurs within the aqueous droplets producing colloidal particles (Fig. 2) [83]. Several reports have demonstrated the preparation of stable particles of hydrophilic and water-soluble polymers [86–89], polyaniline nanoparticles [90], and organic–inorganic hybrid particles [91–93]. This method also allows for the preparation of crosslinked microgels in the presence of difunctional crosslinkers [27,94–100]. In addition, CRP techniques including ATRP [78,79,82,101,102] and RAFT [103] in inverse miniemulsion have been explored to prepare well-defined nanoparticles and nanogels.","['aqueous droplets', 'ATRP', 'CMC', 'colloidal particles', 'critical micellar concentration', 'crosslinked microgels', 'CRP techniques', 'difunctional crosslinkers', 'heterogeneous polymerization process', 'high speed mechanical stirrer', 'homogenizer', 'implement colloidal stability of the resulting inverse emulsion', 'inverse emulsion', 'inverse miniemulsion', 'Inverse miniemulsion polymerization', 'kinetically stable macroemulsions', 'nanogels', 'nanoparticles', 'Oil-soluble nonionic surfactants', 'oil-soluble surfactants', 'organic–inorganic hybrid particles', 'organic medium', 'polyaniline nanoparticles', 'polymerization', 'preparation of crosslinked microgels in the presence of difunctional crosslinkers', 'radical initiators', 'RAFT', 'Stable inverse miniemulsions', 'stable particles of hydrophilic and water-soluble polymers', 'water-in-oil', 'water-soluble monomers', 'W/O']"
S0168365913001521,"The mesoporous silica particles were prepared by the surfactant self-assembly method described previously [18,24]. Briefly, a homogeneous solution of the soluble silica precursor, tetraethylorthosilicate (TEOS; Sigma-Aldrich Corp., St. Louis, MO), and hydrochloric acid was mixed in ethanol and water. A surfactant, cetyltrimethylammonium bromide (CTAB; Sigma-Aldrich Corp., St. Louis, MO), with an initial concentration much less than the critical micelle concentration was added to lower the surface tension of the liquid mixture and act as the mesoporous structure-directing template. Aerosol solutions of soluble silica plus surfactant were then generated with nitrogen as a carrier atomizing gas using a commercially available atomizer (Model 9392A, TSI, Inc., St. Paul, MN). The aerosol droplets were solidified in a tube furnace at 400°C until dry. Once dried, a durapore membrane filter, kept at 80°C, was used to collect the particles. As a final step, the surfactant was removed at 400°C for 5h via calcination. The surface of the mesoporous silica core in these studies was chemically modified with 10wt.% or 15wt.% by aminopropyltriethoxysilane (APTES; Sigma-Aldrich Corp., St. Louis, MO) conducted identically as previously described [17] to create a positive surface charge to increase loading efficiency of negatively charged cargo. Further, Liu and colleagues report the colloidal stability of these protocells with lipid bilayers, excess amount of liposomes (50μg liposomes per 0.5mg silica were used [18]).","['aerosol droplets', 'Aerosol solutions', 'aminopropyltriethoxysilane', 'APTES', 'atomizer', 'atomizing gas', 'calcination', 'cetyltrimethylammonium bromide', 'CTAB', 'durapore membrane filter', 'ethanol', 'homogeneous solution', 'hydrochloric acid', 'liposomes', 'mesoporous silica', 'mesoporous silica core', 'mesoporous structure-directing template', 'nitrogen', 'protocells', 'soluble silica', 'soluble silica precursor', 'surfactant', 'surfactant self-assembly', 'TEOS', 'tetraethylorthosilicate', 'tube furnace', 'water.']"
S0022311514006722,"Zirconium alloys are used as fuel cladding in pressurised and boiling water nuclear reactors. As such these materials are exposed to a large number of environmental factors that will promote degradation mechanisms such as oxidation. At high burn-ups, i.e. extended service life, oxidation and the associated hydrogen pick-up can be a limiting factor in terms of fuel efficiency and safety. The oxidation kinetics for many zirconium alloys are cyclical, demonstrating a series of approximately cubic kinetic curves separated by transitions [1–5]. These transitions are typified by a breakdown in the protective character of the oxide and are potentially linked to a number of mechanical issues. Understanding how these issues influence oxidation is a key to developing a full mechanistic understanding of the corrosion process.","['corrosion process', 'degradation mechanisms', 'developing a full mechanistic understanding of the corrosion process', 'fuel', 'hydrogen pick-up', 'nuclear reactors', 'oxidation', 'oxide', 'Understanding how these issues influence oxidation', 'zirconium alloys', 'Zirconium alloys']"
S2352179115300041,"The main drawback of thermo-oxidation in most actual devices and ITER is its limitation to maintenance periods, when the vessel walls can be heated up around 300–400 °C by hot helium injection through the cooling system [19,20], and also because of the required reconditioning of the walls before plasma operation to remove the absorbed oxygen [10]. However, the temperature achieved is not homogeneous over the vessel, as it is limited to the distance to the cooling tubes, and thus to the device design. The analysis of this study is a continuation of previous works done for the treatment of ITER carbon co-deposits [1–3], so the temperatures studied are in the range of 350 °C for divertor and 200–275 °C for main wall and remote parts. At present, due to budget restrains as well as due to tritium trapped in co-deposited carbon layers, ITER will not use carbon materials at the divertor strike points in spite of their excellent resilience against large heat loads. Nevertheless, many present experimental nuclear fusion devices (DIII-D, TCV, etc.) and new ones (JT-60SA, KSTAR, Wenderstein-7X) use carbon elements, so the removal of carbon co-deposits is still necessary for a better device operation—plasma density control, dust events, etc. The temperatures used in this work are not very different from the ones achievable in present devices, such that the results can be extrapolated to them. Moreover, even for ITER this study could be useful if carbon materials have to be eventually installed in the case that operation with tungsten tiles at the strike points is precluded by unexpected reasons.","['carbon', 'carbon elements', 'carbon layers', 'carbon materials', 'cooling system', 'cooling tubes', 'DIII-D', 'divertor', 'dust', 'helium', 'ITER', 'ITER carbon', 'JT-60SA', 'KSTAR', 'main wall', 'nuclear fusion devices', 'oxygen', 'plasma density control', 'plasma operation', 'reconditioning', 'remote parts', 'removal of carbon co-deposits', 'TCV', 'thermo-oxidation', 'treatment of ITER carbon co-deposits', 'tritium', 'tungsten tiles', 'vessel', 'Wenderstein-7X']"
S0393044012000198,"RemarkThe purely radiative spacetimes used as reference solutions in our analysis are not perturbations of the Minkowski spacetime. A way of seeing this is to consider the Newman–Penrose constants of the spacetime. The Newman–Penrose constants are a set of absolutely conserved quantities defined as integrals of certain components of the Weyl tensor and the Maxwell fields over cuts of null infinity—see [19–21] for the Einstein–Maxwell case. In [22] it has been shown that the value of the Newman–Penrose constants for a vacuum radiative spacetime coincides with the value of the rescaled Weyl spinor at i+—this result can be extended to the electrovacuum case using the methods of this article. For the radiative spacetimes arising from the construction of [17] it can be seen that the value of the Weyl spinor at i+ is essentially the mass quadrupole of the seed static spacetime. It follows, that the Newman–Penrose constants of the radiative spacetime can take arbitrary values. On the other hand, for the Minkowski spacetime, the Newman–Penrose constants are exactly zero, and those of perturbations thereof will be small. Thus, in this precise sense, our radiative spacetimes are, generically, not perturbations of the Minkowski spacetime, unless all the Newman–Penrose constants vanish.","['Maxwell fields', 'Minkowski spacetime', 'Newman–Penrose constants', 'radiative spacetime', 'radiative spacetimes', 'reference solutions', 'vacuum radiative spacetime', 'Weyl tensor']"
S0895611116300684,"In the case of PSR applied to vessels, preservation of high curvature and branches (concavities) demands a high value of the d parameter, resulting in models with high number of polygons. To cope with this problem, Wu et al. (2013) evaluates a variant of PSR (in that work referred to as scale-adaptive [SA]), which includes curvature-dependent polygonization (e.g. increasing/decreasing the size of triangles according to the local curvature) (Wu et al., 2010). In Wu et al. (2013), other methods including MC (without smoothing and decimation) are evaluated with application to vessel modeling. The authors, point at SA as a suitable method for reconstruction of vessels with applications to surgery planning. The methods evaluated by Wu et al. (2013) could be also compared with another set of techniques (known as model-based methods) (Preim and Oeltze, 2008), widely used in the context of vessel modeling for surgery planning.","['curvature-dependent polygonization', 'increasing/decreasing the size of triangles according to the local curvature', 'MC', 'model-based methods', 'PSR', 'PSR applied to vessels', 'reconstruction of vessels', 'SA', 'scale-adaptive', 'surgery planning', 'vessel modeling', 'vessel modeling for surgery planning']"
S1010603009002676,"In this paper, we present our experimental observations on how solvents can vary the TPA and TPF properties of fluorescent rhodamine (Rh) dyes Rh6G, RhB and Rh101. Rhodamines are well-known xanthenes dyes, which have been extensively used for many widespread applications in single-molecule detection [24], DNA-sequence determination [25], fluorescence labelling [26], etc. due to their strong fluorescence over the visible spectral region. Molecular geometries of rhodamine dyes are well-known [27,28] and indicate that all the structures are non-centrosymmetric. In general, for centrosymmetric molecules, TPA is forbidden when tuned to the transitions at one-half of the excitation frequencies. However, for non-centrosymmetric molecules due to symmetry relaxations, the single-photon absorption (SPA) peaks and TPA peaks may coincide. So we set our primary aim to find the effect of solvent polarity on the correlation of SPA and TPA peaks for all the dyes.","['centrosymmetric molecules', 'DNA-sequence determination', 'dyes', 'experimental observations on how solvents can vary the TPA and TPF properties of fluorescent rhodamine (Rh) dyes', 'fluorescence', 'fluorescence labelling', 'fluorescent rhodamine', 'fluorescent rhodamine (Rh) dyes', 'non-centrosymmetric molecules', 'Rh', 'Rh101', 'Rh6G', 'RhB', 'rhodamine dyes', 'Rhodamines', 'single-molecule detection', 'single-photon absorption', 'single-photon absorption (SPA) peaks', 'SPA', 'symmetry relaxations', 'TPA', 'TPA peaks', 'TPF', 'xanthenes dyes']"
S2352179114200032,"Although the presented model is developed and tested with a-C:H layers in mind, it is not necessarily limited to them. Moreover, the only assumptions are chemical reactions between the gas and the solid forming volatiles, the loss of these volatiles from the material and the two stated boundary conditions of gas influx at a single outer surface and the possibility of reactions throughout the bulk. Porosity and significant gas inventories were observed not only for carbon [12] but, e.g. also for beryllium co-deposits [25] and can be expected for other co-deposits formed in plasma devices [1]. Thus, TCR and its description by the presented model may be applicable to all deposits. If a layer has constituents that are not forming volatiles with the reactive gas, e.g. W and Be with O2, these constituents cannot be removed by TCR, as they will not be removed from the deposit. This can influence the removal of other deposit constituents and the time evolution of the process can change. The new understanding of TCR may, for the first time, allow applying the method in a controlled way to nuclear fusion devices, possibly solving the tritium retention issue especially related to carbon based materials.","['a-C:H layers', 'beryllium co-deposits', 'carbon', 'carbon based materials', 'chemical reactions', 'constituents that are not forming volatiles with the reactive gas', 'deposit constituents', 'gas', 'gas influx', 'gas inventories', 'nuclear fusion devices', 'plasma devices', 'Porosity', 'reactions', 'reactive gas', 'single outer surface', 'solid forming volatiles', 'TCR', 'tritium', 'tritium retention', 'tritium retention issue', 'W and Be with O2']"
S0098300413002951,"Hitherto, the investigation of fossil-orientation was only used for the topmost surface of fossil mass occurrences, deposited directly on the sea floor. Due to the fast development of virtual methods (e.g., macro-CT, µ-CT, nano-CT, etc.) it became possible, to investigate the interior orientation of such fossil mass occurrences in three-dimensional detail. Although, a series of paleontological studies deal with 3D-visualization of fossil-elements, no mass occurrence has previously been reconstructed three dimensionally for investigating their interior orientation. This study illustrates an interdisciplinary approach of virtual reconstruction, analyses and interpretation of the interior orientation of an ammonoid mass occurrence. The method established herein produces clear and consistent results using planispirally coiled ammonoid shells – fossils, that so far would have been used only with caution for depositional interpretations. This method can be applied to any kind of fossil mass occurrence, or even other abundant organic elements and particles, to examine their orientation and depositional conditions to conclude on their paleoenvironment, particularly on paleocurrents.","['3D-visualization', 'ammonoid mass occurrence', 'analyses and interpretation of the interior orientation', 'µ-CT', 'depositional interpretations', 'fossil-elements', 'fossil mass occurrence', 'fossil mass occurrences', 'investigating their interior orientation', 'investigation of fossil-orientation', 'macro-CT', 'nano-CT', 'ossil mass occurrences', 'other abundant organic elements', 'particles', 'planispirally coiled ammonoid shells', 'virtual methods', 'virtual reconstruction']"
S0045782515001322,"One of the most important outcomes of the comparative analysis is the fact that in all tested cases the use of FM is associated with a dramatic reduction in computational time when compared with FE, generally being in the order of seconds for FM and in the order of hours for FE. Table 1 reports the timings of the simulations for both methods. Free expansion is the fastest case, where FM reaches the load-free configuration in just 2 s, while simulations inside the vessels with the diameter of around 30 mm take approximately 30 s. Most of the execution time of the FM deployment algorithm is dedicated to the contact check and calculations of the implications the vessel wall has on the stent structure. Interestingly, in both methods, the highest computational time (i.e., curved vessels) is not associated with the most complex geometry (i.e., patient-specific case of aortic dissection). Another fact worth mentioning is the relation of the computational time to the diameter of the vessel in both methods. While the computational time of FM appeared to be directly related to the diameter of the vessel, no immediate relation was found for the FE simulations. Such outcome is probably related to the simplified contact model used by FM, which makes the stent-graft expansion terminate once the nodes come in contact with the vessel wall. On the contrary, it is well known that the contact algorithm used in the FE analyses increases the computational cost of the simulations.","['comparative analysis', 'contact algorithm', 'curved vessels', 'FE', 'FE analyses', 'FE simulations', 'FM', 'FM deployment algorithm', 'nodes', 'simplified contact model', 'simulations', 'stent-graft expansion', 'vessel', 'vessels', 'vessel wall']"
S0039602899010869,"The final contribution to the force is the van der Waals interaction. It includes the following contributions: (i) between the macroscopic Si tip of conical shape with the sphere of radius R at the end [27] and semi-infinite substrate; (ii) the dispersion forces between the atoms in the sample treated atomistically; and (iii) the interaction between the macroscopic part of the tip and the sample atoms. The first contribution is calculated analytically [27]. In fact, the macroscopic contribution to the van der Waals force is the same in each of the three systems described below, as it depends only on the tip–surface separation, macroscopic sphere radius, cone-angle and Hamaker constant of the system [27]. All these quantities are identical in each system we look at, so that the van der Waals force acts as a background attractive force independent of the microscopic properties of the system [8]. The Hamaker constant needed for the calculation of the macroscopic van der Waals force is estimated to be 0.5eV [32].","['atoms', 'calculation', 'dispersion forces', 'macroscopic part of the tip', 'macroscopic Si tip', 'macroscopic van der Waals force', 'sample atoms', 'semi-infinite substrate', 'tip–surface separation', 'van der Waals force', 'van der Waals interaction']"
S0009261414000372,"It is well-known that the optical properties of atoms and molecules can be influenced by their electronic environment. Local field effects on spontaneous emission rates within nanostructured photonic materials for example are familiar, and have been well summarized [1]. Optical processes, including resonance energy transfer are similarly dependent on the local environment of molecular chromophores [2–4]. Many biological systems are known to contain complex organizations of molecules with absorption bands shifted due to the electronic influence of other, nearby optical centres. For instance, in widely studied light-harvesting complexes, there are two identifiable forms of the photosynthetic antenna molecule bacteriochlorophyll, with absorption bands centred on 800 and 850nm; it has been shown that the most efficient forms of energy transfer between the two occurs when there is a neighbouring carotenoid species 5–7. Until now, research on the broader influence of a neighbouring, off-resonant, molecule on photon absorption has mostly centred on the phenomenon of induced circular dichroism, where both quantum electrodynamic (QED) calculations [8–10] and experimental procedures [11–13] predict and verify that a chiral mediator confers the capacity for an achiral acceptor to exhibit circular differential absorption.","['absorption bands', 'achiral acceptor', 'atoms', 'carotenoid species', 'chiral mediator', 'circular differential absorption', 'electronic environment', 'electronic influence', 'energy transfer', 'experimental procedures', 'induced circular dichroism', 'influence of a neighbouring, off-resonant, molecule on photon absorption', 'light-harvesting complexes', 'local environment of molecular chromophores', 'Local field effects', 'molecule', 'molecules', 'nanostructured photonic materials', 'Optical processes', 'optical properties', 'photon', 'photon absorption', 'photosynthetic antenna molecule bacteriochlorophyll', 'QED', 'quantum electrodynamic', 'resonance energy transfer']"
S0379711213001653,"With development of performance-based design, some studies have been conducted on fire risk analysis in buildings from different perspectives and levels. Models such as FiRECAM [11,12] and FiERAsystem [13] were used to calculate the expected life risk. In other studies probabilistic methods have been used to assess levels of people safety in buildings [14]. Quantitative risk analysis approaches have also been used to quantify the risk to occupants using stochastic factors [15]. However, studies to date have largely been concerned with various aspects of fire risk analysis and there has been little in the way of development of systematic theoretical methods for analyzing fire risk in buildings in terms of fire risk management. Existing fire risk management involves the identification of alternative fire safety design options [16,17], the ongoing inspection, maintenance of fire protection systems [18] and evacuation training and drills [19]. In this study, basic process of fire risk analysis in building is described, and a fire risk analysis model based on scenario clusters is established with consideration of the characteristics of fire dynamics and occupants' behavior. The number of deaths and directive property loss are selected as fire risk indices and the average fire risk of residential buildings is quantitatively analyzed, so that appropriate fire risk management measures can be adopted.","['aspects of fire risk analysis', 'average fire risk', 'basic process of fire risk analysis', 'evacuation training and drills', 'FiERAsystem', 'FiRECAM', 'identification of alternative fire safety design options', 'maintenance of fire protection systems', 'Models', 'number of deaths and directive property loss', 'ongoing inspection', 'performance-based design', 'probabilistic methods', 'Quantitative risk analysis approaches', 'stochastic factors', 'systematic theoretical methods']"
S0963869515000572,"Shear horizontal (SH) ultrasound waves are guided waves (they have propagation properties affected by the geometry of the propagation medium), with symmetric and anti-symmetric modes; phase and group speeds are dependent on frequency, sample thickness, and the bulk shear wave speed [11,12]. The properties of the different modes can be very useful, such as in thickness measurement [13], but in this case they are a complication. SH0 has a thickness independent speed, equal to the shear wave speed, and is non-dispersive (the phase and group speed are equal to the shear wave speed for all frequencies). The oscillation direction of SH ultrasound is in the plane of the surface where the wave was generated, and perpendicular to the propagation direction, as shown in Fig. 1, with respect to a reference interface, which is typically a sample surface. Under certain conditions, such as over short propagation distances, SH waves can be treated as bulk waves.","['bulk shear wave speed', 'bulk waves', 'guided waves', 'oscillation', 'phase and group speed', 'propagation', 'SH0', 'Shear horizontal (SH) ultrasound waves', 'shear wave speed', 'SH ultrasound', 'SH waves', 'wave']"
S0021999115003459,"The boundary element method (BEM) has clear advantages when applied to shape optimisation of high-voltage devices, see [4–8] for an introduction to BEM. First of all, BEM relies only on a surface discretisation so that there is no need to maintain an analysis-suitable volume discretisation during the shape optimisation process. Moreover, BEM is ideal for solving problems in unbounded domains that occur in electrostatic field analysis. In gradient-based shape optimisation the shape derivative of the cost functional with respect to geometry perturbations is needed [9–11]. To this purpose, we use the adjoint approach and solve the primary and the adjoint boundary value problems with BEM. The associated linear systems of equations are dense and an acceleration technique, such as the fast multipole method [12,13], is necessary for their efficient solution. For some recent applications of fast BEM in shape optimisation and Bernoulli-type free-boundary problems we refer to [14–16].","['adjoint approach', 'BEM', 'Bernoulli-type free-boundary problems', 'boundary element method', 'electrostatic field analysis', 'fast BEM in shape optimisation', 'fast multipole method', 'gradient-based shape optimisation', 'primary and the adjoint boundary value problems', 'shape derivative', 'shape optimisation', 'shape optimisation of high-voltage devices', 'solving problems in unbounded domains', 'surface discretisation']"
S2212671612002120,A design method for network attack and defense simulation platform is discussed in this paper. Firstly the component and function of the platform are analyzed. Then Visio second development method is used to construct the virtual network topology. The parsing of virtual network topology is also researched and the relative flow sheet is described. Lastly an example is carried out to test performance of the platform. Simulation results show the effectiveness of the proposed method.,"['construct the virtual network topology', 'defense simulation platform', 'design method', 'network attack', 'parsing of virtual network topology', 'platform', 'relative flow sheet', 'test performance', 'test performance of the platform', 'virtual network', 'Visio second development method']"
S2212667812000664,"According to the shortcomings of long time and big errors about the moving plate recognition system, we present the moving plate recognition algorithm based on principal component analysis(PCA) color extraction. On the basis of the analysis of moving plate recognition system's basic principles, it introduces the basic principles and calculation steps about PCA extraction algorithm, and discusses the feasibility of applying the algorithm to PRS in the paper. The experimental results show that the algorithm has the advantages of faster speed and higher accuracy of recognition. The algorithm provides a new thought for the research on the moving plate recognition algorithm.","[""analysis of moving plate recognition system's basic principles"", 'applying the algorithm to PRS', 'color extraction.', 'moving plate recognition', 'moving plate recognition algorithm', 'moving plate recognition system', 'PCA', 'PCA extraction algorithm', 'principal component analysis', 'PRS', 'recognition', 'research on the moving plate recognition algorithm']"
S0009261408017028,"We use open and close aperture Z-scan experiments, in analogy to the saturation absorption work discussed earlier in water [8], to respectively measure the β and n2 for a series of primary alcohols with the help of 1560nm femtosecond laser pulses, however, with the important inclusion of an optical-chopper. The vibrational combination states of the alcohols are coupled by the femtosecond laser pulses at 1560nm. These couplings result in the absorption of 1560nm and the excited molecules undergo relaxation through non-radiative processes, which gives rise to transient thermal effects. These transient thermal effects are related to the pure optical nonlinearity of the samples and can be measured as a change in their n2 values [14]. The transient thermal effects of individual pulses accumulate in case of high repetition-rate lasers to produce a cumulative thermal effect at longer timescales. We measure this cumulative thermal effect with the mode-mismatched two-color pump–probe experiment.","['1560nm femtosecond laser pulses', 'absorption', 'alcohols', 'aperture Z-scan experiments', 'cumulative thermal effect', 'femtosecond laser pulses at 1560nm', 'high repetition-rate lasers', 'measure this cumulative thermal effect', 'mode-mismatched two-color pump–probe experiment', 'molecules', 'non-radiative processes', 'optical-chopper', 'primary alcohols', 'pure optical nonlinearity', 'saturation absorption', 'transient thermal effects', 'vibrational combination states', 'water']"
S2212667814000045,"The opportunity offered by digital technologies to make deep rationalization in purchase of supplies is becoming indispensable in competition between enterprises, considering positive effects in reducing the costs of the companies that have adopted the E-Procurement. As it has been confirmed by numerous case studies, automation of procedures for the purchase through e-procurement technology enables companies to achieve a reduction in costs (average 8-12%) of total purchases. So web-based models are playing a critical role within companies, especially in the generation of value of supply chain. This article focuses on the role of e-procurement within a supply chain showing, through simulations, the advantages and difficulties of implementing a systematic use of the Internet and defining the basic structure of an e-supply chain.","['automation of procedures', 'case studies', 'E-Procurement', 'e-procurement technology', 'e-supply chain', 'generation of value of supply chain', 'rationalization', 'reducing the costs', 'role of e-procurement', 'simulations', 'supplies', 'supply chain', 'web-based models']"
S0022311513011951,"The displacement cascade is a rapid process (of order picoseconds). Further migration of vacancies and SIAs, mainly by diffusion, happens over a timescale of order nanoseconds [17]. This is still short compared to operating times, so is important to consider the equilibrium result of such processes: If the vacancies and SIAs were likely to find their Frenkel partner, recombine, and annihilate, then the metal should essentially return to its original structure; however, if defects instead formed large clusters of a single type this could result in formation of voids, dislocation loops or swelling, possibly weakening the material in the process. Defects can be trapped at grain boundaries or surface, so for an ODS particle to effect the diffusion, there concentration must be such that there are many such particles in each grain.","['defects', 'diffusion', 'dislocation loops', 'displacement cascade', 'grain', 'migration of vacancies and SIAs', 'ODS particle', 'structure', 'swelling', 'voids']"
S0167931711005120,"A nanocomposite system consisting of a semiconducting matrix and embedded ferromagnetic nanostructures has been fabricated. The ferromagnetic characteristics as coercivity, remanence and magnetic anisotropy of the nanocomposite can be adjusted by the electrochemical parameters. Furthermore the spatial distribution of the metal structures within the pores can be varied which means that the magnetic interactions between the particles can be influenced. In the case of densely packed particles within the pores dipolar coupling between them occurs and results in quasi magnetic chains which offer a much larger magnetic anisotropy than non-interacting particles. By modifying the current density small Ni-particles (3–6nm) can be deposited. If the packing density of these particles is sufficiently close, Ni-tubes of a few nanometer in thickness are covering the pore walls. The presented nanocomposite is an interesting system for magnetic applications as magnetic sensor technology. Silicon as substrate renders this composite a good candidate for the integration in existing process technology.","['a good candidate for the integration', 'a much larger magnetic anisotropy', 'A nanocomposite system', 'a semiconducting matrix and embedded ferromagnetic nanostructures', 'coercivity, remanence and magnetic anisotropy of the nanocomposite', 'densely packed particles', 'distribution', 'modifying the current density', 'Ni-tubes', 'non-interacting particles', 'Silicon', 'small Ni-particles (3–6nm)', 'the electrochemical parameters', 'The ferromagnetic characteristics', 'The presented nanocomposite', 'these particles']"
S0885230816300043,"The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure. This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model. However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute. This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show. Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested. This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation.","['aCMLLR transforms', 'adaptive retraining of the GMM–HMM parameters', 'adaptive training', 'aNAT model', 'aNAT procedure', 'factorisation approach using MLLR speaker transforms', 'GMM–HMM model', 'speaker adaptation', 'speaker clustering', 'training show-based aCMLLR transforms']"
S0038092X14004824,"Historically, the interest in accurate measurement of DNI started decades ago. Early studies (e.g., Linke, 1931; Linke and Ulmitz, 1940) identified the difficulty of separating the measurement of DNI from that of the diffuse irradiance in the immediate vicinity of the sun, hereafter referred to as circumsolar irradiance. Pastiels (1959) conducted a detailed study of the geometry of pyrheliometers, and how that geometry interacted with circumsolar radiance, using simplified representations of the latter. Various communications were then presented at a WMO Task Group meeting held in Belgium in 1966 (WMO, 1967) to improve the accuracy of pyrheliometric measurements, including estimates of the circumsolar enhancement. Ångström (1961) and Ångström and Rohde (1966) later contributed to the same topic, followed years later by Major (1973, 1980). The whole issue of instrument geometry vs. circumsolar irradiance was complex and confusing at the time because different makes and models of instruments had differing geometries. This was considerably simplified after WMO issued guidelines about the recommended geometry of pyrheliometers, which led to a relatively “standard” geometry used in all recent instruments. The experimental issues related to the measurement of DNI are discussed in Section 3.2.","['improve the accuracy of pyrheliometric measurements, including estimates of the circumsolar enhancement', 'pyrheliometers', 'separating the measurement of DNI from that of the diffuse irradiance in the immediate vicinity of the sun', 'study of the geometry of pyrheliometers, and how that geometry interacted with circumsolar radiance']"
S2212671612001497,"Our country is rich of line galloping, there are many important galloping data failed to collect systematically and completely because there is no unified management platform. After the galloping occurrence in 2009–2010's winter the department of productive of the State Grid Corporation organized a lot of human to carry out the research of galloping information, this work is time–consuming and inefficient. The State Grid Corporation has used the production management system (PMS) which is a powerful and easy to use. With the help of the system we can create a galloping database which can save resources and storage the galloping data. To build and put it into application of database can provide technical support for line galloping prevention and galloping research work.","['application of database', 'carry out the research of galloping information,', 'collect systematically and completely', 'create a galloping database', 'galloping', 'galloping data', 'galloping occurrence', 'galloping research work', 'line galloping', 'line galloping prevention', 'PMS', 'production management system', 'save resources and storage the galloping data', 'technical support', 'unified management platform']"
S0022311515300295,"Zirconium alloys are commonly used as the fuel cladding for water cooled nuclear fission reactors, mainly due to their low neutron cross-section, good corrosion resistance during normal operating conditions and sufficient mechanical strength [1]. Despite high corrosion resistance at normal operating temperatures (around 300 °C) [2], Zr alloys oxidise very rapidly when exposed to temperatures a few hundred degrees higher. This is an exothermic reaction, which can further accelerate oxidation and, at temperatures beyond 1000 °C, potentially lead to disintegration of the fuel rods, as highlighted during the Fukushima Daiichi nuclear accident. For this reason new research activities have been initiated worldwide to develop accident tolerant fuels (ATF). Additionally, ATFs could also provide further enhancements in corrosion performance during normal operating conditions enabling the development of fuel assemblies for very high burn-up.","['accident tolerant fuels', 'ATF', 'develop accident tolerant fuels', 'development of fuel assemblies', 'disintegration of the fuel rods', 'enhancements in corrosion performance', 'exothermic reaction', 'fuel assemblies', 'fuel cladding', 'fuel rods', 'oxidation', 'oxidise', 'water cooled nuclear fission reactors', 'Zirconium alloys', 'Zr alloys']"
S2212667812000032,"Aspect-oriented Programming (AOP) can well solve the cross-cutting concerns. Because of the different features of aspect, AOP requires new techniques for testing. First, this paper proposes a model to test aspect-oriented software. In order to support the testing model of the first three steps, we propose the algorithm of selecting aspect relevant test cases. Then, we develop a new tool to implement the theoretical of automating select test case. Finally, a case of the Bank Account System is studied to illustrate our testing approach.","['algorithm of selecting aspect relevant test cases', 'AOP', 'Aspect-oriented Programming', 'automating select test case', 'Bank Account System', 'develop a new tool to implement the theoretical of automating select test case', 'model to test aspect-oriented software', 'solve the cross-cutting concerns', 'testing model']"
S0021961414003255,"Moreover, one observes segregation effects by the XRD analysis, which probably took place at high temperature, and were partially quenched to room temperature. The phase analysis showed up to three distinct phases, which should have hence a distinct measurable phase transition temperature, if they crystallise from the liquid on the surface. In the thermograms these effects are not observable as different solidification arrest or clear inflections. The proportion of new appearing phases is small and therefore the latent heat released by this new phase will be also small. The reflected light signal technique only showed one phase change during cooling. As well, the location of this segregation cannot be determined exactly in the molten pool or later in the re-solidified material. At the surface, where the temperature is measured, the material analysis by Raman spectroscopy has not shown signs of segregation, so that also the uncertainties in composition for the phase transition are taken from the uncertainties from the XRD analysis for the most abundant phase at each composition in re-solidified material.","['clear inflections', 'crystallise', 'latent heat', 'liquid', 'material analysis by Raman spectroscopy', 'molten pool', 'phase change', 'phase transition', 'quenched', 'Raman spectroscopy', 'reflected light signal technique', 're-solidified material', 'segregation', 'segregation effects', 'solidification arrest', 'surface', 'thermograms', 'XRD analysis']"
S0167931713002487,"Ge (100) wafers (n- and p-type) were cleaned in ultra high vacuum (<10−6mbar) at 500°C and 600°C for 10min to evaporate any native oxide and so achieve an oxide free surface. Subsequently, wafers were exposed to an Al flux for a range of times to deposit ultrathin Al layers. The samples were then oxidized at ambient temperatures in the MBE load lock to produce Al2O3 layers. The samples were transferred within 1min to an Oxford Instruments OpAL reactor and thin films of HfO2 were deposited on the Al2O3 using atomic layer deposition (ALD). The HfO2 depositions used a [(CpMe)2HfOMeMe] precursor coupled with an O2 plasma as the oxidizing species. Between 30 and 130 ALD cycles were used to grow HfO2 thicknesses from 1.6 to 7nm at 250°C. For electrical measurements, circular gold contacts of area 1.96×10−3cm2 were deposited onto the films to form MOS gate electrodes and Al was deposited on the back of the Ge wafers to provide an ohmic contact. After preliminary measurements, the samples were annealed in forming gas (FGA) at 350°C for 30min. The oxide leakage current was measured using a Keithley 230B voltage source and Keithley 617B electrometer. The HP 4192A low frequency (LF) impedance analyzer at small signal frequencies between 100Hz to 1MHz was used to perform high frequency capacitance–voltage (HF CV) measurements.","['Al', 'Al2O3', 'Al2O3 layers', 'ALD', 'Al flux', 'annealed in forming gas', 'atomic layer deposition', 'circular gold contacts', 'cleaned in ultra high vacuum', '[(CpMe)2HfOMeMe] precursor', 'electrical measurements', 'evaporate any native oxide', 'FGA', 'films', 'forming gas', 'Ge (100) wafers', 'Ge wafers', 'grow HfO2 thicknesses', 'HfO2', 'HfO2 depositions', 'high frequency capacitance–voltage (HF CV) measurements', 'HP 4192A low frequency (LF) impedance analyzer', 'Keithley 230B voltage source', 'Keithley 617B electrometer', 'MBE load lock', 'measured', 'MOS gate electrodes', 'n- and p-type', 'native oxide', 'O2 plasma', 'ohmic contact', 'Oxford Instruments OpAL reactor', 'oxide free surface', 'oxide leakage current', 'oxidized', 'oxidizing species', 'produce Al2O3 layers', 'samples', 'thin films of HfO2', 'to perform high frequency capacitance–voltage (HF CV) measurements', 'ultra high vacuum', 'ultrathin Al layers', 'wafers']"
S2212667812000792,"A process-driven model is presented to build an instinctive and efficient higher educational administrative management system to overcome problems most universities facing. With this model, processes are identified explicitly and the routine of educational administration is broken into small tasks. Each task has designated role of executors. A process describes the activities and relationships among them. A prototype of higher educational administrative system is built with Bonita open solution. The demo shows that the process-driven higher educational administrative system helps end users understand processes they are involved and focus on what to do.","['administrative management system', 'administrative system', 'Bonita open solution', 'educational administration', 'overcome problems', 'process-driven model', 'processes are identified', 'understand processes they are involved']"
S1364815216303541,"As a particular case of survey data, we used the iUTAH “Utah Water Survey,” which was implemented by participating researchers from several Utah institutions of higher education. The objectives of the survey were to document how a representative cross-section of Utah's adult population thinks about water issues. The survey included three core blocks of questions: perceptions of the adequacy of local water supplies, perceptions of the quality of local water resources, and concern about a range of water and non-water issues. A number of additional questions captured information about respondents' familiarity with water cost, lawn-watering behaviors, participation in water based recreation, and demographic attributes. Supplementary material to this paper includes a document with a description of the dataset as a whole, a document containing the complete survey instrument, and two data files containing the results and an associated codebook (see Section 4.3).","['associated codebook', 'awn-watering behaviors', 'complete survey instrument', 'concern about a range of water and non-water issues', 'demographic attributes', 'description of the dataset', 'familiarity with water cost', 'iUTAH “Utah Water Survey,”', 'participation in water based recreation', 'perceptions of the adequacy of local water supplies', 'perceptions of the quality of local water resources', 'Supplementary material', 'survey', ""Utah's adult population thinks about water issues""]"
S0370269304008858,"The reason to investigate the BFKL and DGLAP equations in the case of supersymmetric theories is based on a common belief, that the high symmetry may significantly simplify the structure of these equations. Indeed, it was found in the leading logarithmic approximation (LLA) [10], that the so-called quasi-partonic operators in N=1 SYM are unified in supermultiplets with anomalous dimensions obtained from universal anomalous dimensions γuni(j) by shifting its arguments by an integer number. Further, the anomalous dimension matrices for twist-2 operators are fixed by the superconformal invariance [10]. Calculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized, give even more remarkable results. Namely, it turns out, that here all twist-2 operators enter in the same multiplet, their anomalous dimension matrix is fixed completely by the super-conformal invariance and its universal anomalous dimension in LLA is proportional to Ψ(j−1)−Ψ(1), which means, that the evolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc→∞ are equivalent to the Schrödinger equation for an integrable Heisenberg spin model [11,12]. In QCD the integrability remains only in a small sector of the quasi-partonic operators [13]. In the case of N=4 SYM the equations for other sets of operators are also integrable [14–16]. Evolution equations for quasi-partonic operators are written in an explicitly super-conformal form in Ref. [17].","['anomalous dimension matrices for twist-2 operators', 'anomalous dimension matrix', 'anomalous dimensions obtained from universal anomalous dimensions γuni(j) by shifting its arguments by an integer number', 'Calculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized', 'Evolution equations for quasi-partonic operators', 'evolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc→∞', 'fixed by the superconformal invariance\xa0', 'high symmetry may significantly simplify the structure of these equations', 'investigate the BFKL and DGLAP equations in the case of supersymmetric theories', 'leading logarithmic approximation', 'LLA', 'N=4 SYM', 'super-conformal invariance', 'twist-2 operators']"
S0045782514001947,"Our procedure does not address the issue of how parameterizations can vary for different flow types. However, Edeling et al.  [9] carried out separate calibrations for a set of 13 boundary-layer flows. They summarized this information across calibrations by computing Highest Posterior-Density (HPD) intervals, and subsequently represent the total solution uncertainty with a probability-box (p-box). This p-box represents both parameter variability across flows, and epistemic uncertainty within each calibration. A prediction of a new boundary-layer flow is made with uncertainty bars generated from this uncertainty information, and the resulting error estimate is shown to be consistent with measurement data. This approach is helpful, but it might be extended further by modelling proximity across flows through a distance that would relate to the flow characteristics in order to borrow strength across calibrations instead of splitting the calibrations and then merging the outcomes afterwards. This is a challenging but attractive venue for future research.","['boundary-layer flow', 'boundary-layer flows', 'flow', 'flows', 'Highest Posterior-Density', 'HPD', 'modelling proximity across flows', 'parameterizations', 'p-box', 'probability-box', 'splitting the calibrations and then merging the outcomes']"
S0009261409006666,"We have presented spectrally resolved femtosecond three-pulse photon echo measurements on Zn(II)–OEP, Ni(II)–OEP and Co(II)–OEP. Increased degree of freedom in scans of time delays allows one to separate and extract specific type of spectroscopic information in complex molecules by studying spectral and temporal evolution of the photon echo signals. By varying the population times, population relaxation dynamics and inhomogeneous broadening is revealed in the photon echo spectra. Time-integrated photon echo signals show two different timescales. The electronic relaxation timescale is found to be sub 50fs whereas the timescale for intramolecular vibrational relaxation, occurring in Q00 band, was found to be over a picosecond for Co(II)–OEP and Ni(II)–OEP and within a picosecond for Zn(II)–OEP.","['Co(II)–OEP', 'complex molecules', 'electronic relaxation timescale', 'Increased degree of freedom', 'inhomogeneous broadening', 'intramolecular vibrational relaxation', 'Ni(II)–OEP', 'photon echo signals', 'photon echo spectra', 'population relaxation dynamics', 'population times', 'Q00 band', 'separate and extract specific type of spectroscopic information', 'spectral and temporal evolution of the photon echo signals', 'spectrally resolved femtosecond three-pulse photon echo measurements', 'sub 50fs', 'Time-integrated photon echo signals', 'timescale', 'timescales', 'varying the population times', 'within a picosecond for Zn(II)–OEP', 'Zn(II)–OEP']"
S2212667812000895,"Faced with deficient ability of autonomic learning among learners and low emotional involvement in current web-based instructional environment, here we propose a construct model that is based on inter-subjectivity fusing cognition with emotion to make up for these shortages. Further more, we’ve put the construct model into practice through the online teaching reformation of the quality course apparel production and management.","['autonomic learning', 'construct model', 'fusing cognition with emotion', 'low emotional involvement', 'online teaching reformation', 'propose a construct model', 'web-based instructional environment']"
S0377221716300984,"In this paper, we propose a general agent-based distributed framework where each agent is implementing a different metaheuristic/local search combination. Moreover, an agent continuously adapts itself during the search process using a direct cooperation protocol based on reinforcement learning and pattern matching. Good patterns that make up improving solutions are identified and shared by the agents. This agent-based system aims to provide a modular flexible framework to deal with a variety of different problem domains. We have evaluated the performance of this approach using the proposed framework which embodies a set of well known metaheuristics with different configurations as agents on two problem domains, Permutation Flow-shop Scheduling and Capacitated Vehicle Routing. The results show the success of the approach yielding three new best known results of the Capacitated Vehicle Routing benchmarks tested, whilst the results for Permutation Flow-shop Scheduling are commensurate with the best known values for all the benchmarks tested.","['adapts', 'a direct cooperation protocol', 'agent-based system', 'agents', 'approach', 'Capacitated Vehicle Routing benchmarks', 'deal with a variety of different problem domains', 'each agent is implementing a different metaheuristic/local search combination', 'embodies a set of well known metaheuristics with different configurations', 'framework', 'Good patterns', 'pattern matching', 'Permutation Flow-shop Scheduling', 'Permutation Flow-shop Scheduling and Capacitated Vehicle Routing', 'provide a modular flexible framework', 'reinforcement learning', 'this approach', 'two problem domains']"
S0963869514000954,"Global optimisation algorithms are used in this study to solve the optimisation problem as they are known to be efficient in incorporating statistical information and dealing with complicated objective functions that have multiple local minima/maxima. The genetic algorithm (GA) is such a global optimisation technique that mimics biological evolution processes and is used in this particular study. The algorithm starts with a random selection of a population from the decision variable domain (X). The genetic algorithm repeatedly modifies this population. At each step, the algorithm selects a group of individual values from the population (parent) which are evolved through crossover or mutation to produce members of the next generation. This process is repeated for several generations until an optimum solution is reached. See [19] for a fuller description of the GA.","['crossover', 'decision variable domain', 'GA', 'genetic algorithm', 'Global optimisation algorithms', 'global optimisation technique', 'incorporating statistical information', 'local minima/maxima', 'mimics biological evolution processes', 'mutation', 'objective functions', 'parent', 'population', 'random selection of a population', 'selects a group of individual values', 'solve the optimisation problem', 'X']"
S2214657115000155,"MicroCT has been applied to AM parts in various forms. Some preliminary results demonstrating the visualization of defects including porosity in AM components were reported in [6]. In another study, the porosity structures in parts built with improper settings were investigated [7]. In this work, the average porosity ranged from 0.1–0.5%, and large pores were observed which followed the build direction and may be attributed to the electron beam raster and overlap pattern. This was followed by more recent reports of the porosity distribution as a function of build strategy for electron beam melted samples with average porosity < 0.2% [8]. In another study, similar porosity images from microCT were reported at levels above 0.2% average porosity [9,10]. Very recent work reports similar images and may indicate that the porosity structure depends on the build direction [11]. Other applications of the use of microCT to characterize AM parts include the comparison of the part to its design model [12] and the characterization of surface roughness of such parts [13]. In the present work, the aim is to demonstrate a specific type of defect present at very low average porosity levels below 0.01%, and which does not follow the build direction as in some other reported examples. We also demonstrate how this porosity structure changes after Hot Isostatic Pressing (HIP) treatment of the same sample.","['AM parts', 'average porosity', 'build direction', 'characterization of surface roughness', 'characterize AM parts', 'comparison of the part to its design model', 'demonstrate a specific type of defect', 'electron beam melted samples', 'electron beam raster and overlap pattern.', 'HIP', 'Hot Isostatic Pressing', 'microCT', 'MicroCT', 'not follow the build direction', 'porosity distribution', 'porosity images', 'porosity in AM components', 'porosity structure changes', 'porosity structures in parts', 'preliminary results', 'recent reports', 'very low average porosity levels', 'visualization of defects']"
S0370157312000105,"By the early 1970s, and following the ‘golden age’ of general relativity that took place in the 1960s, there was a wide array of candidate theories of gravity in existence that could rival Einstein’s. A formalism was needed to deal with this great abundance of possibilities, and this was provided in the form of the Parameterised Post-Newtonian (PPN) formalism by Kenneth Nordtvedt, Kip Thorne and Clifford Will. The PPN formalism was built on the earlier work of Eddington and Dicke, and allowed for the numerous theories available at the time to be compared to cutting edge astrophysical observations such as lunar laser ranging, radio echo, and, in 1974, the Hulse–Taylor binary pulsar. The PPN formalism provided a clear structure within which one could compare and assess various theories, and has been the benchmark for how theories of gravity should be evaluated ever since. We will give an outline of the PPN formalism, and the constraints available within it today, in Section 2.","['benchmark for how theories of gravity should be evaluated', 'compare and assess', 'cutting edge astrophysical observations', 'general relativity', 'Hulse–Taylor binary pulsar', 'lunar laser ranging', 'Parameterised Post-Newtonian', 'PPN', 'PPN formalism', 'radio echo', 'theories of gravity']"
S037026930400721X,"In the supersymmetric case, such a small coupling for quartic interaction cannot be realized if the potential is lifted by the gauge D-term interactions, since, if so, the coupling constant λ becomes of the order O(g2) where g is the gauge coupling constant in the standard model. Therefore, we focus our attention on the D-flat directions. For D-flat directions, we have to be more careful since behaviors of the potential depend on which flat direction we consider. In the MSSM, Yukawa interactions exist in the superpotential to generate the fermion masses. Such Yukawa interactions lift some of the D-flat directions. In addition, we can also find several D-flat directions which are not affected by the Yukawa interactions associated with the fermion masses; without R-parity violation, such D-flat directions are only lifted by the effects of supersymmetry breaking.33Here, we assume that coefficients of non-renormalizable terms are suppressed enough to be neglected. This may be explained by the R-symmetry, assigning R-charge 23 to each MSSM chiral superfields. (See Ref. [6] for the details.)","['behaviors of the potential', 'coefficients of non-renormalizable terms', 'D-flat directions', 'gauge coupling constant', 'gauge D-term interactions', 'MSSM chiral superfields', 'R-charge 23', 'R-symmetry', 'small coupling', 'small coupling for quartic interaction', 'standard model', 'the coupling constant λ', 'Yukawa interactions']"
S0379711215000223,"The mentioned difficulties associated with the calibration process inspired the concept of inverse modelling. In this case, the experimental data become entirely integrated in the calibration process and an optimization routine is used to quantify the best set of parameters which explain the observed pyrolysis behaviour (i.e. multivariable curve fitting). The most used experimental data for model calibration have been the mass loss rate and the surface temperature [10–12]. The optimization technique used is function of the number of variables and their interactions. In the past, only the few most uncertain parameters (i.e. the kinetics parameters) were generally used as potentiometers [13]. However, sophisticated mathematical procedures have been developed to increase the number of parameters optimized simultaneously (e.g. Genetic Algorithm (GA) [10,14] or Shuffled Complex Evolution (SCE) [11]). Lautenberger and Fernandez-Pello [12] have recently investigated the influence that the choice of algorithm can have on the optimized parameters. They generated using their code GPYRO a set of synthetic data (mass loss rate and surfaces temperature) and tried with different algorithms to find back the set of input parameters. The four optimization algorithms provided results with an absolute average error between 1% and 25%. SCE was the most suitable algorithm. The use of synthetic data conveniently avoids the problem of agreement between the actual physical phenomena and any modelling assumption.","['algorithm', 'algorithms', 'calibration process', 'choice of algorithm', 'experimental data', 'GA', 'Genetic Algorithm', 'GPYRO', 'inverse modelling', 'kinetics parameters', 'mass loss rate', 'mathematical procedures', 'model calibration', 'multivariable curve fitting', 'optimization algorithms', 'optimization routine', 'optimization technique', 'parameters', 'potentiometers', 'pyrolysis behaviour', 'quantify the best set of parameters', 'SCE', 'Shuffled Complex Evolution', 'surface temperature [', 'synthetic data']"
S0022311513001165,"Our simulations confirm experimental observations that W net erosion represents only tiny fraction (in our simulation ∼1%) of the W gross erosion. The estimated upstream W fluxes, FWupstrem, are in good agreement with the experimentally observed values ⩽1019m-2s-1 [16]. Moreover, this value is not very sensitive to the divertor plasma temperature. For low temperatures the energy of D and C ions hitting to the divertor plates is too low to sputter sufficient amount of W. With increasing energy the W sputtering increases, but the potential drop in the divertor plasma increases too. As a result, most of the W atoms are ionized in the vicinity of the divertor and return back to the plates. There are two effects leading to the observed prompt redeposition of W ions: first is the “near-divertor” ionization of W due to low ionization potential −7.86eV (for comparison the ionization potentials for D and C are 13,6 and 10.6eV), second, W+n ions have large Larmor radius ∼2/nmm, so that they are redeposited within the distance of a Larmor radius. Important to note that a significant fraction of W ions escaping this prompt redeposition are returned back due to the friction with the main ions.","['C', 'D', 'D and C ions', 'divertor', 'divertor plasma', 'divertor plates', 'estimated upstream W fluxes', 'experimentally observed values', 'friction with the main ions', 'FWupstrem', 'hitting to the divertor plates', 'increasing energy', 'ionization', 'ionized', 'large Larmor radius', 'near-divertor” ionization', 'observed prompt redeposition', 'plates', 'return', 'simulations', 'W', 'W atoms', 'W fluxes', 'W gross erosion', 'W ions', 'W net erosion', 'W+n ions', 'W sputtering']"
S2212667814000070,"This paper suggests a design of high quality real-time rotation face detection architecture for gesture recognition of smart TV. For high performance rotated face detection, the multiple-MCT(Modified Census Transform) architecture, which is robust against lighting change, was used. The Adaboost learning algorithm was used for creating optimized learning data. The proposed hardware structure was composed of Color Space Converter, Image Resizer, Noise Filter, Memory Controller Interface, Image Rotator, Image Scaler, MCT Generator, Candidate Detector, Confidence Switch, Confidence Mapper, Position Resizer, Data Grouper, Overlay Processor and Color Overlay Processer. As a result, suggested face detection device can conduct real-time processing at speed of at least 30 frames per second.","['Adaboost learning algorithm', 'Candidate Detector,', 'Color Overlay Processer', 'Color Space Converter', 'Confidence Mappe', 'Confidence Switch', 'creating optimized learning data', 'Data Grouper', 'design of high quality real-time rotation face detection architecture for gesture recognition of smart TV', 'face detection', 'gesture recognition', 'hardware structure', 'Image Resizer,', 'Image Rotator', 'Image Scaler', 'MCT', 'MCT Generator', 'Memory Controller Interface', 'Modified Census Transform', 'Noise Filter', 'Overlay Processor', 'Position Resizer,', 'real-time processing', 'rotation face detection architecture']"
S0038092X14004770,"Shading can be the most detrimental factor on performance for a domestic system. The impact of shading on performance varies depending on the electrical series and parallel arrangement of cells within a module and modules within an installed array. Whilst many approaches to shading analysis have been proposed, computational efficiency is not reported despite being of high importance when incorporating shading algorithms into an overall energy yield model. The lack of consideration of the non-linear impacts of shading on smaller systems for example means that the shading loss is significantly underestimated, especially from supposedly small obstacles such as antennas or chimneys. As an example, the system shown in Fig. 1 illustrates the case where the installer may have attested a shade loss factor close to unity under UK microgeneration guidelines (Microgeneration Certification Scheme, 2013), i.e. negligible, but the performance of the system is severely compromised due to the non-linear cell mismatch effects. An effective shading sub-model therefore needs to give feedback to inform decisions of array layout in the proximity of obstructions but must not rely on high power computing.","['antennas', 'cells', 'chimneys', 'domestic system', 'effective shading sub-model', 'energy yield model', 'give feedback to inform decisions of array layout', 'high power computing', 'installed array', 'module', 'modules', 'non-linear cell mismatch effects', 'shade loss', 'shading', 'Shading', 'shading algorithms', 'shading analysis', 'system', 'system shown in Fig. 1']"
S0029549313003439,"An essential part of nuclear reactor analysis is the prediction of the three-dimensional space-time kinetics of neutrons in a relatively large, finite, heterogeneous, three-dimensional reactor core. In a majority of safety analyses the prediction of reactor physics responses is performed using neutron diffusion theory applied to three-dimensional systems, with inputs usually derived from deterministic neutron transport solutions of two-dimensional lattice geometries. There has been increased activity related to uncertainty and sensitivity in reactor physics calculations, and the Organization for Economic Cooperation and Development – Nuclear Energy Agency (OECD-NEA) has sponsored an ongoing benchmark entitled “Uncertainty Analysis in Modelling” (UAM) related to these efforts. The goal of this work is to offer a strategy for computing lattice sensitivities using the DRAGON lattice code and WIMS-D4 multi-group library. Results are presented with comparison to those from TSUNAMI, developed by Oak Ridge National Laboratories.","['deterministic neutron transport solutions', 'DRAGON lattice code', 'neutron', 'neutron diffusion theory applied to three-dimensional systems', 'neutrons', 'nuclear reactor', 'nuclear reactor analysis', 'prediction of reactor physics responses', 'prediction of the three-dimensional space-time kinetics of neutrons', 'reactor', 'reactor core', 'reactor physics calculations', 'safety analyses', 'strategy for computing lattice sensitivities', 'TSUNAMI', 'two-dimensional lattice geometries', 'UAM', 'Uncertainty Analysis in Modelling', 'WIMS-D4 multi-group library']"
S0370269304007695,"In summary, we have shown that one can describe the experimental data of the HERMES Collaboration for hadron attenuation on nuclei without invoking any changes in the fragmentation function due to gluon radiation. In our dynamical studies, that include the most relevant FSI, we employ only the ‘free’ fragmentation function on a nucleon and attribute the hadron attenuation to the deceleration of the produced (pre-)hadrons due to FSI in the surrounding medium. We find that in particular the z-dependence of RMh is very sensitive to the interaction cross section of leading prehadrons and can be used to determine σlead. The interaction of the leading prehadrons during the formation time could be interpreted as an in-medium change of the fragmentation function, which however could not be given in a closed form. The extracted average hadron formation times of τf≳0.3 fm/c are compatible with the analysis of antiproton attenuation in p+A reactions at AGS energies [17]. In an upcoming work we will investigate in detail the spectra for different particle species (π±,K±,p,p̄) to examine, if the formation times of mesons and antibaryons are about equal. In addition we will improve our model to describe the primary photon–nucleon reaction below the PYTHIA threshold of W⩾4 GeV.","['analysis of antiproton attenuation', 'antibaryons', 'antiproton', 'describe the experimental data of the HERMES Collaboration for hadron attenuation on nuclei', 'detail the spectra for different particle species (π±,K±,p,p̄)', 'determine σlead', 'dynamical studies', 'examine, if the formation times of mesons and antibaryons are about equal', 'fragmentation function', '‘free’ fragmentation function', 'FSI', 'gluon radiation', 'hadron', 'hadron attenuation', 'hadron formation', 'improve our model to describe the primary photon–nucleon reaction below the PYTHIA threshold of W⩾4\xa0GeV', 'in-medium change of the fragmentation function', 'interaction of the leading prehadrons during the formation time', 'K±', 'leading prehadrons', 'mesons', 'nuclei', 'nucleon', 'p', 'p̄', 'p+A reactions', 'particle species', 'photon', 'primary photon–nucleon reaction', 'produced (pre-)hadrons', 'RMh', 'surrounding medium', 'the z-dependence of RMh', 'π±']"
S000926141500651X,"Previous studies have shown that there are two main mechanisms for the development of radiation-induced DSBs [16,17]. For γ-ray radiation, single step is the main process to cause DSBs (see Figure 3b), which is attributed to the generation of number of ROS upon the incident of individual photon of γ-ray. Whereas photo-radiation causes DSBs through two step mechanism (Figure 3a) by reflecting that each single photon causes mostly single ROS and thus induces only single strand break. Then, when a second single strand break occurs where near the existing single strand break, DBS is caused, i.e., the two step mechanism. Summarizing the results and discussion we may conclude as that: (1) The significant protective effect of AA against photo-induced damage may reflect the effective diminish of ROS by AA. (2) For the γ-ray induced DSB, the protective effect by AA is a little bit weaker than the case of photo irradiation. This may be due to the generation of numbers of ROS by single photon of γ-ray. Surviving oxygen species against the diminishment effect by AA may cause DSBs. (3) As for the DSBs by ultrasound, damage is caused by the shockwave through the generation of cavitations [18]. Thus, the chemical effect of AA to diminish ROS is considered to be negligibly small for the protection of DSBs.","['AA', 'cavitations', 'DBS', 'development of radiation-induced DSBs', 'diminish ROS', 'DSBs', 'generation of number of ROS', 'photo', 'photo-induced damage', 'photo irradiation', 'photon', 'photon of γ-ray', 'photo-radiation', 'radiation-induced DSBs', 'ROS', 'shockwave', 'single photon of γ-ray', 'single step', 'single strand break', 'strand', 'two step mechanism', 'ultrasound', 'γ-ray induced DSB', 'γ-ray radiation']"
S0010938X1530161X,"A key part of this problem is that an inspector only has access to data from a small inspected area. In this area, there is only one minimum thickness, which does not provide enough information to build a model of the smallest thicknesses. An inspector can generate a sample of the smallest thickness measurements by partitioning the inspection data into a number of equally sized blocks. In each block the minimum thickness is recorded. This set forms a sample of the smallest thickness measurements. From this sample, one can build a model which takes into account the variations of the smallest thickness measurements. Extreme value analysis (EVA) provides a limiting form for this model. It states that, if the underlying thickness measurements in each block are taken from independent and identical distributions, then the sample of minimum thickness measurements will follow a generalized extreme value distribution (GEVD).","['block', 'build a model of the smallest thicknesses', 'equally sized blocks', 'EVA', 'Extreme value analysis', 'generalized extreme value distribution', 'GEVD', 'inspection data', 'model of the smallest thicknesses', 'model which takes into account the variations of the smallest thickness measurements', 'partitioning the inspection data', 'sample']"
S0021999112003579,"We order the discrete unknowns so that the vector of unknowns, xPS=[X,L], contains the nx unknown nodal coordinates, followed by the nb unknown discrete Lagrange multipliers. The linear systems to be solved in the course of the Newton-based solution of Eq. (10), subject to the displacement constraint (9), then have saddle-point structure,(15)where E is the tangent stiffness matrix of the unconstrained pseudo-solid problem, and the two off-diagonal blocks Cxl and Clx=CxlT arise through the imposition of the displacement constraint by the Lagrange multipliers. We refer to [34] for the proof of the LBB stability of this discretisation; see also [35,36] for a discussion of the LBB stability of the Lagrange-multiplier-based imposition of Dirichlet boundary conditions in related problems. We note that during the first step of the Newton iteration, E is symmetric positive definite since it represents the tangent stiffness matrix relative to the system’s equilibrium configuration.","['Clx=CxlT', 'Cxl', 'Dirichlet boundary conditions', 'discretisation', 'displacement constraint', 'equilibrium configuration', 'Lagrange-multiplier-based imposition', 'Lagrange multipliers', 'LBB', 'Newton-based solution', 'Newton iteration', 'nodal coordinates', 'order the discrete unknowns', 'saddle-point structure', 'tangent stiffness matrix', 'two off-diagonal blocks', 'unconstrained pseudo-solid problem', 'vector of unknowns', 'xPS=[X,L]']"
S0957417416303773,"In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen. Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002). In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances. The result is a set of orthogonal vectors sorted in descending order of achieved variance. The first of these vectors is that onto which the variance of the projection of the samples is maximum. In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance. To be rigorous, up to N synthetic orthogonal KPIs may be computed. However, only a small set of them, the first N^, is enough to account for most of the variance of the data.","['a set of orthogonal vectors', 'data mining', 'dimensionality reduction', 'finds the mutually-uncorrelated vectors', 'linear PCA', 'N-dimensional vector space basis', 'N^ synthetic KPIs', 'PCA', 'Principal Component Analysis method', 'the first N^', 'the projection of the samples generates the highest variances', 'the simplest version of PCA', 'the variance of the projection of the samples is maximum', 'up to N synthetic orthogonal KPIs']"
S2212667814001440,"In this paper, we present a tele-operated mobile robot system for old age surveillance. The robot operates in autonomous mode in which the robots navigates in the environment and search for unusual situation of elderly people. If a patient is lying on the floor, the robot informs the user. The user switches the control mode from autonomous to haptic based user control. In the autonomous mode, the robot utilizes the visual sensor and landmarks to monitor the entire environment. The robot is equipped microphone, speaker and monitor making it possible to communicate with the user in remote place. In addition, the robot utilizes the vital sensors to check the patient's condition. The preliminary surveillance experiments show a good performance.","['autonomous mode', 'communicate', 'elderly people', 'informs the user', 'lying on the floor', 'microphone', 'monitor', 'navigates in the environment', 'old age surveillance', 'patient', ""patient's condition"", 'remote place', 'robot', 'search for unusual situation', 'speaker', 'switches the control mode from autonomous to haptic based', 'tele-operated mobile robot system', 'visual sensor and landmarks to monitor', 'vital sensors']"
S0011227514002136,"Measuring and analysing the hold time of the CPA pill allows the thermal boundary resistance within the pill to be assessed; the thermal boundary dictates the actual temperature of the CPA crystals in comparison to the temperature of the cold finger, which is maintained at a constant temperature by a servo control program. Fig. 17 shows the temperature profile during the recycling of the CPA pill and subsequent operation at 200mK. During the hold time, the servo control program maintained the CPA pill temperature to within a millikelvin. It is expected that microkelvin stability can be achieved with fast read-out thermometry (which was not available at the time of testing but which will be used for the mKCC), as this would allow for temperature control on much faster (millisecond) timescales than the current (approximately 1s) thermometry readout used.","['cold finger', 'CPA crystals', 'CPA pill', 'fast read-out thermometry', 'Measuring and analysing the hold time of the CPA pill', 'microkelvin stability', 'mKCC', 'operation at 200mK', 'pill', 'recycling', 'servo control program', 'temperature', 'thermal boundary', 'thermal boundary resistance']"
S2212667814001397,"In this paper, a regression analysis based method is proposed to calculate the Journal Influence Score. This Influence Score is used to measure the scientific influence of scholarly journals. Journal Influence Score is calculated by using various factors in a weighted manner. The Score is then compared with the SCImago Journal Score. The results show that the error is small between the existing and proposed methods, proving that the model is a feasible and effective way of calculating scientific impact of journals.","['calculate the Journal Influence Score', 'calculating scientific impact of journals', 'compared with the SCImago Journal Score', 'Influence Score', 'Journal Influence Score', 'measure the\xa0scientific influence of\xa0scholarly journals', 'regression analysis', 'regression analysis based method', 'scholarly journals', 'scientific influence', 'SCImago Journal Score', 'using various factors in a weighted manner']"
S0370269303017222,"In the bag model and in linear or harmonic oscillator confining potentials, the first excited S-state lies above the lowest P-state, making the predicted Roper mass heavier than the lightest negative parity baryon mass. Pairwise spin-dependent interactions must reverse the level ordering. As mentioned earlier, color-spin interactions fail in this regard [29], while flavor-spin interactions produce the desired effect. Since the q3 color wave function is antisymmetric, the flavor-spin-orbital wave function is totally symmetric. For all quarks in an S-state, the flavor-spin wave function is totally symmetric all by itself and leads to the most attractive flavor-spin interaction. If one quark is in a P-state, the orbital wave function is mixed symmetry and so is the flavor-spin wave function, and the flavor-spin interaction is a less attractive. In the SU(3)F symmetric case, Eq. (1), one obtains mass splittings (2)ΔMχ=−14Cχ,N(939),N∗(1440),−4Cχ,Δ(1232),−2Cχ,N∗(1535). Here we have approximated the N∗(1535) as a state with total quark spin-1/2.","['bag model', 'color-spin interactions', 'flavor-spin interaction', 'flavor-spin interactions', 'flavor-spin-orbital wave function', 'flavor-spin wave function', 'level ordering', 'mass splittings', 'orbital wave function', 'oscillator confining potentials', 'Pairwise spin-dependent interactions', 'q3 color wave function', 'quark', 'quarks', 'quark spin']"
S0306437913000768,"Modeling collaboration processes is a challenging task. Existing modeling approaches are not capable of expressing the unpredictable, non-routine nature of human collaboration, which is influenced by the social context of involved collaborators. We propose a modeling approach which considers collaboration processes as the evolution of a network of collaborative documents along with a social network of collaborators. Our modeling approach, accompanied by a graphical notation and formalization, allows to capture the influence of complex social structures formed by collaborators, and therefore facilitates such activities as the discovery of socially coherent teams, social hubs, or unbiased experts. We demonstrate the applicability and expressiveness of our approach and notation, and discuss their strengths and weaknesses.","['collaboration processes', 'discovery of socially coherent teams', 'evolution of a network of collaborative documents along with a social network of collaborators', 'graphical notation and formalization', 'human collaboration', 'influence of complex social structures', 'modeling approach', 'modeling approaches', 'Modeling collaboration processes']"
S0032386108010392,"Microhardness can be related to other macroscopic mechanical properties such as yield stress, σ, and elastic modulus, E, both derived from compression testing. For work-hardened metals, Tabor derived a direct proportionality between hardness and compressive yield stress: H≈3σ [20]. However, it was soon realized that Tabor's relationship only applies to materials that exhibit full plasticity [9,10]. Deviations from this relationship have been reported for a number of metals, glasses and polymers where the elastic strains are non-negligible [9]. Hence, the different expressions describing the correlation of hardness with conventional macroscopic mechanical properties rely on the validity of the above-mentioned elasto-plastic models. In this way, hardness and yield stress no longer hold direct proportionality but their relationship depends on the specific material properties, such as Poisson's ratio and elastic modulus [9,11–13]. It has been shown that these elasto-plastic models not only satisfactorily explain an H/σ ratio of ≈2 for a number of polyethylene materials of different nature, but also theoretically account for the range of H/E ratios experimentally determined [21].","['compression testing', 'derived a direct proportionality', 'Deviations from this relationship', 'glasses', 'hold direct proportionality but their relationship', 'materials that exhibit full plasticity', 'Microhardness', 'polyethylene materials', 'polymers where the elastic strains', 'specific material properties,', 'the different expressions describing the correlation of hardness with conventional macroscopic mechanical properties', 'work-hardened metals']"
S2212671612001618,"This paper presents a non-fragile controller design method based on system quadratic performance optimization. For the additive controller gain variations, the necessary and sufficient conditions for the existence of non-fragile state feedback controller are given and transformed to the LMI problems, which simplifies the solutions to obtain non-fragile state feedback controllers. The flight control simulation results prove the reliability and validity of the method.","['additive controller gain variations', 'flight control simulation', 'LMI problems', 'non-fragile controller design method', 'non-fragile state feedback controller', 'non-fragile state feedback controllers', 'simplifies the solutions', 'system quadratic performance optimization', 'transformed']"
S0167931712002936,"A 3D finite element based (FEM) COMSOL capacitance analysis is combined with Monte Carlo single-electron circuit simulations to model device operations during single electron detection. The 3D structural data (Fig. 1b) of the nanoscale DQD pair and multiple gate electrodes are precisely input into COMSOL’s FEM-based electrostatics simulator. Capacitances between different device components are then extracted and fed into the well-tested single electron circuit simulator SETSPICE [11], based on the orthodox theory of single electron tunnelling [12]. For our target d1 of 60nm, simulation results (Fig. 1c) showed that as we sweep the voltage applied on gate G1, VG1, single electron tunnelling into the turnstile’s two QDs should generate shifts in the electrometer current, IDS, of tens of pA. This is well within the charge sensitivity of DQD electrometer [6] and consistent to the same order of magnitude with previous work in single electron detection [13]. In addition, the gate to QD capacitive coupling appear to be sufficient for the control of QD occupations down to the single electron limit, allowing for future manipulation of single electron spins in qubit research.","['3D finite element based (FEM) COMSOL capacitance analysis', '3D structural data', 'COMSOL’s FEM-based electrostatics simulator', 'control of QD occupations', 'device components', 'DQD electrometer', 'electrometer current', 'gate', 'gate G1', 'IDS', 'manipulation of single electron spins', 'model device operations during single electron detection', 'Monte Carlo single-electron circuit simulations', 'multiple gate electrodes', 'nanoscale DQD pair', 'orthodox theory', 'QD capacitive coupling', 'qubit research', 'SETSPICE', 'shifts in the electrometer current', 'simulation', 'single electron detection', 'single electron spins', 'single electron tunnelling', 'target d1', 'turnstile’s two QDs', 'VG1', 'well-tested single electron circuit simulator']"
S221450951400031X,"This phase was completed in 2005. Previous contracts had been procured with the contractor providing the detailed design. For this system the design was undertaken by Mott MacDonald. It was developed by looking at the systems installed previously and calculating what was actually required to achieve cathodic protection of the piers. This resulted in a significant reduction in the number of zones and monitoring probes. The varying amounts of steelwork in the beams had previously lead to up to 5 zones per beam, with multiple layers of mesh to achieve the design current density. On review of the data the operating current density was similar in all zones and so this was reduced to a single zone per beam. The encapsulation was susceptible to ASR and contained post tensioning and so it was decided to use a galvanic system based on Galvashield CC anodes from Fosroc. Our design did not include an option to allow depolarization of the galvanic system, but the contractor supplied one, such that the anodes could be remotely disconnected. The control unit was from Electrotech CP and operated via a broadband connection provided by the contractor.","['ASR', 'broadband connection', 'cathodic protection of the piers', 'data', 'depolarization of the galvanic system', 'detailed design', 'encapsulation', 'galvanic system', 'Galvashield CC anodes', 'looking at the systems installed previously and calculating', 'multiple layers of mesh', 'significant reduction in the number of zones and monitoring probes', 'systems installed previously', 'varying amounts of steelwork']"
S0885230816301759,"This paper proposes a sentence stress feedback system in which sentence stress prediction, detection, and feedback provision models are combined. This system provides non-native learners with feedback on sentence stress errors so that they can improve their English rhythm and fluency in a self-study setting. The sentence stress feedback system was devised to predict and detect the sentence stress of any practice sentence. The accuracy of the prediction and detection models was 96.6% and 84.1%, respectively. The stress feedback provision model offers positive or negative stress feedback for each spoken word by comparing the probability of the predicted stress pattern with that of the detected stress pattern. In an experiment that evaluated the educational effect of the proposed system incorporated in our CALL system, significant improvements in accentedness and rhythm were seen with the students who trained with our system but not with those in the control group.","['CALL system', 'predict and detect the sentence stress', 'prediction and detection models', 'provides non-native learners with feedback on sentence stress errors', 'sentence stress feedback system', 'stress feedback provision model']"
S0370269304007701,"Solitons present the possibility of extended objects as stable states within Quantum Field Theory. Although these solutions are obtained from semi-classical arguments in weak coupling limit, their validity as quantal states is justified based on the associated topological conservation laws. A more curious occurrence is that of fermionic zero-energy modes trapped on such solutions. Their presence requires, according to well-known arguments [1,2], an assignment of half-integer fermion number to the solitonic states. In the usual treatment, the back reaction of the fermion zero-modes on the soliton itself is ignored. However, the fractional values of the fermionic charge have interesting consequence for the fate of the soliton if the latter is not strictly stable. The reason for this is that if the configuration were to relax to trivial vacuum in isolation, there is no particle-like state available for carrying the fractional value of the fermionic charge. Dynamical stability of such objects was pointed out in [3], in cosmological context in [4,5] and more recently in [6–8]. Fractional fermion number phenomenon also occurs in condensed matter systems and its wide ranging implications call for a systematic understanding of the phenomenon.","['assignment of half-integer fermion number to the solitonic states', 'associated topological conservation laws', 'carrying the fractional value of the fermionic charge', 'condensed matter systems', 'Dynamical stability of such objects', 'fermionic zero-energy modes trapped on such solutions', 'fractional values of the fermionic charge', 'quantal states', 'semi-classical arguments in weak coupling limit', 'Solitons', 'systematic understanding of the phenomenon', 'the back reaction of the fermion zero-modes on the soliton', 'the fate of the soliton', 'vacuum']"
S0375960113006725,"Observations show that in the same area with dimensions of a few tenths of a parsec could be many sources, some of which only emits OH lines, and some – only lines H2O. The only known in physics the emission mechanism that can give tremendous power within a narrow range of the spectrum, is coherent (i.e. the same phase and direction) light lasers, which are called optical lasers, and radio-masers. Cosmic maser radio sources emitting in the lines of the molecules have an extremely high brightness temperature radiation Tb. In the molecules of methanol masers (CH3OH) Tb value can reach 109 K, with masers hydroxyl molecules (OH) 6×1012 K. The typical size of the maser clusters is about 1014–1015 m and the neutron star radius is of the order of 10 km. Thus, the radiation dilution coefficient is equaled approximately (2.5×10−23)–(2.5×10−21) and, therefore, μB2B2/4(hν)2∼(2.4×10−5)–(2.4×10−7) for the hydrogen line 21 cm and of the order 10−5–10−7 for the OH 18 cm line or the same order as Eq. (1).","['CH3OH', 'Cosmic maser radio sources', 'emission mechanism', 'H2O', 'hydrogen line', 'light lasers', 'maser clusters', 'masers hydroxyl molecules', 'methanol masers', 'neutron star', 'Observations', 'OH', 'OH lines', 'optical lasers', 'radiation dilution coefficient', 'radio-masers']"
S0925838814009669,"SPS has been utilized in several studies to retain the nanostructure of aluminum alloy powders during consolidation. Ye et al. investigated the effect of processing of cryomilled Al 5083 powder via SPS [13]. X-ray Diffraction (XRD) grain size calculations before and after SPS showed that the average grain size of the alloy only increased from 25nm to 50nm (from powder to bulk state). Subsequently, the hardness values obtained through nanoindentation for specimens of AA5083 produced via SPS were highly improved in comparison to conventional sintering methods were grain coarsening takes place on a larger scale. In another study the combination of cryomilling and SPS of AA-5356/B4C nanocomposites powder was found to largely improve the microhardness and flexural strengths of the bulk nanocomposite. Rana et al. [14] investigated the effect of SPS on mechanically milled AA6061 (Al–Mg–Si) micro-alloy powder. The average grain size after 20h of milling was ∼35nm and increased to only ∼85nm after processing with SPS at 500°C. Microhardness and compressive tests were carried out on the consolidated near full density specimens of both unmilled and milled powders and the results showed significant increase in both hardness and compressive strengths for the milled nanocrystalline powders as a result of the very fine grain size.","['AA5083', 'AA-5356/B4C nanocomposites powder', 'alloy', 'Al–Mg–Si', 'aluminum alloy powders', 'bulk nanocomposite', 'combination of cryomilling and SPS of AA-5356/B4C nanocomposites powder', 'consolidation', 'cryomilled Al 5083 powder', 'cryomilling', 'investigated the effect of processing of cryomilled Al 5083 powder', 'investigated the effect of SPS on mechanically milled AA6061 (Al–Mg–Si) micro-alloy powder', 'mechanically milled AA6061', 'mechanically milled AA6061 (Al–Mg–Si) micro-alloy powder', 'milled nanocrystalline powders', 'nanoindentation', 'nanostructure of aluminum alloy powders', 'processing of cryomilled Al 5083 powder', 'retain the nanostructure of aluminum alloy powders during consolidation', 'SPS', 'unmilled and milled powders', 'X-ray Diffraction', 'X-ray Diffraction (XRD) grain size calculations', 'XRD']"
S0022311514005480,"The second stress state is a tri-axial tensile stress designed to represent the zone ahead of an advancing crack tip. Micro-scale lateral cracks have been observed in the oxide layer, and appear to form very close to or at the metal–oxide interface (Fig. 1). Finite element analysis by Parise et al. indicated that these cracks form as a result of localised tensile stresses above peaks in the metal–oxide interface roughness [31]. These cracks are considered separate to any nano-scale cracks that might result from the tetragonal to monoclinic phase transformation. An assumption is made here that whether the micro-scale lateral cracks form via fracture of the oxide or by de-bonding at the interface a triaxial tensile stress state will still be present. In manufactured partially stabilised zirconia cracks would be expected to destabilise the tetragonal phase. This is simulated by applying tensile stress in direction 1, 2 and 3. As this the maximum stress at the crack tip is not known, the applied tensile stresses cover a range from 0.1GPa up to a maximum stress value of 2.2GPa as it is approximately equal to three times the fracture strength of bulk fracture strength for manufactured stabilized zirconia [34]. For the biaxial compressive and triaxial tensile stress states it is the trends in behaviour rather than the absolute values that are considered of greatest importance for this work.","['advancing crack tip', 'applied tensile stresses', 'applying tensile stress', 'biaxial compressive and triaxial tensile stress states', 'cracks', 'de-bonding', 'Finite element analysis', 'fracture', 'localised tensile stresses', 'manufactured partially stabilised zirconia', 'manufactured stabilized zirconia', 'maximum stress', 'metal–oxide interface', 'oxide', 'oxide layer', 'second stress state', 'simulated by applying tensile stress in direction 1, 2 and 3', 'tetragonal phase', 'tetragonal to monoclinic phase transformation', 'tri-axial tensile stress', 'triaxial tensile stress']"
S0045782513000479,"Algorithms regarding distance fields go back to the level set equation. The level set method was presented by Osher and Sethian [20] who described the temporal propagation of moving interfaces by numerical methods solving the Hamilton–Jacobi equation. This is performed by a finite difference scheme working on a rectangular grid in two or three dimensions. Information on normal vectors and curvature can be obtained. The fast marching method [21] provides an efficient numerical scheme of complexity nlogn to compute the support values on the grid. It is a reinterpretation of the propagation process, i.e. the time where the interface passes a certain grid point is influenced only by those neighboring grid points which are previously passed by the interface. An overview on the theory of level set and fast marching methods and their applications to problems of various areas are given in [22,23], for example shape offsetting, computing distances, photolithography development, seismic travel times, etc. Distance fields are a special case of the level set equation where the absolute value of the advection velocity is 1.","['Algorithms regarding distance fields', 'areas', 'complexity nlogn', 'computing distances', 'Distance fields', 'fast marching', 'fast marching method', 'finite difference scheme', 'Hamilton–Jacobi equation', 'level set', 'level set equation', 'level set method', 'numerical methods', 'numerical scheme of complexity nlogn to compute the support values on the grid', 'photolithography development', 'propagation process', 'rectangular grid in two or three dimensions', 'seismic travel times', 'shape offsetting', 'temporal propagation', 'theory of level set', 'the time where the interface passes a certain grid point is influenced only by those neighboring grid points which are previously passed by the interface']"
S0009261415002730,"Both methods of structure solution reveal a bent conformation of the central terthiophene units of the DOTT molecule as is clearly visible in all three cases in Figure 5. However, there is a fundamental difference in the conformation of the octyl side chains. Whilst for the single crystal phase at T=100K linearly extended chains are observed (Figure 5B), a defined rotation of the octyl chains relative to the terthiophene unit is found for the three thin film phases (Figure 5A). The rotation angle of about ±70° results from a twist of the first CC single bond at the link between the terthiophene unit and the octyl chain (see arrows Figure 5A). Two features of this rotated conformation are interesting. First, a molecule with rotated side chains represents the equilibrium state of an isolated single DOTT molecule as obtained by combined MD and VASP calculations [33]. Second, the rotated conformation of the octyl chains allows a dense packing of the octyl side chains for both molecules. Interestingly, the single crystal structure at room temperature shows the twisted as well as the linear conformation of the octyl side chains within one molecule (Figure 5C).","['bent conformation', 'central terthiophene units', 'combined MD and VASP calculations', 'crystal structure', 'defined rotation of the octyl chains', 'dense packing', 'DOTT molecule', 'linearly extended chains', 'octyl chain', 'octyl chains', 'octyl side chains', 'rotated side chains', 'structure solution', 'terthiophene unit', 'thin film', 'twist of the first CC single bond']"
S037596011300741X,"In exploring the WKB limit of quantum theory, Bohm [2] was the first to notice that although one starts with all the ambiguities about the nature of a quantum system, the first order approximation fits the ordinary classical ontology. By that we mean that the real part of the Schrödinger equation under polar decomposition of the wave function becomes the classical Hamilton–Jacobi equation in the limit where terms involving ℏ are neglected. In contrast to this approach, in this Letter we show that the classical trajectories arise from a short-time quantum propagator when terms of O(Δt2) can be neglected. This fact was actually already observed by Holland some twenty years ago: In page 269 of his book [6] infinitesimal time intervals are considered whose sequence constructs a finite path. It is shown that along each segment the motion is classical (negligible quantum potential), and that it follows that the quantum path may be decomposed into a sequence of segments along each of which the classical action is a minimum. The novel contribution of the present Letter is an improved proof of Hollandʼs result using an improved version of the propagator due to Makri and Miller [9,10]. (See also de Gosson [3] for a further discussion.)","['Hamilton–Jacobi equation', 'infinitesimal time intervals', 'propagator', 'Schrödinger equation under polar decomposition of the wave function', 'short-time quantum propagator', 'WKB limit of quantum theory']"
S0021999114008523,"A multi-physics description of a multiscale system is often referred to as a ‘hybrid’ model. In fluid dynamics, a typical hybrid combines a molecular treatment (a ‘micro’ model) with a continuum-fluid one (a ‘macro’ model), with the aim of obtaining the accuracy of the former with the efficiency of the latter [1–4]. The micro and macro models generally have characteristic timescales that are very different, which means that time-accurate simulations can be extremely challenging: the size of the timestep required to make the micro model stable and accurate is so small that simulations over significant macro-scale time periods are intractable. If the system is ‘scale-separated’, a physical (as distinct from numerical) approximation can be made that enables the coupled models to advance at different rates (asynchronously) with negligible penalty on macro-scale accuracy. E et al. [5] were the first to introduce and implement this concept in a time-stepping method for coupled systems, referred to in the classification of Lockerby et al. [6] as a continuous asynchronous (CA) scheme (‘continuous’ since the micro and macro models advance without interruption [5]). In this paper we extend this idea to multiscale systems comprising an arbitrary number of coupled models.","['CA', 'continuous asynchronous', 'continuum-fluid one', 'coupled models', 'fluid', 'fluid dynamics', 'hybrid', '‘hybrid’ model', 'macro’ model', 'micro and macro models', '‘micro’ model', 'molecular treatment', 'multi-physics description of a multiscale system', 'multiscale systems comprising an arbitrary number of coupled models', 'obtaining the accuracy of the former with the efficiency of the latter', 'physical (as distinct from numerical) approximation', 'scale-separated', 'time-stepping method for coupled systems']"
S2212667814001348,"Some nonlinear wave equations are more difficult to investigate mathematically, as no general analytical method for their solutions exists. The Exponential Time Differencing (ETD) technique requires minimum stages to obtain the requiredaccurateness, which suggests an efficient technique relatingto computational duration thatensures remarkable stability characteristicsupon resolving nonlinear wave equations. This article solves the diagonal example of Kawahara equation via the ETD Runge-Kutta 4 technique. Implementation of this technique is proposed by short Matlab programs.","['analytical method', 'computational duration', 'ETD', 'ETD Runge-Kutta 4 technique', 'Exponential Time Differencing', 'investigate mathematically', 'Matlab programs', 'minimum stages', 'nonlinear wave equations', 'resolving nonlinear wave equations', 'solves the diagonal example of Kawahara equation', 'stability characteristicsupon']"
S0010938X15301554,"AA 2024-T3 aluminium alloy is widely used for aerospace applications due to its high strength to weight ratio and high damage tolerance that result from copper and magnesium as the principal alloying elements and appropriate thermomechanical processing. The microstructure of the alloy is relatively complex and a number of compositionally-distinct phases have been identified [1]. Although possessing favourable mechanical properties, the alloy is relatively susceptible to corrosion and generally requires surface treatment in practical applications. The corrosion behaviour of the alloy is particularly affected by the presence of the intermetallic particles due to their differing potentials with respect to the alloy matrix [2–9]. Copper-containing second phase particles at the alloy surface are particularly detrimental to the corrosion resistance as they provide preferential cathodic sites [2,10]. One of the principle types of second phase particle that is important to the corrosion behaviour of the alloy is the S phase (Al2CuMg) particle [1,11]. Dealloying of S phase particles, which may account for ∼60% of the constituent particles in AA2024 alloys [11], is commonly observed when the alloy is exposed to an aggressive environment. The particles are considered as important initiation sites for severe localized corrosion in the alloy [11–22]. The dealloying of the S phase particles and the resulting enrichment of copper result in a decrease of the Volta potential with respect to the matrix and hence the dealloyed particles become active cathodic sites [23–25].","['AA2024 alloys', 'AA 2024-T3 aluminium alloy', 'aerospace applications', 'Al2CuMg', 'alloy', 'alloying elements', 'alloy matrix', 'cathodic sites', 'compositionally-distinct phases', 'constituent particles', 'copper', 'Copper-containing second phase particles', 'corrosion', 'corrosion behaviour', 'corrosion resistance', 'dealloyed particles', 'dealloying', 'decrease of the Volta potential', 'enrichment', 'intermetallic particles', 'magnesium', 'particles', 'second phase particle', 'S phase', 'S phase particles', 'surface treatment', 'thermomechanical processing']"
S0377025714000135,"This conclusion is a consequence of the high jet speeds and small nozzle diameters in combination with the relatively high viscosity solvent and modest molecular weights of the polystyrene, which results in high Weissenberg numbers and moderate values of the extensibility, L studied here. As discussed in earlier papers [3,6], other jetting fluid combinations, such as those of de Gans et al. [4], lie in a different jetting regime where full extension does not occur and relaxation time controls the viscoelastic behaviour. Consequently inkjet fluid assessment methods need to provide a full characterisation including both linear and nonlinear viscoelastic properties. This complexity suggests assessments of inkjet fluids might have to include jetting from sets of DoD print head devices with different sensitivities to all the various VE parameters [37], rather than reliance on testing without jetting. This was not the expected outcome from the present work but does echo the very pragmatic viewpoint expressed as a “map of misery” by Clasen et al. [38] and may provide a way forward for future R&D strategies towards ink testing.","['DoD print head devices', 'inkjet fluid assessment methods', 'inkjet fluids', 'ink testing', 'jetting', 'jetting fluid combinations', '“map of misery”', 'polystyrene', 'provide a full characterisation including both linear and nonlinear viscoelastic properties']"
S1364815216302705,"The main objective of this manuscript is to present and discuss the application of SLAMM to the New York coast. Although the base analysis considers a range of different possible SLR scenarios, the effects of various sources of uncertainties such as input parameters and driving data are not accounted for. In addition, refined and site-specific data are often not available requiring the use of regional data collected from literature and professional judgement in order to run the model. To ignore the effects of these uncertainties on predictions may make interpretation of the results and subsequent decision making misleading since the likelihood and probabilities of predicted outcomes would be unknown. A unique capability of the current version of SLAMM is the ability to aggregate multiple types of input-data uncertainty to create outputs accompanied by probability statements and confidence intervals. Uncertainty in elevation data layers have been considered by several modeling groups to various extents (Gesch, 2009; Gilmer and Ferdaña, 2012; Schmid et al., 2014). However, to the best of our knowledge, no other marsh migration model simultaneously accounts for the combined effects of uncertainty in spatial inputs (DEM, VDATUM, etc.) and parameter choices (accretion rates, tide ranges, etc.) on landcover projections. This added feature of SLAMM allows results to be evaluated in terms of their likelihood of occurrence with respect to input-data and parameter uncertainties. Further, by assigning wide ranges of uncertainty when appropriate, it permits the production of meaningful projections in areas where high-quality local data are not available.","['accretion rates', 'DEM', 'high-quality local data', 'landcover projections', 'marsh migration model', 'parameter choices', 'present and discuss the application of SLAMM to the New York coast', 'SLAMM', 'SLR', 'spatial inputs', 'tide ranges', 'VDATUM']"
S0021999113005718,"Numerical simulation of the gas flow through such non-trivial internal geometries is, however, extremely challenging. This is because conventional continuum fluid dynamics, which assumes that locally a gas is close to a state of thermodynamic equilibrium, becomes invalid or inaccurate as the smallest characteristic scale of the geometry (e.g. the channel height) approaches the mean distance between molecular collisions, λ [1]. An accurate and flexible modelling alternative for these cases is the direct simulation Monte Carlo method (DSMC) [2]. However, DSMC can be prohibitively expensive for internal-flow applications, which typically have a geometry of high-aspect ratio (i.e. are extremely long, relative to their cross-section). The high-aspect ratio creates a formidable multiscale problem: processes need to be resolved occurring over the smallest characteristic scale of the geometry (e.g. a channelʼs height), as well as over the largest characteristic scale of the geometry (e.g. the length of a long channel network), simultaneously.","['accurate and flexible modelling alternative', 'channel height', 'conventional continuum fluid dynamics', 'direct simulation Monte Carlo method', 'DSMC', 'gas flow', 'high-aspect ratio', 'internal-flow applications', 'long channel network', 'molecular collisions', 'multiscale problem', 'Numerical simulation', 'thermodynamic equilibrium']"
S0370269304007257,"Some non-standard couplings, which should be determined here, could also be studied in the standard e+e− option of a linear collider. Therefore, it is worth while to compare the potential power of the two options. As far as the parameter αγ1 is concerned, the γγ collider does not allow for its determination, while it could be determined at e+e−. The second tt̄γ coupling αγ2, which is proportional to the real part of the top-quark electric dipole moment,44See [23] taking into account that the operators OuB, OqB and OqW are redundant. can be measured here. It should be recalled that energy and polar-angle distributions of leptons and b-quarks in e+e− colliders are sensitive only to the imaginary part of the electric dipole moment,55However, it should be emphasized that there exist observables sensitive also to the real part of the top-quark electric dipole moment, see [24]. while here the real part could be determined. For the measurement of γγH couplings, e+e− colliders are, of course, useless, while here, for the bX final state both αh1 and αh2 could be measured. In the case of the decay form factor αd measurement, the e+e− option seems to be a little more advantageous, especially if e+e− polarization can be tuned appropriately [25].","['coupling αγ2', 'e+e−', 'e+e− colliders', 'e+e− option', 'e+e− polarization', 'he real part of the top-quark electric dipole moment', 'imaginary part of the electric dipole moment', 'linear collider', 'measurement of γγH couplings', 'non-standard couplings', 'parameter αγ1', 'polar-angle distributions of leptons and b-quarks', 'potential power', 'standard e+e− option', 'the bX final state', 'the decay form factor αd measurement', 'αh1 and αh2 could be measured', 'γγ collider']"
S0378381215301291,"Recently, fundamental (thermophysical property) research on ionic clathrate hydrates has experienced remarkable growth, particularly over the last ten years [21–30]. Previously, beginning with the first paper on unusual hydrates of tetrabutylammonium salts in 1940 [31], a number of studies could be found on ionic clathrate hydrates (hereafter, semiclathrate hydrates) [32–35] before the unified terminology semiclathrate hydrate was generally accepted. Semiclathrate hydrates have been attracting increased attention because of their promising applications as phase change materials for refrigeration systems and in gas capture and storage [36–41]. In addition, there is interesting speculation that semiclathrate hydrate may be regarded as a representative substance for the study of thermal conductivity in clathrate hydrate in general. This is because: (1) it can reduce characterization problems as a solid sample, since semiclathrate hydrate is formed around ambient temperature under atmospheric pressure and is easy to handle; (2) accurately measuring the thermal conductivity of semiclathrate hydrates, which have many similarities to clathrate hydrates, may make possible a deeper understanding of the unique (anomalous) behavior of the thermal conductivity of clathrate hydrates; and (3) currently, there are no experimental studies on the thermal conductivity of semiclathrate hydrates.","['clathrate hydrates', 'gas capture and storage', 'ionic clathrate hydrates', 'measuring the thermal conductivity', 'phase change materials for refrigeration systems', 'semiclathrate hydrate', 'semiclathrate hydrates', 'Semiclathrate hydrates', 'study of thermal conductivity', 'thermal conductivity', 'unusual hydrates of tetrabutylammonium salts']"
S003238610801080X,"Up to now, morphological studies of the multi-component polymeric materials have been carried out by various microscopic and scattering methods. Optical microscopes, transmission electron microscopes (TEMs), scanning electron microscopes (SEMs) and atomic force microscopes (AFMs) are commercially available and widely used. The biggest advantage of microscopy is that they provide intuitive real-space representations of the various morphologies. However, when it comes to “measurements”, especially in a quantitative way, microscopy sometimes lacks a statistical accuracy due to the small field of view. In contrast, the scattering methods provide much a superior statistical accuracy than that of microscopy simply because the observation volume is larger than that of the microscopes. One must remember, however, that the scattering methods normally require “(hypothesized) models” for data analysis in advance: They do not provide an intuitive insight into the morphologies as does microscopy. After all, for the complete characterization of a specific morphology, one may need to first know the morphologies from the microscopy and subsequently to evaluate the structural parameters by scattering on the basis of the morphology; the two methods are complementary.","['AFMs', 'atomic force microscopes', 'data analysis', '“(hypothesized) models”', 'microscopes', 'microscopic and scattering methods', 'microscopy', 'morphological studies', 'multi-component polymeric materials', 'Optical microscopes', 'scanning electron microscopes', 'scattering methods', 'SEMs', 'TEMs', 'transmission electron microscopes']"
S0370269302014880,"Production of charmonium states J/ψ and ψ′ in nucleus–nucleus collisions has been studied at CERN SPS over the previous 15 years by the NA38 and NA50 Collaborations. This experimental program was mainly motivated by the suggestion [1] to use the J/ψ as a probe of the state of matter created at the early stage of the collision. The original picture [1] (see also [2] for a modern review) assumes that charmonia are created exclusively at the initial stage of the reaction in primary nucleon–nucleon collisions. During the subsequent evolution of the system, the number of hidden charm mesons is reduced because of: (a) absorption of pre-resonance charmonium states by nuclear nucleons (normal nuclear suppression), (b) interactions of charmonia with secondary hadrons (comovers), (c) dissociation of cc̄ bound states in deconfined medium (anomalous suppression). It was found [3] that J/ψ suppression with respect to Drell–Yan muon pairs measured in proton–nucleus and nucleus–nucleus collisions with light projectiles can be explained by the so-called “normal” (due to sweeping nucleons) nuclear suppression alone. In contrast, the NA50 experiment with a heavy projectile and target (Pb+Pb) revealed essentially stronger J/ψ suppression for central collisions [4–7]. This anomalous J/ψ suppression was attributed to formation of quark–gluon plasma (QGP) [7], but a comover scenario cannot be excluded [8].","['absorption of pre-resonance charmonium states by nuclear nucleons', 'anomalous suppression', 'cc̄', 'central collisions', 'charmonia', 'charmonium', 'charmonium states', 'comovers', 'comover scenario', 'dissociation of cc̄ bound states in deconfined medium', 'Drell–Yan muon pairs', 'experimental program', 'formation of quark–gluon plasma', 'hadrons', 'heavy projectile and target', 'hidden charm mesons', 'interactions of charmonia with secondary hadrons', 'J/ψ', 'J/ψ suppression', 'light projectiles', 'NA50 experiment', 'normal nuclear suppression', 'nuclear nucleons', 'nuclear suppression', 'nucleon', 'nucleus', 'nucleus–nucleus collisions', 'original picture', 'Pb+Pb', 'primary nucleon–nucleon collisions', 'Production of charmonium states J/ψ and ψ′ in nucleus–nucleus collisions', 'proton–nucleus', 'QGP', 'quark–gluon plasma', 'subsequent evolution of the system,', 'sweeping nucleons', 'use the J/ψ as a probe of the state of matter created at the early stage of the collision', 'ψ′']"
S0370269304006082,"The aim of this note is nothing more than to bring both approaches on equal footing and to relax the assumptions under which the results of [11,13] have been derived using the first approach. More concretely, we generalize the one-loop partition functions, as derived in [11,13] for levels being odd, to the case of even levels. Moreover, on the level of partition functions we implement additional dressings of the world-sheet parity symmetry and identify them with the dressings introduced in [12] in the crosscap state approach. As expected, all the physical information can be read off entirely from the various amplitudes. We will end up with a collection of very explicit and general one-loop partition functions and tadpole cancellation conditions covering simple current extensions of all 168 Gepner models with additional dressings of the parity symmetry. In fact providing a compact collection of the main relevant formulas for constructing supersymmetric Gepner model orientifolds was one of the motivations for writing this Letter. We hope that these expressions turn out to be useful for a systematic search for Standard-like models respectively for providing a statistical ensemble in the spirit of [29].","['additional dressings of the world-sheet parity symmetry', 'bring both approaches on equal footing', 'constructing supersymmetric Gepner model orientifolds', 'crosscap state approach', 'current extensions of all 168 Gepner models', 'dressings', 'generalize the one-loop partition functions,', 'one-loop partition functions', 'parity symmetry', 'providing a statistical ensemble in the spirit of [29]', 'relax the assumptions', 'systematic search for Standard-like models', 'tadpole cancellation conditions']"
S0009261413004612,"The control of the RP re-encounter probability finds a direct application to improve the performance of chemical devices. Here, we show how a simple-to-implement control scheme highly enhances the sensitivity of a model chemical magnetometer by up to two orders of magnitude. The basic idea behind a chemical magnetometer is that, since a change in the magnetic field modifies the amount of singlet products, one can reverse the reasoning and measure the chemical yield to estimate B. Intuitively, the magnetic sensitivity is high when a small change in the magnetic field intensity produces large effects on the singlet yield. Formally, it is defined as:(2)Λs(B)≡∂Φs(B)∂B=∫0∞pre(t)gs(B,t)dt,with gs(B,t)≡∂fs(B,t)∂B being the instantaneous magnetic sensitivity. The functional form of fs(B,t)=Sρel(t)S strongly depends on the specific realization of the radical pair, in particular on the number of the surrounding nuclear spins. Here, we consider a radical pair in which the first electron spin is devoid of hyperfine interactions, while the second electron spin interacts isotropically with one spin-1 nucleus, e.g. nitrogen. In the context of the chemical compass (i.e. when the task is determining the magnetic field direction through anisotropic hyperfine interactions), an analogous configuration (with only one spin-1/2 nucleus) has been proposed [3], and numerically characterized [8], as being optimal: Additional nuclear spins would perturb the intuitive ‘reference and probe’ picture. The Hamiltonian then simplifies to H=-γeB(S1(z)+S2(z))+|γe|αS→2·I→, where α is the isotropic hyperfine coupling.","['(2)Λs(B)≡∂Φs(B)∂B=∫0∞pre(t)gs(B,t)dt', 'analogous configuration', 'anisotropic hyperfine interactions', 'B', 'chemical compass', 'chemical devices', 'chemical magnetometer', 'chemical yield', 'control scheme', 'determining the magnetic field direction through anisotropic hyperfine interactions', 'electron spin', 'gs(B,t)≡∂fs(B,t)∂B', 'hyperfine interactions', 'H=-γeB(S1(z)+S2(z))+|γe|αS→2·I→', 'improve the performance of chemical devices', 'instantaneous magnetic sensitivity', 'isotropic hyperfine coupling', 'magnetic field', 'magnetic field direction', 'magnetic field intensity', 'magnetic sensitivity', 'model chemical magnetometer', 'nitrogen', 'nuclear spins', 'realization of the radical pair', 'RP re-encounter probability', 'singlet products', 'singlet yield', 'spin-1/2 nucleus', 'spin-1 nucleus', 'α']"
S0009261412012365,"Under these experimental conditions, the observed dynamics has to occur where the probe laser induces the reactions resulting in further ionization [30]. The two-step decay model [26] was applied to explain the above-mentioned fragmentation of DCPD to CPD, shown in Figure 8a. The fitting of the rise and decay components of the transients were done by Matlab® programming using the curve fitting Levenberg–Marquardt algorithm. The best fit decay constants for the biexponential decay components of C10H12+ ion signal is τ1=35fs and τ2=240fs, while that for C5H6+ ion signal is τ1=36fs and τ2=280fs, respectively. These decay constants conform to the previously reported time constants of norbornene and norbornadiene [22,23]. The transients of the reaction fragment C5H6+ are sufficiently different from that of the parent ion C10H12+ indicating that we are studying the distinct dynamics of the neutrals and not that of the parent ion fragmentation [24]. Applying laser control principles under such experimental circumstances also confirms that we are controlling the product yield of C5H6+, resulting from the photochemical reaction of DCPD.","['Applying laser control principles', 'best fit decay constants', 'biexponential decay components', 'C10H12+', 'C10H12+ ion signal', 'C5H6+', 'C5H6+ ion signal', 'controlling the product yield of C5H6+', 'CPD', 'curve fitting Levenberg–Marquardt algorithm', 'DCPD', 'decay constants', 'distinct dynamics of the neutrals', 'fragmentation of DCPD to CPD', 'further ionization', 'laser control principles', 'Matlab® programming', 'norbornadiene', 'norbornene', 'observed dynamics', 'parent ion', 'parent ion fragmentation', 'photochemical reaction of DCPD', 'probe laser', 'reactions', 'rise and decay components', 'transients', 'two-step decay model', 'τ1=35fs', 'τ1=36fs', 'τ2=240fs', 'τ2=280fs']"
S0045782515001899,"In recent years, the Discontinuous Galerkin (DG) method has emerged as a more thorough alternative for locally solving conservation laws of the shallow water equations with higher accuracy  [21–27]. The DG method further involves finite element weak formulation to–inherently from conservation principles–shape a piecewise-polynomial solution over each local discrete cell, via local basis functions. On this basis, the DG polynomial accuracy is spanned by a set of coefficients, describing accuracy information, which are all locally evolved in time from conservation principles at the discrete level, with an arbitrary order of accuracy. A DG-based shallow water model appeals in providing higher quality solutions on very coarse meshes than a traditional finite volume counterpart, but is comparatively expensive to run and imposes a more restrictive stability condition for the CFL number  [28,29].","['CFL number', 'conservation principles', 'DG', 'DG-based shallow water model', 'DG method', 'DG polynomial accuracy', 'Discontinuous Galerkin', 'finite element weak formulation', 'local basis functions', 'shallow water equations', 'shape a piecewise-polynomial solution over each local discrete cell', 'solving conservation laws of the shallow water equations', 'very coarse meshes']"
S0254058415300766,"Half metallic ferromagnets (HMF) have attracted enormous interest due to their applications in spintronic devices [1]. Dilute magnetic semiconductors (DMSs) are considered to be the best materials to show half metallicity. These materials have two components, one being a semiconducting material with diamagnetic properties while the other is a magnetic dopant such as transition metal having un-paired d electrons [2]. The major advantage of these materials is utilization of electron's spin as information carrier since advanced functionalities in spintronic devices can be viable by the use of spin degree of freedom along with the charge of electrons [3]. The major issue regarding the applicability of these materials is to enhance the Curie temperature above room temperature. That's why the research interest shifted towards large band gap materials. A lot of work has been reported on DMSs with different II–VI and III–V semiconductors as host material such as, ZnS, CdS, GaN, ZnO, ZnSe, ZnTe, TiO2, SnO2 [4–12].","['CdS', 'Dilute magnetic semiconductors', 'DMSs', 'electrons', 'enhance the Curie temperature above room temperature', 'GaN', 'Half metallic ferromagnets', 'HMF', 'host material', 'II–VI and III–V semiconductors', 'large band gap materials', 'magnetic dopant', 'materials to show half metallicity', 'semiconducting material', 'SnO2', 'spintronic devices', 'TiO2', 'transition metal', 'un-paired d electrons', ""utilization of electron's spin as information carrier"", 'ZnO', 'ZnS', 'ZnSe', 'ZnTe']"
S0165212511000862,"In this paper we construct such a physical model with a continuous distribution of relaxations. It is based on the phenomenological theory of relaxation processes which have a long history in physics literature and was recently summarized in a monograph in which references to other relevant publications can be found, [24]; also see [25]. The present work is confined to relaxation mechanisms which result from changes in normal stresses. More specifically, we are interested in the local mechanisms of irreversible energy loss caused by uniform compression or expansion of a medium for which all components remain unchanged, rather than the losses caused by friction between different layers of a medium which move with different velocities (for a more detailed discussion of this issue see [26]). No attempt is made to model effects of shear viscosity and heat conduction beyond the conventional Navier–Stokes approach, since this topic goes far beyond the scope of this paper.","['compression or expansion of a medium', 'continuous distribution of relaxations', 'friction between different layers of a medium', 'heat conduction', 'irreversible energy loss', 'local mechanisms of irreversible energy loss', 'Navier–Stokes approach', 'phenomenological theory of relaxation processes', 'physical model', 'relaxation mechanisms', 'relaxation processes', 'relaxations', 'shear viscosity']"
S0167931714000203,"To restrict pollen tube growth to a single focal plane is an important subject to enable their accurate growth analysis under microscopic observation. In the conventional method to assay pollen tube growth, the pollen tubes grow in a disorderly manner on solid medium, rendering it impossible to observe their growth in detail. Here, we present a new method to assay pollen tube growth using poly-dimethylsiloxane microchannel device to isolate individual pollen tubes. The growth of the pollen tube is confined to the microchannel and to the same focal plane, allowing accurate microscopic observations. This methodology has the potential for analyses of pollen tube growth in microfluidic environments in response to chemical products and signaling molecules, which paves the way for various experiments on plant reproduction.","['accurate microscopic observations', 'analyses of pollen tube growth', 'assay pollen tube', 'assay pollen tube growth', 'assay pollen tube growth,', 'chemical products', 'experiments on plant reproduction', 'growth analysis under microscopic observation', 'isolate individual pollen tubes', 'new method to assay pollen tube growth', 'observe their growth in detail.', 'plant reproduction', 'pollen tube', 'pollen tube growth', 'pollen tubes', 'pollen tubes grow', 'poly-dimethylsiloxane microchannel device', 'restrict pollen tube growth to a single focal plane', 'signaling molecules']"
S1524070312000380,"Isogeometric analysis (IGA) is a numerical simulation method which is directly based on the NURBS-based representation of CAD models. It exploits the tensor-product structure of 2- or 3-dimensional NURBS objects to parameterize the physical domain. Hence the physical domain is parameterized with respect to a rectangle or to a cube. Consequently, singularly parameterized NURBS surfaces and NURBS volumes are needed in order to represent non-quadrangular or non-hexahedral domains without splitting, thereby producing a very compact and convenient representation.The Galerkin projection introduces finite-dimensional spaces of test functions in the weak formulation of partial differential equations. In particular, the test functions used in isogeometric analysis are obtained by composing the inverse of the domain parameterization with the NURBS basis functions. In the case of singular parameterizations, however, some of the resulting test functions do not necessarily fulfill the required regularity properties. Consequently, numerical methods for the solution of partial differential equations cannot be applied properly.We discuss the regularity properties of the test functions. For one- and two-dimensional domains we consider several important classes of singularities of NURBS parameterizations. For specific cases we derive additional conditions which guarantee the regularity of the test functions. In addition we present a modification scheme for the discretized function space in case of insufficient regularity. It is also shown how these results can be applied for computational domains in higher dimensions that can be parameterized via sweeping.","['CAD models', 'consider several important classes of singularities of NURBS parameterizations', 'derive additional conditions which guarantee the regularity of the test functions', 'discretized function space', 'discuss the regularity properties of the test functions', 'formulation of partial differential equations', 'Galerkin projection', 'IGA', 'isogeometric analysis', 'Isogeometric analysis', 'modification scheme', 'numerical methods', 'numerical simulation method', 'NURBS', 'NURBS-based representation', 'NURBS-based representation of CAD models', 'NURBS basis functions', 'NURBS objects', 'NURBS surfaces', 'NURBS volumes', 'parameterized', 'physical domain is parameterized', 'present a modification scheme', 'splitting', 'sweeping']"
S0045782513001473,"In this paper, however, we prefer the simpler ‘framed’ cell employed by Hadjiconstantinou and Patera [16], where the shear stress is generated by constraining the velocity in a ‘frame’ rather than by modifying the shape of the box. The framed cell is periodic, but we cannot simply calculate the average stress in the whole box because the presence of an external buffer would produce spurious results. We need the local stress in the core region, but this complicates the Oij term in Eq. (3). There are other methods to calculate the stress tensor such as the method of planes [32], the volume-average approach [26,14], or the method derived from the Schweitz virial relation [25], but, in general, we must choose between a complicated computational cell (i.e. Lees–Edwards cell) and simplifying the calculation of the momentum flux, or a simple cell (i.e. framed cell) and complicating the calculation of the momentum flux. The new method we propose here does not need the direct calculation of the flux, so it avoids this issue altogether: we can use the framed cell and, at the same time, avoid the calculation of the IK equation.","['calculate the stress tensor', 'complicated computational cell', 'constraining the velocity', 'framed cell', 'framed’ cell', 'IK equation', 'Lees–Edwards cell', 'method of planes', 'Schweitz virial relation', 'shear stress', 'simple cell', 'volume-average approach']"
S2212667812000780,"With the development of sport normal students in china, Some ideas to teaching and learning that view learning as a simple process of knowledge have become outdated and ineffective, therefore, In order to improving the quality of teaching and learning on sport normal students in china, this author discussed some factors on promoting the level of teaching and learning for sport normal students, such as implementation principle, curriculum design, education policy, and so on. The meaning of results and their implication of future research are discussed.","['curriculum design', 'education policy', 'factors on promoting the level of teaching and learning', 'future research', 'implementation principle', 'improving the quality of teaching and learning on sport normal students', 'knowledge', 'learning', 'teaching']"
S0021999116303291,"DPD was first proposed in order to recover the properties of isotropy (and Galilean invariance) that were broken in the so-called lattice-gas automata method [5]. In DPD, each body is regarded as a coarse-grained particle. These particles interact in a soft (and short-ranged) potential, allowing larger integration timesteps than would be possible in MD, while simultaneously decreasing the number of degrees of freedom required. As in Langevin dynamics, a thermostat consisting of well-balanced damping and stochastic terms is applied to each particle. However, unlike in Langevin dynamics, both terms are pairwise and the damping term is based on relative velocities, leading to the conservation of both the angular momentum and the linear momentum. The property of Galilean invariance (i.e., the dependence on the relative velocity) makes DPD a profile-unbiased thermostat (PUT) [6,7] by construction and thus it is an ideal thermostat for nonequilibrium molecular dynamics (NEMD) [8]. The momentum is expected to propagate locally (while global momentum is conserved) and thus the correct hydrodynamics is expected in DPD [8], as demonstrated previously in [9]. Due to the aforementioned properties, DPD has been widely used to recover thermodynamic, dynamical, and rheological properties of complex fluids, with applications in polymer solutions [10], colloidal suspensions [11], multiphase flows [12], and biological systems [13]. DPD has been compared with Langevin dynamics for out-of-equilibrium simulations of polymeric systems in [14], where as expected the correct dynamic fluctuations of the polymers were obtained with the former but not with the latter.","['angular momentum', 'biological systems', 'coarse-grained particle', 'colloidal suspensions', 'complex fluids', 'dependence on the relative velocity', 'DPD', 'dynamic fluctuations', 'Galilean invariance', 'isotropy', 'Langevin dynamics', 'lattice-gas automata', 'linear momentum', 'MD', 'multiphase flows', 'NEMD', 'nonequilibrium molecular dynamics', 'nonequilibrium molecular dynamics (NEMD)', 'particles', 'polymeric systems', 'polymers', 'polymer solutions', 'profile-unbiased thermostat', 'PUT', 'recover thermodynamic, dynamical, and rheological properties', 'thermostat']"
S096386951400070X,"This paper has highlighted a band of frequencies, outside the conventional operation range, and close to electrical resonance of an eddy current probe, where the magnitude of impedance SNR reaches a peak. The SNR of scans of three slots of varying depth were enhanced by a factor of up to 3.7, from the SNR measured at 1MHz. This is a result of a defect-decoupling resonance-shift effect and is referred to as the near electrical resonance signal enhancement (NERSE) phenomenon. NERSE frequency operation has significant potential for ECT inspection, and opens up a range of investigative possibilities. Within this investigation, only the magnitude of the electrical impedance has been analyzed. An immediate extension of this investigation will be to consider phase information, and determine whether a similar exploitable NERSE effect exists.","['defect-decoupling resonance-shift effect', 'ECT inspection', 'eddy current probe', 'electrical impedance', 'electrical resonance of an eddy current probe', 'highlighted a band of frequencies', 'impedance SNR', 'near electrical resonance signal enhancement', 'NERSE', 'SNR']"
S0045782512000266,"In this work, a numerical strategy for designing an optimal maintenance scheduling for a structure, accounting explicitly for the effects of uncertainty is suggested. This contribution, which can be regarded as an extension of the methods developed in [23], presents several novel aspects over similar approaches proposed in the literature. Firstly, the initiation and propagation of fatigue crack is modeled efficiently by means of cohesive zone elements [24–26]. The application of this class of elements allows modeling the crack initiation and propagation within a unified framework. It should be noted that cohesive zone elements have already been used for uncertainty quantification of the crack propagation phenomenon [27,28]. However its application within the context of maintenance scheduling constitutes a novelty. The second innovative aspect of this contribution refers to the assessment of the reliability sensitivity with respect to the variables that define the maintenance scheduling. The estimation of this sensitivity, which is required in order to determine the optimal maintenance schedule within the proposed framework, can be quite demanding as the model characterizing repair of a cracked structure leads to a discontinuous performance function associated with the failure probability. A new approach for modeling this function is proposed herein. The continuous and discontinuous parts respectively of the function are considered separately to estimate accurately the gradients of the failure events.","['accounting explicitly for the effects of uncertainty', 'assessment of the reliability sensitivity with respect to the variables that define the maintenance scheduling', 'class of elements', 'cohesive zone elements', 'continuous and discontinuous parts', 'discontinuous performance function associated with the failure probability', 'estimate accurately the gradients of the failure events', 'estimation of this sensitivity', 'initiation and propagation of fatigue crack', 'maintenance scheduling', 'model characterizing repair of a cracked structure', 'modeling the crack initiation and propagation within a unified framework', 'numerical strategy', 'optimal maintenance schedule within the proposed framework', 'optimal maintenance scheduling for a structure', 'uncertainty quantification of the crack propagation phenomenon', 'unified framework']"
S0021999115008207,"Multi-phase flows are common, in fact quite general, in environmental and industrial processes. Broadly these may be modelled as continuous problems where phases are mixed (e.g. oil–water homogenisation [36], sediment transport [18]) or interface problems where phases are distinct and interact at the interface (e.g. gas-assisted injection moulding [21], liquid jet breakup [40]). In some cases flows start as interface problems but as mixing occurs at the interface they become effectively continuous, at least locally. Air entrainment, perhaps due to wave breaking, is an obvious example. We consider here two-phase interface problems where the interface remains distinct and the density difference is high, e.g. air and water, and where one phase may be considered incompressible. The interface is transient and may become highly distorted and interconnected. Such problems have been tackled with mesh-based methods using periodic (or adaptive) re-meshing or additional phase tracking functions [40]. However, these approaches can be time-consuming to implement and prone to errors in surface representation [50] or mass conservation [34].","['air', 'Air entrainment', 'distorted and interconnected', 'environmental and industrial processes', 'gas-assisted injection moulding', 'interface problems', 'liquid jet breakup', 'mesh-based methods', 'mixing', 'Multi-phase flows', 'oil–water homogenisation', 'phases are mixed', 'phase tracking functions', 're-meshing', 'sediment transport', 'two-phase interface problems', 'water', 'wave breaking']"
S221266781300018X,"Amodel are proposed for modeling data-centric Web services which are powered by relational databases and interact with users according to logical formulas specifying input constraints, control-flow constraints and state/output/action rules. The Linear Temporal First-Order Logic (LTL-FO) formulas over inputs, states, outputs and actions are used to express the properties to be verified.We have proven that automatic verification of LTL-FO properties of data-centric Web services under input-bounded constraints is decidable by reducing Web services to data-centric Web applications. Thus, we can verify Web service specifications using existing verifier designed for Web applications.","['control-flow constraints', 'data-centric Web applications', 'data-centric Web services', 'interact with users', 'Linear Temporal First-Order Logic', 'logical formulas', 'LTL-FO', 'modeling data-centric Web services', 'reducing Web services to data-centric Web applications', 'relational databases', 'state/output/action rules', 'verify Web service specifications', 'Web applications']"
S0021999114002587,"Designers of microfluidic devices are in need of computational tools that can be used to analyse problems that involve rarefied gas flows in complex micro geometries. Numerical simulation of the gas flow through such geometries is, however, extremely challenging. Conventional continuum fluid dynamics (CFD) becomes invalid or inaccurate as the characteristic scale of the geometry (e.g. the channel height, h) approaches the molecular mean free path, λ [1,2]. When λ/h≳0.1, the error in solutions obtained from CFD may be significant, and we must consider the fluid for what it is: a collection of interacting particles. However, the computational expense of simulating the flow of a rarefied gas in high-aspect-ratio micro geometries (i.e. ones that are long, relative to their cross section) using a particle method, such as the direct simulation Monte Carlo (DSMC) method [2], can be prohibitively high [3,4]. The computational intensity of the particle method is greater still when simulating low-speed microfluidic devices where there are only small deviations from equilibrium, characterised by extremely low Mach numbers and weak temperature gradients.","['analyse problems that involve rarefied gas flows in complex micro geometries', 'CFD', 'collection of interacting particles', 'Conventional continuum fluid dynamics', 'direct simulation Monte Carlo', 'DSMC', 'extremely low Mach numbers and weak temperature gradients', 'flow of a rarefied gas', 'gas flow', 'high-aspect-ratio micro geometries', 'long, relative to their cross section', 'microfluidic devices', 'Numerical simulation', 'particle method', 'rarefied gas flows', 'small deviations from equilibrium']"
S0370269304008494,"One major goal of current nuclear physics is the observation of at least partial restoration of chiral symmetry. Since the chiral order parameter 〈q̄q〉 is expected to decrease by about 30% already at normal nuclear matter density [1–4], any in-medium change due to the dropping quark condensate should in principle be observable in photonuclear reactions. The conjecture that such a partial restoration of chiral symmetry causes a softening and narrowing of the σ meson as the chiral partner of the pion in the nuclear medium [5,6] has led to the idea of measuring the π0π0 invariant mass distribution near the 2π threshold in photon induced reactions on nuclei [7]. In contrast to its questionable nature as a proper quasiparticle in vacuum, the σ meson might develop a much narrower peak at finite baryon density due to phase-space suppression for the σ→ππ decay, hence making it possible to explore its properties when embedded in a nuclear many-body system [8–11]. Measuring a threshold enhancement of the π0π0 invariant mass spectrum might serve as a signal for the partial restoration of chiral symmetry inside nuclei and, therefore, give information about one of the most fundamental features of QCD.","['chiral order parameter', 'chiral partner', 'Measuring a threshold enhancement of the π0π0 invariant mass spectrum', 'measuring the π0π0 invariant mass distribution', 'nuclear many-body system', 'nuclear matter', 'nuclear medium', 'nuclear physics', 'nuclei', 'observation of at least partial restoration of chiral symmetry', 'partial restoration of chiral symmetry', 'photon', 'photon induced reactions', 'photonuclear reactions', 'pion', 'QCD', 'q̄q', 'quark condensate', 'quasiparticle', 'σ meson', 'σ→ππ decay']"
S0009261415001517,"Since the receptors in human biology mostly consist of chiral molecules, drug action mostly involves a specified enantiomeric form. This has spurred the development, especially in the pharmaceutical industry, of a host of techniques to secure enantiopure products. Such methods, mostly multi-step and time-consuming, can typically be cast in one of two distinct categories: synthetic mechanisms designed to produce a single stereoisomer, or separation techniques to isolate distinct enantiomers from a racemic mixture. A significant drawback, for either approach, is a dependence on a supply of enantiopure reagents or substrates – synthesis routes generally utilise chiral building blocks or enantioselective catalysts [7,8], while enantiomer separation techniques typically incorporate chiral selector molecules to form chemically distinct and distinguishable diastereomeric complexes [8,9]. A key requirement in aiming to achieve enantiopure products, irrespective of the synthetic method, is therefore a means to measure, and duly quantitate the enantiomeric excess – signifying the degree of chirality within molecular products. Chiral discrimination through optical means is well-known to offer direct, non-contact ways to distinguish between molecules of different handedness, based on observations such as the subtle differences in absorption of left- and right-handed circularly polarised light, or indeed the twisting of polarisation in optical rotation. Other optical methods, under more recent development, also show some promise to achieve enantiomer separation, as will be introduced later.","['absorption', 'achieve enantiopure products, irrespective of the synthetic method', 'chiral building blocks', 'Chiral discrimination', 'chiral molecules', 'chiral selector molecules', 'degree of chirality', 'diastereomeric complexes', 'echniques to secure enantiopure products', 'enantiomeric excess', 'enantiomers', 'enantiomer separation', 'enantiomer separation techniques', 'enantiopure products', 'enantiopure reagents or substrates', 'enantioselective catalysts', 'means to measure, and duly quantitate the enantiomeric excess', 'methods, mostly multi-step and time-consuming', 'molecular products', 'molecules', 'optical means', 'optical methods', 'separation techniques', 'stereoisomer', 'synthesis routes', 'synthetic mechanisms', 'synthetic method', 'twisting of polarisation in optical rotation']"

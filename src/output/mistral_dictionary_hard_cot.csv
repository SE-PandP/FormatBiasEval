doc_id,document,top_k,category,input_data,answer,python_output,yaml_output
02567fd428a675ca91a0c6786f47f3e35881bcbd,"document : Deep Label Distribution Learning With Label Ambiguity Convolutional Neural Networks ( ConvNets ) have achieved excellent recognition performance in various visual recognition tasks . A large labeled training set is one of the most important factors for its success . However , it is difficult to collect sufficient training images with precise labels in some domains such as apparent age estimation , head pose estimation , multi - label classification and semantic segmentation . Fortunately , there is ambiguous information among labels , which makes these tasks different from traditional classification .",5,"['Task', 'Method', 'Material', 'Metric']","document : Deep Label Distribution Learning With Label Ambiguity Convolutional Neural Networks ( ConvNets ) have achieved excellent recognition performance in various visual recognition tasks . A large labeled training set is one of the most important factors for its success . However , it is difficult to collect sufficient training images with precise labels in some domains such as apparent age estimation , head pose estimation , multi - label classification and semantic segmentation . Fortunately , there is ambiguous information among labels , which makes these tasks different from traditional classification .","{'Task': ['semantic segmentation', 'age estimation', 'head pose estimation', 'Label Ambiguity', 'classification', 'multi - label classification', 'recognition', 'visual recognition tasks'], 'Method': ['Deep Label Distribution Learning', 'Convolutional Neural Networks', 'ConvNets'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['apparent age estimation', 'head pose estimation', 'multi-label classification', 'semantic segmentation'],
 'Method': ['Deep Label Distribution Learning', 'Convolutional Neural Networks (ConvNets)'],
 'Material': ['large labeled training set'],
 'Metric': ['recognition performance']}
</ANSWER>","<ANSWER>
Task:
- Apparent age estimation
- Head pose estimation
- Multi-label classification
- Semantic segmentation

Method:
- Deep Label Distribution Learning
- Convolutional Neural Networks (ConvNets)

Material:
- Labeled training set

Metric:
- Recognition performance
</ANSWER>"
02b3d1d162080d9aefd3fc30a0bcc9a843073b5d,"In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling , a task central to language understanding . We extend current models to deal with two key challenges present in this task : corpora and vocabulary sizes , and complex , long term structure of language . We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long - Short Term Memory , on the One Billion Word Benchmark . Our best single model significantly improves state - of - the - art perplexity from 51.3 down to 30.0 ( whilst reducing the number of parameters by a factor of 20 ) , while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7 . We also release these models for the NLP and ML community to study and improve upon .",5,"['Task', 'Method', 'Material', 'Metric']","In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling , a task central to language understanding . We extend current models to deal with two key challenges present in this task : corpora and vocabulary sizes , and complex , long term structure of language . We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long - Short Term Memory , on the One Billion Word Benchmark . Our best single model significantly improves state - of - the - art perplexity from 51.3 down to 30.0 ( whilst reducing the number of parameters by a factor of 20 ) , while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7 . We also release these models for the NLP and ML community to study and improve upon .","{'Task': ['language understanding', 'Language Modeling', 'NLP and ML community'], 'Method': ['character Convolutional Neural Networks', 'Recurrent Neural Networks', 'ensemble of models', 'Long - Short Term Memory'], 'Material': ['One Billion Word Benchmark'], 'Metric': ['perplexity']}","<ANSWER>
{'Task': ['large scale Language Modeling', 'dealing with corpora and vocabulary sizes, complex, long term structure of language'],
 'Method': ['Recurrent Neural Networks', 'character Convolutional Neural Networks', 'Long - Short Term Memory'],
 'Material': ['One Billion Word Benchmark'],
 'Metric': ['perplexity']}
</ANSWER>","<ANSWER>
Task: [ ""large scale Language Modeling"", ""exploring recent advances in Recurrent Neural Networks"" ]
Method: [ ""character Convolutional Neural Networks"", ""Long - Short Term Memory"" ]
Material: [ ""One Billion Word Benchmark"" ]
Metric: [ ""perplexity"" ]
</ANSWER>"
0398552184f80db111e9c28bf533b395f233ac00,"Bridging Saliency Detection to Weakly Supervised Object Detection Based on Self - paced Curriculum Learning section : Abstract Weakly - supervised object detection ( WOD ) is a challenging problems in computer vision . The key problem is to simultaneously infer the exact object locations in the training images and train the object detectors , given only the training images with weak image - level labels . Intuitively , by simulating the selective attention mechanism of human visual system , saliency detection technique can select attractive objects in scenes and thus is a potential way to provide useful priors for WOD .",5,"['Task', 'Method', 'Material', 'Metric']","Bridging Saliency Detection to Weakly Supervised Object Detection Based on Self - paced Curriculum Learning section : Abstract Weakly - supervised object detection ( WOD ) is a challenging problems in computer vision . The key problem is to simultaneously infer the exact object locations in the training images and train the object detectors , given only the training images with weak image - level labels . Intuitively , by simulating the selective attention mechanism of human visual system , saliency detection technique can select attractive objects in scenes and thus is a potential way to provide useful priors for WOD .","{'Task': ['WOD', 'saliency detection', 'Weakly Supervised Object Detection', 'Bridging Saliency Detection', 'Weakly - supervised object detection', 'computer vision'], 'Method': ['object detectors', 'Self - paced Curriculum Learning', 'human visual system', 'selective attention mechanism'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['weakly supervised object detection', 'inferring the exact object locations', 'training the object detectors'],
 'Method': ['simulating the selective attention mechanism of human visual system', 'saliency detection technique'],
 'Material': [],
 'Metric': []
}
</ANSWER>","<ANSWER>
Task:
- Weakly-supervised object detection (WOD)

Method:
- Self-paced Curriculum Learning
- Saliency detection technique

Material:
- Training images
- Weak image-level labels

Metric:
- Object locations
</ANSWER>"
05d2700846c0323f79c1344aca5333994c7c03a5,"document : The IBM 2016 English Conversational Telephone Speech Recognition System We describe a collection of acoustic and language modeling techniques that lowered the word error rate of our English conversational telephone LVCSR system to a record 6.6 % on the Switchboard subset of the Hub5 2000 evaluation testset . On the acoustic side , we use a score fusion of three strong models : recurrent nets with maxout activations , very deep convolutional nets with 3x3 kernels , and bidirectional long short - term memory nets which operate on FMLLR and i - vector features . On the language modeling side , we use an updated model “ M ” and hierarchical neural network LMs . GeorgeSaon , TomSercu , StevenRennieandHong - KwangJ.Kuo IBMT.J.WatsonResearchCenter , YorktownHeights , NY , 10598 gsaon@us.ibm.com",5,"['Task', 'Method', 'Material', 'Metric']","document : The IBM 2016 English Conversational Telephone Speech Recognition System We describe a collection of acoustic and language modeling techniques that lowered the word error rate of our English conversational telephone LVCSR system to a record 6.6 % on the Switchboard subset of the Hub5 2000 evaluation testset . On the acoustic side , we use a score fusion of three strong models : recurrent nets with maxout activations , very deep convolutional nets with 3x3 kernels , and bidirectional long short - term memory nets which operate on FMLLR and i - vector features . On the language modeling side , we use an updated model “ M ” and hierarchical neural network LMs . GeorgeSaon , TomSercu , StevenRennieandHong - KwangJ.Kuo IBMT.J.WatsonResearchCenter , YorktownHeights , NY , 10598 gsaon@us.ibm.com","{'Task': ['language modeling side', 'IBM 2016 English Conversational Telephone Speech Recognition System'], 'Method': ['3x3 kernels', 'maxout activations', 'strong models', 'recurrent nets', 'bidirectional long short - term memory nets', 'deep convolutional nets', 'updated model', 'acoustic and language modeling techniques', 'English conversational telephone LVCSR system', 'M ”', 'hierarchical neural network LMs'], 'Material': ['Switchboard subset'], 'Metric': ['word error rate']}","<ANSWER>
{'Task': ['English conversational telephone LVCSR system evaluation'],
 'Method': ['score fusion', 'recurrent nets with maxout activations', 'very deep convolutional nets with 3x3 kernels', 'bidirectional long short - term memory nets', 'updated model “ M ”', 'hierarchical neural network LMs'],
 'Material': ['IBM 2016 English Conversational Telephone Speech Recognition System', 'Switchboard subset of the Hub5 2000 evaluation testset', 'FMLLR and i-vector features'],
 'Metric': ['word error rate']}
</ANSWER>","<ANSWER>
Tasks:
- Developing and improving an English conversational telephone LVCSR system

Methods:
- Score fusion of three strong acoustic models: recurrent nets with maxout activations, very deep convolutional nets with 3x3 kernels, and bidirectional long short-term memory nets
- Use of an updated language model ""M""
- Hierarchical neural network LMs

Materials:
- Hub5 2000 evaluation testset (Switchboard subset)
- FMLLR and i-vector features

Metrics:
- Word error rate (WER)
- Record WER of 6.6%
</ANSWER>"
0626908dd710b91aece1a81f4ca0635f23fc47f3,"document : Rethinking the Inception Architecture for Computer Vision Convolutional networks are at the core of most state - of - the - art computer vision solutions for a wide variety of tasks . Since 2014 very deep convolutional networks started to become mainstream , yielding substantial gains in various benchmarks . Although increased model size and computational cost tend to translate to immediate quality gains for most tasks ( as long as enough labeled data is provided for training ) , computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big - data scenarios . Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization .",5,"['Task', 'Method', 'Material', 'Metric']","document : Rethinking the Inception Architecture for Computer Vision Convolutional networks are at the core of most state - of - the - art computer vision solutions for a wide variety of tasks . Since 2014 very deep convolutional networks started to become mainstream , yielding substantial gains in various benchmarks . Although increased model size and computational cost tend to translate to immediate quality gains for most tasks ( as long as enough labeled data is provided for training ) , computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big - data scenarios . Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization .","{'Task': ['big - data scenarios', 'Computer Vision', 'mobile vision'], 'Method': ['deep convolutional networks', 'factorized convolutions', 'aggressive regularization', 'computer vision solutions', 'Inception Architecture', 'Convolutional networks'], 'Material': [], 'Metric': ['computational cost', 'computational efficiency', 'model size', 'low parameter count']}","<ANSWER>
{'Task': ['state-of-the-art computer vision solutions', 'various tasks', 'use cases such as mobile vision and big-data scenarios'],
 'Method': ['very deep convolutional networks', 'scaling up networks', 'suitably factorized convolutions', 'aggressive regularization'],
 'Material': ['convolutional networks', 'enough labeled data'],
 'Metric': ['immediate quality gains', 'computational efficiency', 'low parameter count']
}
</ANSWER>","<ANSWER>
Task: [""computer vision solutions"", ""various tasks"", ""mobile vision"", ""big data scenarios""]
Method: [""very deep convolutional networks"", ""suitably factorized convolutions"", ""aggressive regularization""]
Material: []
Metric: [""immediate quality gains"", ""computational efficiency"", ""low parameter count""]
</ANSWER>"
0678a8abea82793993cd89383319da75f6dc4be3,"document : ProNet : Learning to Propose Object - specific Boxes for Cascaded Neural Networks This paper aims to classify and locate objects accurately and efficiently , without using bounding box annotations . It is challenging as objects in the wild could appear at arbitrary locations and in different scales . In this paper , we propose a novel classification architecture ProNet based on convolutional neural networks . It uses computationally efficient neural networks to propose image regions that are likely to contain objects , and applies more powerful but slower networks on the proposed regions .",5,"['Task', 'Method', 'Material', 'Metric']","document : ProNet : Learning to Propose Object - specific Boxes for Cascaded Neural Networks This paper aims to classify and locate objects accurately and efficiently , without using bounding box annotations . It is challenging as objects in the wild could appear at arbitrary locations and in different scales . In this paper , we propose a novel classification architecture ProNet based on convolutional neural networks . It uses computationally efficient neural networks to propose image regions that are likely to contain objects , and applies more powerful but slower networks on the proposed regions .","{'Task': [], 'Method': ['convolutional neural networks', 'classification architecture ProNet', 'neural networks', 'ProNet', 'Learning to Propose Object - specific Boxes for Cascaded Neural Networks'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['classify', 'locate', 'objects', 'accurately', 'efficiently', 'without using bounding box annotations'],
 'Method': ['ProNet', 'convolutional neural networks', 'computationally efficient neural networks', 'applies more powerful but slower networks'],
 'Material': [],
 'Metric': []
}
</ANSWER>","<ANSWER>
Task: Object classification and localization without using bounding box annotations
Method: Proposing image regions using computationally efficient neural networks (ProNet) and applying more powerful but slower networks on the proposed regions
Material: Convolutional neural networks
Metric: Accuracy and efficiency
</ANSWER>"
081531984770a74e87dbd68907061b4b0f3631bf,"document : Real - Time Video Super - Resolution with Spatio - Temporal Networks and Motion Compensation Convolutional neural networks have enabled accurate image super - resolution in real - time . However , recent attempts to benefit from temporal correlations in video super - resolution have been limited to naive or inefficient architectures . In this paper , we introduce spatio - temporal sub - pixel convolution networks that effectively exploit temporal redundancies and improve reconstruction accuracy while maintaining real - time speed . Specifically , we discuss the use of early fusion , slow fusion and 3D convolutions for the joint processing of multiple consecutive video frames .",5,"['Task', 'Method', 'Material', 'Metric']","document : Real - Time Video Super - Resolution with Spatio - Temporal Networks and Motion Compensation Convolutional neural networks have enabled accurate image super - resolution in real - time . However , recent attempts to benefit from temporal correlations in video super - resolution have been limited to naive or inefficient architectures . In this paper , we introduce spatio - temporal sub - pixel convolution networks that effectively exploit temporal redundancies and improve reconstruction accuracy while maintaining real - time speed . Specifically , we discuss the use of early fusion , slow fusion and 3D convolutions for the joint processing of multiple consecutive video frames .","{'Task': ['Real - Time Video Super - Resolution', 'video super - resolution', 'image super - resolution in real - time', 'video'], 'Method': ['Convolutional neural networks', 'early fusion', 'spatio - temporal sub - pixel convolution networks', 'slow fusion', 'Motion Compensation', 'Spatio - Temporal Networks', '3D convolutions'], 'Material': [], 'Metric': ['real - time speed', 'reconstruction accuracy']}","<ANSWER>
{'Task': ['Real-time Video Super-Resolution'], 'Method': ['Spatio-temporal sub-pixel convolution networks', 'Early fusion', 'Slow fusion', '3D convolutions'], 'Material': [], 'Metric': ['Reconstruction accuracy']}
</ANSWER>","<ANSWER>
Task: Real-time video super-resolution
Method:
- Spatio-temporal sub-pixel convolution networks
- Early fusion
- Slow fusion
- 3D convolutions
Material: Convolutional neural networks
Metric: Reconstruction accuracy
</ANSWER>"
0834e74304b547c9354b6d7da6fa78ef47a48fa8,"document : LINE : Large - scale Information Network Embedding This paper studies the problem of embedding very large information networks into low - dimensional vector spaces , which is useful in many tasks such as visualization , node classification , and link prediction . Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes . In this paper , we propose a novel network embedding method called the “ LINE , ” which is suitable for arbitrary types of information networks : undirected , directed , and / or weighted . The method optimizes a carefully designed objective function that preserves both the local and global network structures .",5,"['Task', 'Method', 'Material', 'Metric']","document : LINE : Large - scale Information Network Embedding This paper studies the problem of embedding very large information networks into low - dimensional vector spaces , which is useful in many tasks such as visualization , node classification , and link prediction . Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes . In this paper , we propose a novel network embedding method called the “ LINE , ” which is suitable for arbitrary types of information networks : undirected , directed , and / or weighted . The method optimizes a carefully designed objective function that preserves both the local and global network structures .","{'Task': ['low - dimensional vector spaces', 'link prediction', 'node classification', 'real world information networks', 'information networks', 'embedding very large information networks', 'visualization'], 'Method': ['graph embedding methods', 'objective function', 'LINE', 'Large - scale Information Network Embedding', 'network embedding method'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['visualization', 'node classification', 'link prediction'],
 'Method': ['LINE'],
 'Material': [],
 'Metric': ['local network structures', 'global network structures']}
</ANSWER>","<ANSWER>
Task: [[""visualization"", ""node classification"", ""link prediction""]]
Method: [""LINE""]
Material: []
Metric: [""local network structures"", ""global network structures""]
</ANSWER>"
0a3381f0432c5cfe491c718349d7a44e5814592c,"document : Compositional Sequence Labeling Models for Error Detection in Learner Writing In this paper , we present the first experiments using neural network models for the task of error detection in learner writing . We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs . Experiments on the CoNLL - 14 shared task dataset show the model is able to outperform other participants on detecting errors in learner writing . Finally , the model is integrated with a publicly deployed self - assessment system , leading to performance comparable to human annotators .",5,"['Task', 'Method', 'Material', 'Metric']","document : Compositional Sequence Labeling Models for Error Detection in Learner Writing In this paper , we present the first experiments using neural network models for the task of error detection in learner writing . We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs . Experiments on the CoNLL - 14 shared task dataset show the model is able to outperform other participants on detecting errors in learner writing . Finally , the model is integrated with a publicly deployed self - assessment system , leading to performance comparable to human annotators .","{'Task': ['Learner Writing', 'detecting errors in learner writing', 'error detection', 'learner writing', 'Error Detection'], 'Method': ['LSTMs', 'neural network models', 'compositional architectures', 'self - assessment', 'Compositional Sequence Labeling Models'], 'Material': ['CoNLL - 14 shared task dataset'], 'Metric': []}","<ANSWER>
{'Task': ['error detection in learner writing', 'detecting errors'],
 'Method': ['neural network models', 'compositional architectures', 'bidirectional LSTMs'],
 'Material': ['CoNLL - 14 shared task dataset'],
 'Metric': ['performance']}
</ANSWER>","<ANSWER>
Task: error detection in learner writing
Method: neural network models, compositional architectures, bidirectional LSTMs
Material: CoNLL-14 shared task dataset
Metric: performance comparable to human annotators
</ANSWER>"
0a6c36de8726b6feaab586046ddc1d1a008f44f9,"document : Filtered Channel Features for Pedestrian Detection This paper starts from the observation that multiple top performing pedestrian detectors can be modelled by using an intermediate layer filtering low - level features in combination with a boosted decision forest . Based on this observation we propose a unifying framework and experimentally explore different filter families . We report extensive results enabling a systematic analysis . Using filtered channel features we obtain top performance on the challenging Caltech and KITTI datasets , while using only HOG + LUV as low - level features .",5,"['Task', 'Method', 'Material', 'Metric']","document : Filtered Channel Features for Pedestrian Detection This paper starts from the observation that multiple top performing pedestrian detectors can be modelled by using an intermediate layer filtering low - level features in combination with a boosted decision forest . Based on this observation we propose a unifying framework and experimentally explore different filter families . We report extensive results enabling a systematic analysis . Using filtered channel features we obtain top performance on the challenging Caltech and KITTI datasets , while using only HOG + LUV as low - level features .","{'Task': ['Pedestrian Detection'], 'Method': ['boosted decision forest', 'unifying framework', 'Filtered Channel Features', 'top performing pedestrian detectors', 'intermediate layer filtering low - level features', 'filter families'], 'Material': ['Caltech', 'KITTI'], 'Metric': []}","<ANSWER>
{'Task': ['pedestrian detection'], 'Method': ['intermediate layer filtering', 'using an intermediate layer to filter low-level features', 'boosted decision forest'], 'Material': ['multiple top performing pedestrian detectors', 'low-level features (HOG + LUV)'], 'Metric': ['extensive results', 'top performance on the challenging Caltech and KITTI datasets']}
</ANSWER>","<ANSWER>
Task: Pedestrian Detection
Method: Intermediate layer filtering, Boosted decision forest
Material: Low-level features (Filtered channel features, HOG + LUV)
Metric: Extensive results enabling systematic analysis, Top performance on Caltech and KITTI datasets
</ANSWER>"
0c47cad9729c38d9db1f75491b1ee4bd883a5d4e,"document : Semi - Supervised Sequence Modeling with Cross - View Training kevclark@cs.stanford.edu , thangluong@google.com , manning@cs.stanford.edu , qvl@google.com Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models , mainly because they can take advantage of large amounts of unlabeled text . However , the supervised models only learn from task - specific labeled data during the main training phase . We therefore propose Cross - View Training ( CVT ) , a semi - supervised learning algorithm that improves the representations of a Bi - LSTM sentence encoder using a mix of labeled and unlabeled data . On labeled examples , standard supervised learning is used .",5,"['Task', 'Method', 'Material', 'Metric']","document : Semi - Supervised Sequence Modeling with Cross - View Training kevclark@cs.stanford.edu , thangluong@google.com , manning@cs.stanford.edu , qvl@google.com Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models , mainly because they can take advantage of large amounts of unlabeled text . However , the supervised models only learn from task - specific labeled data during the main training phase . We therefore propose Cross - View Training ( CVT ) , a semi - supervised learning algorithm that improves the representations of a Bi - LSTM sentence encoder using a mix of labeled and unlabeled data . On labeled examples , standard supervised learning is used .","{'Task': [], 'Method': ['Semi - Supervised Sequence Modeling', 'Unsupervised representation learning algorithms', 'Bi - LSTM sentence encoder', 'semi - supervised learning algorithm', 'CVT', 'supervised models', 'supervised learning', 'Cross - View Training', 'supervised NLP models', 'word2vec', 'ELMo'], 'Material': [], 'Metric': ['accuracy']}","<ANSWER>
{'Task': ['improving the accuracy of many supervised NLP models', 'learning from task-specific labeled data during the main training phase', 'using a mix of labeled and unlabeled data'],
 'Method': ['Semi-supervised learning algorithm', 'Cross-View Training (CVT)', 'standard supervised learning'],
 'Material': ['large amounts of unlabeled text', 'Bi-LSTM sentence encoder'],
 'Metric': ['accuracy']}
</ANSWER>","<ANSWER>
Task:
- Semi-supervised sequence modeling
- Improving representations of a Bi-LSTM sentence encoder

Method:
- Cross-View Training (CVT)
- Unsupervised representation learning (word2vec, ELMo)
- Standard supervised learning

Material:
- Large amounts of unlabeled text
- Labeled text
- Bi-LSTM sentence encoder

Metric:
- Accuracy (of NLP models)
</ANSWER>"
0dc9eb7d17f2def56ad930945f2521653f04c3fa,Skip - gram Language Modeling Using Sparse Non - negative Matrix Probability Estimation section : Abstract We present a novel family of language model ( LM ) estimation techniques named Sparse Non - negative Matrix ( SNM ) estimation . A first set of experiments empirically evaluating it on the One Billion Word Benchmark [ reference ] shows that SNM n - gram LMs perform almost as well as the well - established Kneser - Ney ( KN ) models .,5,"['Task', 'Method', 'Material', 'Metric']",Skip - gram Language Modeling Using Sparse Non - negative Matrix Probability Estimation section : Abstract We present a novel family of language model ( LM ) estimation techniques named Sparse Non - negative Matrix ( SNM ) estimation . A first set of experiments empirically evaluating it on the One Billion Word Benchmark [ reference ] shows that SNM n - gram LMs perform almost as well as the well - established Kneser - Ney ( KN ) models .,"{'Task': ['Skip - gram Language Modeling'], 'Method': ['SNM n - gram LMs', 'KN', 'LM ) estimation techniques', 'Sparse Non - negative Matrix Probability Estimation', 'Sparse Non - negative Matrix', 'language model', 'Kneser - Ney', 'SNM'], 'Material': ['One Billion Word Benchmark'], 'Metric': []}","<ANSWER>
{'Task': ['language model estimation'], 'Method': ['Sparse Non-negative Matrix (SNM) estimation'], 'Material': ['One Billion Word Benchmark'], 'Metric': ['performance']}
</ANSWER>","<ANSWER>
Task: Empirically evaluating and comparing the performance of different language model estimation techniques
Method: Sparse Non-negative Matrix (SNM) estimation
Material: One Billion Word Benchmark
Metric: Performance of SNM n-gram LMs compared to well-established Kneser-Ney (KN) models
</ANSWER>"
107010b7f2abe3c0c9df62bcef35eb77f6fc76df,"Domain - Adversarial Training of Neural Networks section : Abstract We introduce a new representation learning approach for domain adaptation , in which data at training and test time come from similar but different distributions . Our approach is directly inspired by the theory on domain adaptation suggesting that , for effective domain transfer to be achieved , predictions must be made based on features that can not discriminate between the training ( source ) and test ( target ) domains . The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain ( no labeled target - domain data is necessary ) .",5,"['Task', 'Method', 'Material', 'Metric']","Domain - Adversarial Training of Neural Networks section : Abstract We introduce a new representation learning approach for domain adaptation , in which data at training and test time come from similar but different distributions . Our approach is directly inspired by the theory on domain adaptation suggesting that , for effective domain transfer to be achieved , predictions must be made based on features that can not discriminate between the training ( source ) and test ( target ) domains . The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain ( no labeled target - domain data is necessary ) .","{'Task': ['domain transfer', 'predictions'], 'Method': ['neural network architectures', 'Domain - Adversarial Training of Neural Networks', 'representation learning approach', 'domain adaptation'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['domain adaptation'], 'Method': ['representation learning approach', 'neural network architectures', 'trained on labeled data from the source domain and unlabeled data from the target domain'], 'Material': ['data at training and test time', 'labeled data from the source domain', 'unlabeled data from the target domain'], 'Metric': ['effective domain transfer']}
</ANSWER>","<ANSWER>
Task: Domain adaptation
Method: Neural network training with labeled source data and unlabeled target data
Material: Neural network architectures
Metric: Effective domain transfer, predictions based on features that can't discriminate between source and target domains
</ANSWER>"
1130d8fdd931225c2d7563c3808367726cfa1c3a,"document : PixelGAN Autoencoders In this paper , we describe the ‘ ‘ PixelGAN autoencoder ’ ’ , a generative autoencoder in which the generative path is a convolutional autoregressive neural network on pixels ( PixelCNN ) that is conditioned on a latent code , and the recognition path uses a generative adversarial network ( GAN ) to impose a prior distribution on the latent code . We show that different priors result in different decompositions of information between the latent code and the autoregressive decoder . For example , by imposing a Gaussian distribution as the prior , we can achieve a global vs. local decomposition , or by imposing a categorical distribution as the prior , we can disentangle the style and content information of images in an unsupervised fashion . We further show how the PixelGAN autoencoder with a categorical prior can be directly used in semi - supervised settings and achieve competitive semi - supervised classification results on the MNIST , SVHN and NORB datasets .",5,"['Task', 'Method', 'Material', 'Metric']","document : PixelGAN Autoencoders In this paper , we describe the ‘ ‘ PixelGAN autoencoder ’ ’ , a generative autoencoder in which the generative path is a convolutional autoregressive neural network on pixels ( PixelCNN ) that is conditioned on a latent code , and the recognition path uses a generative adversarial network ( GAN ) to impose a prior distribution on the latent code . We show that different priors result in different decompositions of information between the latent code and the autoregressive decoder . For example , by imposing a Gaussian distribution as the prior , we can achieve a global vs. local decomposition , or by imposing a categorical distribution as the prior , we can disentangle the style and content information of images in an unsupervised fashion . We further show how the PixelGAN autoencoder with a categorical prior can be directly used in semi - supervised settings and achieve competitive semi - supervised classification results on the MNIST , SVHN and NORB datasets .","{'Task': ['competitive semi - supervised classification', 'semi - supervised settings'], 'Method': ['unsupervised fashion', 'recognition path', 'autoregressive decoder', 'GAN', 'Gaussian distribution', 'convolutional autoregressive neural network on pixels', 'autoencoder', 'PixelGAN Autoencoders', 'generative path', 'PixelCNN', 'generative adversarial network', 'generative autoencoder'], 'Material': ['MNIST'], 'Metric': []}","<ANSWER>
{'Task': ['generative autoencoder', 'semi-supervised settings', 'classification'],
 'Method': ['PixelGAN autoencoder', 'convolutional autoregressive neural network (PixelCNN)', 'generative adversarial network (GAN)', 'imposing a prior distribution'],
 'Material': ['latent code', 'autoregressive decoder', 'MNIST', 'SVHN', 'NORB'],
 'Metric': ['competitive results']}
</ANSWER>","<ANSWER>
Task:
- Generative model training
- Image decomposition
- Semi-supervised classification

Method:
- PixelGAN autoencoder
- Convolutional autoregressive neural network (PixelCNN)
- Generative adversarial network (GAN)
- Gaussian distribution
- Categorical distribution

Material:
- Pixel data
- Latent code

Metric:
- Classification accuracy (MNIST, SVHN, NORB datasets)
</ANSWER>"
11356cd6bb0f2776a88cd584ff108470414c6594,"document : Submanifold Sparse Convolutional Networks Convolutional network are the de - facto standard for analysing spatio - temporal data such as images , videos , 3D shapes , etc . Whilst some of this data is naturally dense ( for instance , photos ) , many other data sources are inherently sparse . Examples include pen - strokes forming on a piece of paper , or ( colored ) 3D point clouds that were obtained using a LiDAR scanner or RGB - D camera . Standard",5,"['Task', 'Method', 'Material', 'Metric']","document : Submanifold Sparse Convolutional Networks Convolutional network are the de - facto standard for analysing spatio - temporal data such as images , videos , 3D shapes , etc . Whilst some of this data is naturally dense ( for instance , photos ) , many other data sources are inherently sparse . Examples include pen - strokes forming on a piece of paper , or ( colored ) 3D point clouds that were obtained using a LiDAR scanner or RGB - D camera . Standard","{'Task': [], 'Method': ['Submanifold Sparse Convolutional Networks', 'RGB - D camera', 'LiDAR scanner', 'Convolutional network'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['analysing spatio-temporal data', 'obtaining 3D point clouds using LiDAR scanner or RGB-D camera'],
 'Method': ['Convolutional network'],
 'Material': ['images', 'videos', '3D shapes', 'pen strokes on a piece of paper', 'colored 3D point clouds'],
 'Metric': ['Ellipsis']}
</ANSWER>","<ANSWER>
Tasks:
- Analyzing spatio-temporal data
- Handling sparse data

Methods:
- Using Convolutional Neural Networks (CNNs)
- Handling sparse data with Submanifold Sparse Convolutional Networks

Materials:
- Images
- Videos
- 3D shapes
- Pen strokes on a piece of paper
- Colored 3D point clouds
- LiDAR scanner data
- RGB-D camera data

Metrics:
- Accuracy
- Precision
- Recall
- F1 score
- Loss function
</ANSWER>"
11da0c54ba904a1cb31a09d10da55f73e8825c61,"document : Natural Language Inference by Tree - Based Convolution and Heuristic Matching In this paper , we propose the TBCNN - pair model to recognize entailment and contradiction between two sentences . In our model , a tree - based convolutional neural network ( TBCNN ) captures sentence - level semantics ; then heuristic matching layers like concatenation , element - wise product / difference combine the information in individual sentences . Experimental results show that our model outperforms existing sentence encoding - based approaches by a large margin . section : Introduction",5,"['Task', 'Method', 'Material', 'Metric']","document : Natural Language Inference by Tree - Based Convolution and Heuristic Matching In this paper , we propose the TBCNN - pair model to recognize entailment and contradiction between two sentences . In our model , a tree - based convolutional neural network ( TBCNN ) captures sentence - level semantics ; then heuristic matching layers like concatenation , element - wise product / difference combine the information in individual sentences . Experimental results show that our model outperforms existing sentence encoding - based approaches by a large margin . section : Introduction","{'Task': ['recognize entailment and contradiction', 'Natural Language Inference'], 'Method': ['sentence encoding - based approaches', 'concatenation', 'Heuristic Matching', 'TBCNN - pair model', 'Tree - Based Convolution', 'TBCNN', 'tree - based convolutional neural network', 'heuristic matching layers'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['recognize entailment and contradiction between two sentences'],
 'Method': ['Tree - based convolutional neural network (TBCNN)', 'heuristic matching layers'],
 'Material': ['sentences'],
 'Metric': ['experimental results']}
</ANSWER>","<ANSWER>
Tasks:
- Recognizing entailment and contradiction between two sentences

Methods:
- Tree-based convolutional neural network (TBCNN)
- Heuristic matching layers (concatenation, element-wise product / difference)

Materials:
- Sentences

Metrics:
- Performance comparison with existing sentence encoding-based approaches
</ANSWER>"
1235dd37312cb20aced0e97d953f6379d8a0c7d4,"document : Grounded Textual Entailment Capturing semantic relations between sentences , such as entailment , is a long - standing challenge for computational semantics . Logic - based models analyse entailment in terms of possible worlds ( interpretations , or situations ) where a premise P entails a hypothesis H iff in all worlds where P is true , H is also true . Statistical models view this relationship probabilistically , addressing it in terms of whether a human would likely infer H from P. In this paper , we wish to bridge these two perspectives , by arguing for a visually - grounded version of the Textual Entailment task .",5,"['Task', 'Method', 'Material', 'Metric']","document : Grounded Textual Entailment Capturing semantic relations between sentences , such as entailment , is a long - standing challenge for computational semantics . Logic - based models analyse entailment in terms of possible worlds ( interpretations , or situations ) where a premise P entails a hypothesis H iff in all worlds where P is true , H is also true . Statistical models view this relationship probabilistically , addressing it in terms of whether a human would likely infer H from P. In this paper , we wish to bridge these two perspectives , by arguing for a visually - grounded version of the Textual Entailment task .","{'Task': ['Grounded Textual Entailment', 'computational semantics', 'Textual Entailment task', 'Capturing semantic relations between sentences'], 'Method': ['Statistical models', 'Logic - based models', 'visually - grounded version'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['Grounded Textual Entailment', 'Textual Entailment'],
 'Method': ['Logic-based models', 'Statistical models', 'visually-grounded version'],
 'Material': [Ellipsis],
 'Metric': [Ellipsis, 'entailment', 'probabilistically']}
</ANSWER>","<ANSWER>
Task: [""Grounded Textual Entailment"", ""Bridging two perspectives of Textual Entailment""]
Method: [""Logic-based models analyzing entailment in terms of possible worlds"", ""Statistical models addressing entailment probabilistically""]
Material: [""sentences"", ""premise P"", ""hypothesis H"", ""worlds"", ""interpretations"", ""situations""]
Metric: [""entailment""]
</ANSWER>"
14318685b5959b51d0f1e3db34643eb2855dc6d9,"document : Going deeper with convolutions We propose a deep convolutional neural network architecture codenamed âInceptionâ , which was responsible for setting the new state of the art for classification and detection in the ImageNet Large - Scale Visual Recognition Challenge 2014 ( ILSVRCâ14 ) . The main hallmark of this architecture is the improved utilization of the computing resources inside the network . This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant . To optimize quality , the architectural decisions were based on the Hebbian principle and the intuition of multi - scale processing .",5,"['Task', 'Method', 'Material', 'Metric']","document : Going deeper with convolutions We propose a deep convolutional neural network architecture codenamed âInceptionâ , which was responsible for setting the new state of the art for classification and detection in the ImageNet Large - Scale Visual Recognition Challenge 2014 ( ILSVRCâ14 ) . The main hallmark of this architecture is the improved utilization of the computing resources inside the network . This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant . To optimize quality , the architectural decisions were based on the Hebbian principle and the intuition of multi - scale processing .","{'Task': ['classification', 'detection', 'ImageNet Large - Scale Visual Recognition Challenge 2014', 'ILSVRCâ\x80\x9914'], 'Method': ['deep convolutional neural network architecture codenamed â\x80\x9cInceptionâ\x80\x9d', 'intuition of multi - scale processing', 'convolutions', 'Hebbian principle'], 'Material': [], 'Metric': ['quality']}","<ANSWER>
{'Task': ['ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC''14)'], 'Method': ['deep convolutional neural network architecture', 'improved utilization of computing resources', 'carefully crafted design', 'Hebbian principle', 'intuition of multi-scale processing'], 'Material': [], 'Metric': ['state of the art for classification and detection']}
</ANSWER>","<ANSWER>
Task:
- ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC'14)
- Classification
- Detection

Method:
- Deep convolutional neural network architecture
- Improved utilization of computing resources
- Carefully crafted design
- Hebbian principle
- Multi-scale processing

Material:
- Computing resources

Metric:
- State of the art
- Computational budget
- Quality
</ANSWER>"
16051bbe3a7f7c77a952ebf76722ea655e8906ca,"Image Super - resolution via Feature - augmented Random Forest section : Abstract - Recent random - forest ( RF )- based image super - resolution approaches inherit some properties from dictionary - learning - based algorithms , but the effectiveness of the properties in RF is overlooked in the literature . In this paper , we present a novel feature - augmented random forest ( FARF ) for image super - resolution , where the conventional gradient - based features are augmented with gradient magnitudes and different feature recipes are formulated on different stages in an RF . The advantages of our method are that , firstly , the dictionary - learning - based features are enhanced by adding gradient magnitudes , based on the observation that the non - linear gradient magnitude are with highly discriminative property .",5,"['Task', 'Method', 'Material', 'Metric']","Image Super - resolution via Feature - augmented Random Forest section : Abstract - Recent random - forest ( RF )- based image super - resolution approaches inherit some properties from dictionary - learning - based algorithms , but the effectiveness of the properties in RF is overlooked in the literature . In this paper , we present a novel feature - augmented random forest ( FARF ) for image super - resolution , where the conventional gradient - based features are augmented with gradient magnitudes and different feature recipes are formulated on different stages in an RF . The advantages of our method are that , firstly , the dictionary - learning - based features are enhanced by adding gradient magnitudes , based on the observation that the non - linear gradient magnitude are with highly discriminative property .","{'Task': ['image super - resolution', 'Image Super - resolution'], 'Method': ['Recent random - forest', 'dictionary - learning - based features', 'RF', 'dictionary - learning - based algorithms', 'FARF', 'feature - augmented random forest', 'gradient - based features', 'Feature - augmented Random Forest'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['image super- resolution'], 'Method': ['random forest', 'feature-augmented random forest (FARF)'], 'Material': [], 'Metric': ['effectiveness', 'discriminative property']}
</ANSWER>","<ANSWER>
Task: Image Super-resolution
Method: Feature-augmented Random Forest (FARF)
Material:
- Conventional gradient-based features
- Gradient magnitudes
Metric: Discriminative property of non-linear gradient magnitudes
</ANSWER>"
160563abbd75265b19afc8b4169bab9e1eb33d97,"document : Massively Multilingual Sentence Embeddings for Zero - Shot Cross - Lingual Transfer and Beyond We introduce an architecture to learn joint multilingual sentence representations for 93 languages , belonging to more than 30 different language families and written in 28 different scripts . Our system uses a single BiLSTM encoder with a shared BPE vocabulary for all languages , which is coupled with an auxiliary decoder and trained on publicly available parallel corpora . This enables us to learn a classifier on top of the resulting sentence embeddings using English annotated data only , and transfer it to any of the 93 languages without any modification . Our approach sets a new state - of - the - art on zero - shot cross - lingual natural language inference for all the 14 languages in the XNLI dataset but one .",5,"['Task', 'Method', 'Material', 'Metric']","document : Massively Multilingual Sentence Embeddings for Zero - Shot Cross - Lingual Transfer and Beyond We introduce an architecture to learn joint multilingual sentence representations for 93 languages , belonging to more than 30 different language families and written in 28 different scripts . Our system uses a single BiLSTM encoder with a shared BPE vocabulary for all languages , which is coupled with an auxiliary decoder and trained on publicly available parallel corpora . This enables us to learn a classifier on top of the resulting sentence embeddings using English annotated data only , and transfer it to any of the 93 languages without any modification . Our approach sets a new state - of - the - art on zero - shot cross - lingual natural language inference for all the 14 languages in the XNLI dataset but one .","{'Task': ['Massively Multilingual Sentence Embeddings', 'sentence representations', 'Zero - Shot Cross - Lingual Transfer', 'cross - lingual natural language inference'], 'Method': ['auxiliary decoder', 'sentence embeddings', 'BiLSTM encoder', 'BPE', 'classifier'], 'Material': ['XNLI dataset'], 'Metric': []}","<ANSWER>
{'Task': ['zero-shot cross-lingual natural language inference'],
 'Method': ['learning joint multilingual sentence representations', 'using a single BiLSTM encoder with a shared BPE vocabulary', 'coupled with an auxiliary decoder', 'trained on publicly available parallel corpora', 'enables us to learn a classifier on top of the resulting sentence embeddings'],
 'Material': ['93 languages', 'belonging to more than 30 different language families and written in 28 different scripts', 'English annotated data'],
 'Metric': ['sets a new state-of-the-art']}
</ANSWER>","<ANSWER>
Task:
- Zero-shot Cross-lingual Natural Language Inference

Method:
- Joint multilingual sentence representations learning using a single BiLSTM encoder
- Shared BPE vocabulary for all languages
- Auxiliary decoder
- Transfer learning using English annotated data

Material:
- Parallel corpora for training
- BPE vocabulary

Metric:
- State-of-the-art performance on zero-shot cross-lingual natural language inference for 14 languages in the XNLI dataset
</ANSWER>"
175f74a09241b6cb5101a2a09978095720db7d5f,"document : Image Super - Resolution via Dual - State Recurrent Networks Advances in image super - resolution ( SR ) have recently benefited significantly from rapid developments in deep neural networks . Inspired by these recent discoveries , we note that many state - of - the - art deep SR architectures can be reformulated as a single - state recurrent neural network ( RNN ) with finite unfoldings . In this paper , we explore new structures for SR based on this compact RNN view , leading us to a dual - state design , the Dual - State Recurrent Network ( DSRN ) . Compared to its single - state counterparts that operate at a fixed spatial resolution , DSRN exploits both low - resolution ( LR ) and high - resolution ( HR ) signals jointly .",5,"['Task', 'Method', 'Material', 'Metric']","document : Image Super - Resolution via Dual - State Recurrent Networks Advances in image super - resolution ( SR ) have recently benefited significantly from rapid developments in deep neural networks . Inspired by these recent discoveries , we note that many state - of - the - art deep SR architectures can be reformulated as a single - state recurrent neural network ( RNN ) with finite unfoldings . In this paper , we explore new structures for SR based on this compact RNN view , leading us to a dual - state design , the Dual - State Recurrent Network ( DSRN ) . Compared to its single - state counterparts that operate at a fixed spatial resolution , DSRN exploits both low - resolution ( LR ) and high - resolution ( HR ) signals jointly .","{'Task': ['image super - resolution', 'Image Super - Resolution', 'HR', 'high - resolution', 'SR'], 'Method': ['finite unfoldings', 'deep neural networks', 'single - state recurrent neural network', 'compact RNN view', 'Dual - State Recurrent Network', 'DSRN', 'Dual - State Recurrent Networks', 'dual - state design', 'RNN'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['image super - resolution'], 'Method': ['single - state recurrent neural network ( RNN )', 'compact RNN view', 'dual - state design', 'Dual - State Recurrent Network ( DSRN )'], 'Material': [], 'Metric': ['spatial resolution']}
</ANSWER>","<ANSWER>
Task: Image Super-Resolution
Method: Single-state Recurrent Neural Network (RNN), Dual-state Design, Dual-State Recurrent Network (DSRN)
Material: Deep Neural Networks
Metric: Spatial resolution
</ANSWER>"
1778e32c18bd611169e64c1805a51abff341ca53,"document : Natural Language Inference over Interaction Space Natural Language Inference ( NLI ) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis . We introduce Interactive Inference Network ( IIN ) , a novel class of neural network architectures that is able to achieve high - level understanding of the sentence pair by hierarchically extracting semantic features from interaction space . We show that an interaction tensor ( attention weight ) contains semantic information to solve natural language inference , and a denser interaction tensor contains richer semantic information . One instance of such architecture , Densely Interactive Inference Network ( DIIN ) , demonstrates the state - of - the - art performance on large scale NLI copora and large - scale NLI alike corpus .",5,"['Task', 'Method', 'Material', 'Metric']","document : Natural Language Inference over Interaction Space Natural Language Inference ( NLI ) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis . We introduce Interactive Inference Network ( IIN ) , a novel class of neural network architectures that is able to achieve high - level understanding of the sentence pair by hierarchically extracting semantic features from interaction space . We show that an interaction tensor ( attention weight ) contains semantic information to solve natural language inference , and a denser interaction tensor contains richer semantic information . One instance of such architecture , Densely Interactive Inference Network ( DIIN ) , demonstrates the state - of - the - art performance on large scale NLI copora and large - scale NLI alike corpus .","{'Task': ['Interaction Space', 'NLI', 'natural language inference', 'Natural Language Inference'], 'Method': ['DIIN', 'Densely Interactive Inference Network', 'IIN )', 'Interactive Inference Network', 'neural network architectures'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['Natural Language Inference'], 'Method': ['Interactive Inference Network', 'Densely Interactive Inference Network'], 'Material': ['sentence pair', 'interaction space', 'interaction tensor', 'attention weight'], 'Metric': ['high-level understanding', 'state-of-the-art performance']}
</ANSWER>","<ANSWER>
Task: Natural Language Inference (NLI)
Method: Interactive Inference Network (IIN), Densely Interactive Inference Network (DIIN)
Material: Interaction tensor (attention weight)
Metric: Performance on large scale NLI corpora and large-scale NLI alike corpus
</ANSWER>"
178275dbdcfa267e41a9d5efe386ee5874c6d23f,"document : Fraternal Dropout Recurrent neural networks ( RNNs ) form an important class of architectures among neural networks useful for language modeling and sequential prediction . However , optimizing RNNs is known to be harder compared to feed - forward neural networks . A number of techniques have been proposed in literature to address this problem . In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal .",5,"['Task', 'Method', 'Material', 'Metric']","document : Fraternal Dropout Recurrent neural networks ( RNNs ) form an important class of architectures among neural networks useful for language modeling and sequential prediction . However , optimizing RNNs is known to be harder compared to feed - forward neural networks . A number of techniques have been proposed in literature to address this problem . In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal .","{'Task': ['optimizing RNNs', 'language modeling', 'sequential prediction'], 'Method': ['Recurrent neural networks', 'RNNs', 'feed - forward neural networks', 'neural networks', 'fraternal dropout', 'dropout'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['language modeling', 'sequential prediction', 'optimizing RNNs'],
 'Method': ['fraternal dropout'],
 'Material': ['RNNs'],
 'Metric': ['goal']}
</ANSWER>","<ANSWER>
Task: optimizing RNNs, language modeling, sequential prediction
Method: fraternal dropout, dropout
Material: RNNs (Recurrent neural networks)
Metric: not explicitly mentioned in the document
</ANSWER>"
178631e0f0e624b1607c7a7a2507ed30d4e83a42,"document : Speech Recognition with Deep Recurrent Neural Networks Recurrent neural networks ( RNNs ) are a powerful model for sequential data . End - to - end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input - output alignment is unknown . The combination of these methods with the Long Short - term Memory RNN architecture has proved particularly fruitful , delivering state - of - the - art results in cursive handwriting recognition . However RNN performance in speech recognition has so far been disappointing , with better results returned by deep feedforward networks .",5,"['Task', 'Method', 'Material', 'Metric']","document : Speech Recognition with Deep Recurrent Neural Networks Recurrent neural networks ( RNNs ) are a powerful model for sequential data . End - to - end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input - output alignment is unknown . The combination of these methods with the Long Short - term Memory RNN architecture has proved particularly fruitful , delivering state - of - the - art results in cursive handwriting recognition . However RNN performance in speech recognition has so far been disappointing , with better results returned by deep feedforward networks .","{'Task': ['sequence labelling problems', 'Speech Recognition', 'handwriting recognition', 'speech'], 'Method': ['Connectionist Temporal Classification', 'Recurrent neural networks', 'deep feedforward networks', 'RNNs', 'end training methods', 'Deep Recurrent Neural Networks', 'RNN', 'Long Short - term Memory RNN architecture'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['sequence labelling', 'speech recognition'], 'Method': ['End-to-end training', 'Connectionist Temporal Classification', 'Long Short-term Memory RNN architecture'], 'Material': ['recurrent neural networks (RNNs)'], 'Metric': ['state-of-the-art results']}
</ANSWER>","<ANSWER>
Task: [""sequence labelling"", ""cursive handwriting recognition"", ""speech recognition""]
Method: [""End-to-end training methods"", ""Connectionist Temporal Classification"", ""Long Short-term Memory RNN architecture""]
Material: []
Metric: [""state-of-the-art results""]
</ANSWER>"
18168aea48a22f6fe2fe407c0ff70083cba225a7,"document : Image Restoration Using Very Deep Convolutional Encoder - Decoder Networks with Symmetric Skip Connections In this paper , we propose a very deep fully convolutional encoding - decoding framework for image restoration such as denoising and super - resolution . The network is composed of multiple layers of convolution and de - convolution operators , learning end - to - end mappings from corrupted images to the original ones . The convolutional layers act as the feature extractor , which capture the abstraction of image contents while eliminating noises / corruptions . De - convolutional layers are then used to recover the image details .",5,"['Task', 'Method', 'Material', 'Metric']","document : Image Restoration Using Very Deep Convolutional Encoder - Decoder Networks with Symmetric Skip Connections In this paper , we propose a very deep fully convolutional encoding - decoding framework for image restoration such as denoising and super - resolution . The network is composed of multiple layers of convolution and de - convolution operators , learning end - to - end mappings from corrupted images to the original ones . The convolutional layers act as the feature extractor , which capture the abstraction of image contents while eliminating noises / corruptions . De - convolutional layers are then used to recover the image details .","{'Task': ['image restoration', 'super - resolution', 'denoising', 'Image Restoration'], 'Method': ['Symmetric Skip Connections', 'convolutional layers', 'Deep Convolutional Encoder - Decoder Networks', 'deep fully convolutional encoding - decoding framework', 'De - convolutional layers', 'feature extractor', 'layers of convolution and de - convolution operators'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['image restoration', 'denoising', 'super-resolution'],
 'Method': ['very deep fully convolutional encoding-decoding framework', 'convolution and de-convolution operators', 'learning end-to-end mappings'],
 'Material': ['corrupted images'],
 'Metric': ['original images']}
</ANSWER>","<ANSWER>
Task: [""Image Restoration"", ""Denoising"", ""Super-Resolution""]
Method: [""Very Deep Fully Convolutional Encoding-Decoding Framework""]
Material: [""Multiple layers of convolution and de-convolution operators""]
Metric: [""End-to-end mappings from corrupted images to the original ones""]
</ANSWER>"
193089d56758ab88391d846edd08d359b1f9a863,"document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( CNN ) in person re - identification ( re - ID ) , i.e. , verification and identification models . The two models have their respective advantages and limitations due to different loss functions . In this paper , we shed light on how to combine the two models to learn more discriminative pedestrian descriptors . Specifically , we propose a siamese network that simultaneously computes the identification loss and verification loss .",5,"['Task', 'Method', 'Material', 'Metric']","document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( CNN ) in person re - identification ( re - ID ) , i.e. , verification and identification models . The two models have their respective advantages and limitations due to different loss functions . In this paper , we shed light on how to combine the two models to learn more discriminative pedestrian descriptors . Specifically , we propose a siamese network that simultaneously computes the identification loss and verification loss .","{'Task': ['discriminative pedestrian descriptors', 'verification', 'Person Re - identification', 'identification models', 'person re - identification', 're - ID'], 'Method': ['siamese network', 'Discriminatively Learned CNN Embedding', 'CNN', 'convolutional neural networks'], 'Material': [], 'Metric': ['identification loss']}","<ANSWER>
{'Task': ['person re-identification', 'verification', 'identification'],
 'Method': ['convolutional neural networks (CNN)', 'siamese network', 'discriminatively learned CNN embedding'],
 'Material': [Ellipsis],
 'Metric': ['identification loss', 'verification loss']}
</ANSWER>","<ANSWER>
Task:
- Person re-identification
- Verification
- Identification

Method:
- Convolutional Neural Networks (CNN)
- Siamese network
- Discriminatively Learned CNN Embedding

Material:
- Pedestrian descriptors

Metric:
- Identification loss
- Verification loss
</ANSWER>"
193b518bc3025804c6d587c74cbc154d91478417,"document : Learning to Adapt Structured Output Space for Semantic Segmentation Convolutional neural network - based approaches for semantic segmentation rely on supervision with pixel - level ground truth , but may not generalize well to unseen image domains . As the labeling process is tedious and labor intensive , developing algorithms that can adapt source ground truth labels to the target domain is of great interest . In this paper , we propose an adversarial learning method for domain adaptation in the context of semantic segmentation . Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains , we adopt adversarial learning in the output space .",5,"['Task', 'Method', 'Material', 'Metric']","document : Learning to Adapt Structured Output Space for Semantic Segmentation Convolutional neural network - based approaches for semantic segmentation rely on supervision with pixel - level ground truth , but may not generalize well to unseen image domains . As the labeling process is tedious and labor intensive , developing algorithms that can adapt source ground truth labels to the target domain is of great interest . In this paper , we propose an adversarial learning method for domain adaptation in the context of semantic segmentation . Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains , we adopt adversarial learning in the output space .","{'Task': ['semantic segmentation', 'Semantic Segmentation', 'labeling process', 'semantic segmentations'], 'Method': ['adversarial learning', 'Convolutional neural network - based approaches', 'adversarial learning method', 'domain adaptation'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['semantic segmentation', 'developing algorithms', 'adapt source ground truth labels to the target domain'],
 'Method': ['adversarial learning method', 'adversarial learning in the output space'],
 'Material': [],
 'Metric': []}
</ANSWER>","<ANSWER>
Task: Semantic segmentation, Domain adaptation
Method: Adversarial learning
Material: Convolutional neural networks, Pixel-level ground truth, Source ground truth labels
Metric: Spatial similarities between source and target domains
</ANSWER>"
1d0dcb458aa4d30b51f7c74b159be687f39120a0,"document : Pose - driven Deep Convolutional Model for Person Re - identification Feature extraction and matching are two crucial components in person Re - Identification ( ReID ) . The large pose deformations and the complex view variations exhibited by the captured person images significantly increase the difficulty of learning and matching of the features from person images . To overcome these difficulties , in this work we propose a Pose - driven Deep Convolutional ( PDC ) model to learn improved feature extraction and matching models from end to end . Our deep architecture explicitly leverages the human part cues to alleviate the pose variations and learn robust feature representations from both the global image and different local parts .",5,"['Task', 'Method', 'Material', 'Metric']","document : Pose - driven Deep Convolutional Model for Person Re - identification Feature extraction and matching are two crucial components in person Re - Identification ( ReID ) . The large pose deformations and the complex view variations exhibited by the captured person images significantly increase the difficulty of learning and matching of the features from person images . To overcome these difficulties , in this work we propose a Pose - driven Deep Convolutional ( PDC ) model to learn improved feature extraction and matching models from end to end . Our deep architecture explicitly leverages the human part cues to alleviate the pose variations and learn robust feature representations from both the global image and different local parts .","{'Task': ['learning and matching of the features', 'matching', 'person Re - Identification', 'ReID', 'Person Re - identification', 'Feature extraction'], 'Method': ['deep architecture', 'PDC', 'robust feature representations', 'feature extraction', 'Pose - driven Deep Convolutional', 'matching models'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['Person Re-Identification'], 'Method': ['Pose-driven Deep Convolutional (PDC) model', 'feature extraction and matching'], 'Material': ['person images'], 'Metric': ['difficulty of learning and matching of the features']}
</ANSWER>","<ANSWER>
Task: Person Re-Identification (ReID)
Method: Pose-driven Deep Convolutional (PDC) model
Material: Human part cues, global image, local parts
Metric: Not explicitly mentioned in the document
</ANSWER>"
1f08598381af9146d0fd9a61b30d0e51a7331689,"document : Distributed Prioritized Experience Replay We propose a distributed architecture for deep reinforcement learning at scale , that enables agents to learn effectively from orders of magnitude more data than previously possible . The algorithm decouples acting from learning : the actors interact with their own instances of the environment by selecting actions according to a shared neural network , and accumulate the resulting experience in a shared experience replay memory ; the learner replays samples of experience and updates the neural network . The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors . Our architecture substantially improves the state of the art on the Arcade Learning Environment , achieving better final performance in a fraction of the wall - clock training time .",5,"['Task', 'Method', 'Material', 'Metric']","document : Distributed Prioritized Experience Replay We propose a distributed architecture for deep reinforcement learning at scale , that enables agents to learn effectively from orders of magnitude more data than previously possible . The algorithm decouples acting from learning : the actors interact with their own instances of the environment by selecting actions according to a shared neural network , and accumulate the resulting experience in a shared experience replay memory ; the learner replays samples of experience and updates the neural network . The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors . Our architecture substantially improves the state of the art on the Arcade Learning Environment , achieving better final performance in a fraction of the wall - clock training time .","{'Task': ['Distributed Prioritized Experience Replay', 'deep reinforcement learning', 'learning', 'prioritized experience replay'], 'Method': ['shared neural network', 'distributed architecture', 'neural network'], 'Material': ['Arcade Learning Environment'], 'Metric': ['wall - clock training time']}","<ANSWER>
{'Task': ['deep reinforcement learning', 'learning'],
 'Method': ['distributed architecture', 'decouples acting from learning', 'prioritized experience replay'],
 'Material': [],
 'Metric': ['final performance', 'wall - clock training time']
}
</ANSWER>","<ANSWER>
Task:
- Deep Reinforcement Learning
- Learning from large scale data

Method:
- Distributed architecture
- Decoupling acting from learning
- Shared neural network
- Prioritized experience replay

Material:
- Neural network
- Experience replay memory

Metric:
- Final performance
- Wall-clock training time
</ANSWER>"
2138a7127429d67746ec78de46d6820fee0e548e,"document : Graph2Seq : Graph to Sequence Learning with Attention - Based Neural Networks The celebrated Sequence to Sequence learning ( Seq2Seq ) technique and its numerous variants achieve excellent performance on many tasks . However , many machine learning tasks have inputs naturally represented as graphs ; existing Seq2Seq models face a significant challenge in achieving accurate conversion from graph form to the appropriate sequence . To address this challenge , we introduce a novel general end - to - end graph - to - sequence neural encoder - decoder model that maps an input graph to a sequence of vectors and uses an attention - based LSTM method to decode the target sequence from these vectors . Our method first generates the node and graph embeddings using an improved graph - based neural network with a novel aggregation strategy to incorporate edge direction information in the node embeddings .",5,"['Task', 'Method', 'Material', 'Metric']","document : Graph2Seq : Graph to Sequence Learning with Attention - Based Neural Networks The celebrated Sequence to Sequence learning ( Seq2Seq ) technique and its numerous variants achieve excellent performance on many tasks . However , many machine learning tasks have inputs naturally represented as graphs ; existing Seq2Seq models face a significant challenge in achieving accurate conversion from graph form to the appropriate sequence . To address this challenge , we introduce a novel general end - to - end graph - to - sequence neural encoder - decoder model that maps an input graph to a sequence of vectors and uses an attention - based LSTM method to decode the target sequence from these vectors . Our method first generates the node and graph embeddings using an improved graph - based neural network with a novel aggregation strategy to incorporate edge direction information in the node embeddings .","{'Task': ['conversion', 'machine learning tasks', 'Graph to Sequence Learning'], 'Method': ['LSTM', 'node and graph embeddings', 'graph - based neural network', 'Seq2Seq', 'end - to - end graph - to - sequence neural encoder - decoder model', 'aggregation strategy', 'Sequence to Sequence learning', 'Attention - Based Neural Networks', 'Graph2Seq'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['graph-to-sequence learning', 'achieving accurate conversion from graph form to the appropriate sequence'],
 'Method': ['Seq2Seq technique', 'numerous variants', 'introduce a novel general end-to-end graph-to-sequence neural encoder-decoder model', 'maps an input graph to a sequence of vectors', 'uses an attention-based LSTM method to decode the target sequence from these vectors', 'generates the node and graph embeddings using an improved graph-based neural network', 'novel aggregation strategy to incorporate edge direction information in the node embeddings'],
 'Material': [],
 'Metric': []
}
</ANSWER>","<ANSWER>
Task: Graph-to-Sequence learning
Method:
- Sequence to Sequence learning
- Neural encoder-decoder model
- Attention-based LSTM method
- Improved graph-based neural network
- Novel aggregation strategy
Material: Graph form
Metric: Accuracy
</ANSWER>"
21a1654b856cf0c64e60e58258669b374cb05539,"document : You Only Look Once : Unified , Real - Time Object Detection We present YOLO , a new approach to object detection . Prior work on object detection repurposes classifiers to perform detection . Instead , we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities . A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation .",5,"['Task', 'Method', 'Material', 'Metric']","document : You Only Look Once : Unified , Real - Time Object Detection We present YOLO , a new approach to object detection . Prior work on object detection repurposes classifiers to perform detection . Instead , we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities . A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation .","{'Task': ['Real - Time Object Detection', 'regression problem', 'detection', 'object detection'], 'Method': ['neural network', 'classifiers', 'YOLO'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['object detection'], 'Method': ['framing object detection as a regression problem', 'predicting bounding boxes and class probabilities directly from full images in one evaluation'], 'Material': [], 'Metric': ['spatially separated bounding boxes', 'associated class probabilities']}
</ANSWER>","<ANSWER>
Task: Object Detection
Method: Framing object detection as a regression problem, using a single neural network to predict bounding boxes and class probabilities directly from full images
Material: Neural network
Metric: Spatially separated bounding boxes, associated class probabilities
</ANSWER>"
232b43584b2236669c0a53702ad89ab10c3886ea,"In this work , we build on recent advances in distributional reinforcement learning to give a generally applicable , flexible , and state - of - the - art distributional variant of DQN . We achieve this by using quantile regression to approximate the full quantile function for the state - action return distribution . By reparameterizing a distribution over the sample space , this yields an implicitly defined return distribution and gives rise to a large class of risk - sensitive policies . We demonstrate improved performance on the 57 Atari 2600 games in the ALE , and use our algorithm ’s implicitly defined distributions to study the effects of risk - sensitive policies in Atari games . ImplicitQuantileNetworksforDistributionalReinforcementLearning",5,"['Task', 'Method', 'Material', 'Metric']","In this work , we build on recent advances in distributional reinforcement learning to give a generally applicable , flexible , and state - of - the - art distributional variant of DQN . We achieve this by using quantile regression to approximate the full quantile function for the state - action return distribution . By reparameterizing a distribution over the sample space , this yields an implicitly defined return distribution and gives rise to a large class of risk - sensitive policies . We demonstrate improved performance on the 57 Atari 2600 games in the ALE , and use our algorithm ’s implicitly defined distributions to study the effects of risk - sensitive policies in Atari games . ImplicitQuantileNetworksforDistributionalReinforcementLearning","{'Task': ['risk - sensitive policies', 'Atari games'], 'Method': ['risk - sensitive policies', 'distributional variant', 'reinforcement learning', 'quantile regression', 'DQN'], 'Material': ['Atari 2600 games'], 'Metric': []}","<ANSWER>
{'Task': ['Atari 2600 games in the ALE'], 'Method': ['distributional reinforce learning', 'quantile regression', 'ImplicitQuantileNetworks'], 'Material': [], 'Metric': ['performance']}
</ANSWER>","<ANSWER>
Task: Building a distributional variant of Deep Q-Network (DQN) for Atari 2600 games using distributional reinforcement learning.
Method: Using quantile regression to approximate the full quantile function for the state-action return distribution and reparameterizing a distribution over the sample space to yield an implicitly defined return distribution.
Material: Distributional reinforcement learning, Atari 2600 games, Deep Q-Network (DQN), Quantile regression.
Metric: Improved performance on the 57 Atari 2600 games in the ALE, effects of risk-sensitive policies in Atari games.
</ANSWER>"
23d2d3a6ffebfecaa8930307fdcf451c147757c8,"document : SeqGAN : Sequence Generative Adversarial Nets with Policy Gradient As a new way of training generative models , Generative Adversarial Net ( GAN ) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real - valued data . However , it has limitations when the goal is for generating sequences of discrete tokens . A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model . Also , the discriminative model can only assess a complete sequence , while for a partially generated sequence , it is non - trivial to balance its current score and the future one once the entire sequence has been generated .",5,"['Task', 'Method', 'Material', 'Metric']","document : SeqGAN : Sequence Generative Adversarial Nets with Policy Gradient As a new way of training generative models , Generative Adversarial Net ( GAN ) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real - valued data . However , it has limitations when the goal is for generating sequences of discrete tokens . A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model . Also , the discriminative model can only assess a complete sequence , while for a partially generated sequence , it is non - trivial to balance its current score and the future one once the entire sequence has been generated .","{'Task': ['generating sequences of discrete tokens'], 'Method': ['generative models', 'discriminative model', 'Sequence Generative Adversarial Nets', 'generative model', 'Policy Gradient', 'Generative Adversarial Net', 'GAN', 'SeqGAN'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['training generative models', 'generating sequences of discrete tokens'],
 'Method': ['Generative Adversarial Net (GAN)', 'Sequence Generative Adversarial Nets (SeqGAN)', 'policy gradient'],
 'Material': [],
 'Metric': ['score']}
</ANSWER>","<ANSWER>
Task: Generating sequences of discrete tokens
Method: Sequence Generative Adversarial Nets (SeqGAN), Policy Gradient
Material: Generative Model, Discriminative Model
Metric: Current score, Future score
</ANSWER>

Task: Generating sequences of discrete tokens
Method: SeqGAN, Policy Gradient
Material: Generative Model, Discriminative Model
Metric: Current sequence score, Future sequence score"
24730424236724d3f798dec02901e7a1f1c4710e,"Joint Maximum Purity Forest with Application to Image Super - Resolution section : Abstract - In this paper , we propose a novel random - forest scheme , namely Joint Maximum Purity Forest ( JMPF ) , for classification , clustering , and regression tasks . In the JMPF scheme , the original feature space is transformed into a compactly pre - clustered feature space , via a trained rotation matrix . The rotation matrix is obtained through an iterative quantization process , where the input data belonging to different classes are clustered to the respective vertices of the new feature space with maximum purity .",5,"['Task', 'Method', 'Material', 'Metric']","Joint Maximum Purity Forest with Application to Image Super - Resolution section : Abstract - In this paper , we propose a novel random - forest scheme , namely Joint Maximum Purity Forest ( JMPF ) , for classification , clustering , and regression tasks . In the JMPF scheme , the original feature space is transformed into a compactly pre - clustered feature space , via a trained rotation matrix . The rotation matrix is obtained through an iterative quantization process , where the input data belonging to different classes are clustered to the respective vertices of the new feature space with maximum purity .","{'Task': ['Image Super - Resolution', 'classification , clustering , and regression tasks'], 'Method': ['Joint Maximum Purity Forest', 'iterative quantization process', 'random - forest scheme', 'JMPF'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['classification', 'clustering', 'regression'], 'Method': ['Joint Maximum Purity Forest (JMPF)', 'iterative quantization process', 'trained rotation matrix'], 'Material': [], 'Metric': ['maximum purity']}
</ANSWER>","<ANSWER>
Task: [""classification"", ""clustering"", ""regression""]
Method: [""Joint Maximum Purity Forest (JMPF)"", ""iterative quantization process"", ""trained rotation matrix""]
Material: []
Metric: [""purity""]
</ANSWER>"
249b3b7421d3cdb932eecfe4b67203e0e46806b2,"document : Cell - aware Stacked LSTMs for Modeling Sentences We propose a method of stacking multiple long short - term memory ( LSTM ) layers for modeling sentences . In contrast to the conventional stacked LSTMs where only hidden states are fed as input to the next layer , our architecture accepts both hidden and memory cell states of the preceding layer and fuses information from the left and the lower context using the soft gating mechanism of LSTMs . Thus the proposed stacked LSTM architecture modulates the amount of information to be delivered not only in horizontal recurrence but also in vertical connections , from which useful features extracted from lower layers are effectively conveyed to upper layers . We dub this architecture Cell - aware Stacked LSTM ( CAS - LSTM ) and show from experiments that our models achieve state - of - the - art results on benchmark datasets for natural language inference , paraphrase detection , and sentiment classification .",5,"['Task', 'Method', 'Material', 'Metric']","document : Cell - aware Stacked LSTMs for Modeling Sentences We propose a method of stacking multiple long short - term memory ( LSTM ) layers for modeling sentences . In contrast to the conventional stacked LSTMs where only hidden states are fed as input to the next layer , our architecture accepts both hidden and memory cell states of the preceding layer and fuses information from the left and the lower context using the soft gating mechanism of LSTMs . Thus the proposed stacked LSTM architecture modulates the amount of information to be delivered not only in horizontal recurrence but also in vertical connections , from which useful features extracted from lower layers are effectively conveyed to upper layers . We dub this architecture Cell - aware Stacked LSTM ( CAS - LSTM ) and show from experiments that our models achieve state - of - the - art results on benchmark datasets for natural language inference , paraphrase detection , and sentiment classification .","{'Task': ['natural language inference', 'paraphrase detection', 'modeling sentences', 'sentiment classification', 'Modeling Sentences'], 'Method': ['LSTMs', 'Cell - aware Stacked LSTM', 'LSTM', 'CAS - LSTM', 'multiple long short - term memory', 'soft gating mechanism'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['natural language inference', 'paraphrase detection', 'sentiment classification'],
 'Method': ['stacking multiple long short - term memory (LSTM) layers', 'proposed stacked LSTM architecture', 'Cell - aware Stacked LSTM (CAS - LSTM)', 'soft gating mechanism of LSTMs'],
 'Material': ['hidden states', 'memory cell states'],
 'Metric': ['state - of - the - art results']}
</ANSWER>","<ANSWER>
Task:
- Natural Language Inference
- Paraphrase Detection
- Sentiment Classification

Method:
- Cell-aware Stacked LSTMs
- Stacking multiple LSTM layers
- Accepting both hidden and memory cell states
- Fusing information using soft gating mechanism

Material:
- Long short-term memory (LSTM) layers
- Hidden states
- Memory cell states

Metric:
- State-of-the-art results
</ANSWER>"
25a784f7f8c94c42821ee078587fc38dffcd00a4,"document : Robust Face Detection via Learning Small Faces on Hard Images Recent anchor - based deep face detectors have achieved promising performance , but they are still struggling to detect hard faces , such as small , blurred and partially occluded faces . A reason is that they treat all images and faces equally , without putting more effort on hard ones ; however , many training images only contain easy faces , which are less helpful to achieve better performance on hard images . In this paper , we propose that the robustness of a face detector against hard faces can be improved by learning small faces on hard images . Our intuitions are ( 1 ) hard images are the images which contain at least one hard face , thus they facilitate training robust face detectors ; ( 2 ) most hard faces are small faces and other types of hard faces can be easily converted to small faces by shrinking .",5,"['Task', 'Method', 'Material', 'Metric']","document : Robust Face Detection via Learning Small Faces on Hard Images Recent anchor - based deep face detectors have achieved promising performance , but they are still struggling to detect hard faces , such as small , blurred and partially occluded faces . A reason is that they treat all images and faces equally , without putting more effort on hard ones ; however , many training images only contain easy faces , which are less helpful to achieve better performance on hard images . In this paper , we propose that the robustness of a face detector against hard faces can be improved by learning small faces on hard images . Our intuitions are ( 1 ) hard images are the images which contain at least one hard face , thus they facilitate training robust face detectors ; ( 2 ) most hard faces are small faces and other types of hard faces can be easily converted to small faces by shrinking .","{'Task': ['Learning Small Faces', 'Robust Face Detection'], 'Method': ['face detectors', 'anchor - based deep face detectors', 'face detector'], 'Material': ['Hard Images', 'hard images'], 'Metric': ['robustness']}","<ANSWER>
{'Task': ['improving robustness of face detectors', 'detecting hard faces'],
 'Method': ['learning small faces on hard images'],
 'Material': ['hard images', 'training images'],
 'Metric': ['performance']}
</ANSWER>","<ANSWER>
Task: Face Detection, Robust Face Detection
Method: Anchor-based deep learning, Learning small faces on hard images
Material: Hard images, Training images
Metric: Performance (on detecting hard faces), Robustness
</ANSWER>"
25f5df29342a04936ba0d308b4d1b8245a7e8f5c,Convolutional Pose Machines section : Abstract Pose Machines provide a sequential prediction framework for learning rich implicit spatial models . In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image - dependent spatial models for the task of pose estimation . The contribution of this paper is to implicitly model long - range dependencies between variables in structured prediction tasks such as articulated pose estimation .,5,"['Task', 'Method', 'Material', 'Metric']",Convolutional Pose Machines section : Abstract Pose Machines provide a sequential prediction framework for learning rich implicit spatial models . In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image - dependent spatial models for the task of pose estimation . The contribution of this paper is to implicitly model long - range dependencies between variables in structured prediction tasks such as articulated pose estimation .,"{'Task': ['structured prediction tasks', 'long - range dependencies between variables', 'learning image features', 'pose estimation'], 'Method': ['Convolutional Pose Machines', 'convolutional networks', 'Pose Machines', 'sequential prediction framework', 'image - dependent spatial models', 'pose machine', 'implicit spatial models'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['pose estimation'], 'Method': ['convolutional networks', 'pose machine framework', 'structured prediction tasks', 'articulated pose estimation'], 'Material': ['image features', 'image-dependent spatial models'], 'Metric': ['implicitly modeling long-range dependencies']}
</ANSWER>","<ANSWER>
Task: pose estimation
Method: Convolutional networks, Sequential prediction framework, Structured prediction tasks, Articulated pose estimation
Material: Implicit spatial models, Image features, Image-dependent spatial models
Metric: Long-range dependencies between variables
</ANSWER>"
269730dbbabed8b8b5ba720e44a4c31b1f51e8f1,"Published as a conference paper at ICLR 2017 QUERY - REDUCTION NETWORKS FOR QUESTION ANSWERING section : ABSTRACT In this paper , we study the problem of question answering when reasoning over multiple facts is required . We propose Query - Reduction Network ( QRN ) , a variant of Recurrent Neural Network ( RNN ) that effectively handles both short - term ( local ) and long - term ( global ) sequential dependencies to reason over multiple facts . QRN considers the context sentences as a sequence of state - changing triggers , and reduces the original query to a more informed query as it observes each trigger ( context sentence ) through time .",5,"['Task', 'Method', 'Material', 'Metric']","Published as a conference paper at ICLR 2017 QUERY - REDUCTION NETWORKS FOR QUESTION ANSWERING section : ABSTRACT In this paper , we study the problem of question answering when reasoning over multiple facts is required . We propose Query - Reduction Network ( QRN ) , a variant of Recurrent Neural Network ( RNN ) that effectively handles both short - term ( local ) and long - term ( global ) sequential dependencies to reason over multiple facts . QRN considers the context sentences as a sequence of state - changing triggers , and reduces the original query to a more informed query as it observes each trigger ( context sentence ) through time .","{'Task': ['QUESTION ANSWERING', 'question answering'], 'Method': ['QUERY - REDUCTION NETWORKS', 'QRN', 'Query - Reduction Network', 'Recurrent Neural Network', 'RNN'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['question answering'], 'Method': ['Query-Reduction Network (QRN)', 'variant of Recurrent Neural Network (RNN)'], 'Material': ['multiple facts'], 'Metric': ['short-term (local) and long-term (global) sequential dependencies']}
</ANSWER>","<ANSWER>
Task: Question Answering
Method: Query-Reduction Network (QRN), a variant of Recurrent Neural Network (RNN)
Material: Sequence of state-changing triggers (context sentences)
Metric: Not explicitly mentioned in the abstract
</ANSWER>"
270e65acc071b9e4e2a632720130c0e10cb6fa08,"document : Neural Tree Indexers for Text Understanding Recurrent neural networks ( RNNs ) process input text sequentially and model the conditional transition between word tokens . In contrast , the advantages of recursive networks include that they explicitly model the compositionality and the recursive structure of natural language . However , the current recursive architecture is limited by its dependence on syntactic tree . In this paper , we introduce a robust syntactic parsing - independent tree structured model , Neural Tree Indexers ( NTI ) that provides a middle ground between the sequential RNNs and the syntactic tree - based recursive models .",5,"['Task', 'Method', 'Material', 'Metric']","document : Neural Tree Indexers for Text Understanding Recurrent neural networks ( RNNs ) process input text sequentially and model the conditional transition between word tokens . In contrast , the advantages of recursive networks include that they explicitly model the compositionality and the recursive structure of natural language . However , the current recursive architecture is limited by its dependence on syntactic tree . In this paper , we introduce a robust syntactic parsing - independent tree structured model , Neural Tree Indexers ( NTI ) that provides a middle ground between the sequential RNNs and the syntactic tree - based recursive models .","{'Task': ['Text Understanding'], 'Method': ['tree', 'Recurrent neural networks', 'Neural Tree Indexers', 'RNNs', 'NTI', 'recursive networks', 'recursive architecture'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['text understanding', 'introducing a new model'],
 'Method': ['processing input text sequentially (RNNs)', 'modeling conditional transition between word tokens', 'explicitly modeling compositionality and recursive structure of natural language', 'introducing Neural Tree Indexers (NTI)'],
 'Material': ['recurrent neural networks (RNNs)', 'syntactic tree', 'syntactic parsing'],
 'Metric': ['robustness']
}
</ANSWER>","<ANSWER>
Task:
- Text understanding

Method:
- Recurrent neural networks (RNNs)
- Recursive networks
- Neural Tree Indexers (NTI)

Material:
- Input text
- Word tokens
- Syntactic tree

Metric:
- Conditional transition
- Compositionality
- Recursive structure
</ANSWER>"
2777cd26b2c257843273fe41ad4c5b8cdf1b1b75,"document : Understanding Humans in Crowded Scenes : Deep Nested Adversarial Learning and A New Benchmark for Multi - Human Parsing Despite the noticeable progress in perceptual tasks like detection , instance segmentation and human parsing , computers still perform unsatisfactorily on visually understanding humans in crowded scenes , such as group behavior analysis , person re - identification and autonomous driving , etc . To this end , models need to comprehensively perceive the semantic information and the differences between instances in a multi - human image , which is recently defined as the multi - human parsing task . In this paper , we present a new large - scale database “ M ulti - H uman",5,"['Task', 'Method', 'Material', 'Metric']","document : Understanding Humans in Crowded Scenes : Deep Nested Adversarial Learning and A New Benchmark for Multi - Human Parsing Despite the noticeable progress in perceptual tasks like detection , instance segmentation and human parsing , computers still perform unsatisfactorily on visually understanding humans in crowded scenes , such as group behavior analysis , person re - identification and autonomous driving , etc . To this end , models need to comprehensively perceive the semantic information and the differences between instances in a multi - human image , which is recently defined as the multi - human parsing task . In this paper , we present a new large - scale database “ M ulti - H uman","{'Task': ['autonomous driving', 'detection', 'instance segmentation', 'group behavior analysis', 'Understanding Humans in Crowded Scenes', 'human parsing', 'visually understanding humans in crowded scenes', 'perceptual tasks', 'person re - identification', 'Multi - Human Parsing', 'M ulti -'], 'Method': ['Deep Nested Adversarial Learning'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['multi-human parsing', 'group behavior analysis', 'person re-identification', 'autonomous driving'],
 'Method': ['Deep Nested Adversarial Learning'],
 'Material': ['multi-human image'],
 'Metric': ['semantic information', 'differences between instances']
}
</ANSWER>","<ANSWER>
Task:
- multi-human parsing

Method:
- Deep Nested Adversarial Learning

Material:
- multi-human image

Metric:
- semantic information
- differences between instances
</ANSWER>"
27a99c21a1324f087b2f144adc119f04137dfd87,"document : Deep Fried Convnets The fully - connected layers of deep convolutional neural networks typically contain over 90 % of the network parameters . Reducing the number of parameters while preserving predictive performance is critically important for training big models in distributed systems and for deployment in embedded devices . In this paper , we introduce a novel Adaptive Fastfood transform to reparameterize the matrix - vector multiplication of fully connected layers . Reparameterizing a fully connected layer with inputs and outputs with the Adaptive Fastfood transform reduces the storage and computational costs costs from to and respectively .",5,"['Task', 'Method', 'Material', 'Metric']","document : Deep Fried Convnets The fully - connected layers of deep convolutional neural networks typically contain over 90 % of the network parameters . Reducing the number of parameters while preserving predictive performance is critically important for training big models in distributed systems and for deployment in embedded devices . In this paper , we introduce a novel Adaptive Fastfood transform to reparameterize the matrix - vector multiplication of fully connected layers . Reparameterizing a fully connected layer with inputs and outputs with the Adaptive Fastfood transform reduces the storage and computational costs costs from to and respectively .","{'Task': ['distributed systems', 'embedded devices'], 'Method': ['Deep Fried Convnets', 'matrix - vector multiplication of fully connected layers', 'big models', 'fully - connected layers of deep convolutional neural networks', 'Adaptive Fastfood transform', 'fully connected layer'], 'Material': [], 'Metric': ['predictive performance', 'storage and computational costs costs']}","<ANSWER>
{'Task': ['training big models', 'deployment'],
 'Method': ['Adaptive Fastfood transform', 'reparameterizing a fully connected layer'],
 'Material': ['big models', 'distributed systems', 'embedded devices'],
 'Metric': ['predictive performance', 'storage costs', 'computational costs']}
</ANSWER>","<ANSWER>
Task: Introducing a novel method for reducing parameters in deep convolutional neural networks
Method: Adaptive Fastfood transform for reparameterizing matrix-vector multiplication in fully connected layers
Material: Fully connected layers in deep convolutional neural networks
Metric: Storage costs, computational costs
</ANSWER>"
27aa0f3ec934925265f93fac7ff1cd1d70ceb618,"Strong Baselines for Neural Semi - supervised Learning under Domain Shift section : Abstract Novel neural models have been proposed in recent years for learning under domain shift . Most models , however , only evaluate on a single task , on proprietary datasets , or compare to weak baselines , which makes comparison of models difficult . In this paper , we re - evaluate classic general - purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi - task tri - training method that reduces the time and space complexity of classic tri - training .",5,"['Task', 'Method', 'Material', 'Metric']","Strong Baselines for Neural Semi - supervised Learning under Domain Shift section : Abstract Novel neural models have been proposed in recent years for learning under domain shift . Most models , however , only evaluate on a single task , on proprietary datasets , or compare to weak baselines , which makes comparison of models difficult . In this paper , we re - evaluate classic general - purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi - task tri - training method that reduces the time and space complexity of classic tri - training .","{'Task': ['Neural Semi - supervised Learning', 'learning under domain shift', 'Domain Shift'], 'Method': ['tri - training', 'neural models', 'general - purpose bootstrapping approaches', 'neural networks', 'multi - task tri - training method', 'neural approaches'], 'Material': [], 'Metric': ['time and space complexity']}","<ANSWER>
{'Task': ['learning under domain shift'], 'Method': ['classic general-purpose bootstrapping approaches', 'tri-training', 'multi-task tri-training'], 'Material': [], 'Metric': ['time and space complexity']}
</ANSWER>","<ANSWER>
Tasks: [""learning under domain shift"", ""comparison of models""]
Methods: [""classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts"", ""recent neural approaches"", ""propose a novel multi-task tri-training method""]
Materials: []
Metrics: [""time complexity"", ""space complexity""]
</ANSWER>"
2a86bcdfb1d817ddb76ba202319f8267a36c0f62,"document : PCL : Proposal Cluster Learning for Weakly Supervised Object Detection Weakly Supervised Object Detection ( WSOD ) , using only image - level annotations to train object detectors , is of growing importance in object recognition . In this paper , we propose a novel deep network for WSOD . Unlike previous networks that transfer the object detection problem to an image classification problem using Multiple Instance Learning ( MIL ) , our strategy generates proposal clusters to learn refined instance classifiers by an iterative process . The proposals in the same cluster are spatially adjacent and associated with the same object .",5,"['Task', 'Method', 'Material', 'Metric']","document : PCL : Proposal Cluster Learning for Weakly Supervised Object Detection Weakly Supervised Object Detection ( WSOD ) , using only image - level annotations to train object detectors , is of growing importance in object recognition . In this paper , we propose a novel deep network for WSOD . Unlike previous networks that transfer the object detection problem to an image classification problem using Multiple Instance Learning ( MIL ) , our strategy generates proposal clusters to learn refined instance classifiers by an iterative process . The proposals in the same cluster are spatially adjacent and associated with the same object .","{'Task': ['WSOD )', 'image classification problem', 'Weakly Supervised Object Detection', 'object recognition', 'object detection', 'WSOD'], 'Method': ['Proposal Cluster Learning', 'PCL', 'MIL', 'object detectors', 'instance classifiers', 'proposal clusters', 'iterative process', 'deep network', 'Multiple Instance Learning'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['Weakly Supervised Object Detection'],
 'Method': ['Proposal Cluster Learning', 'Iterative process', 'Multiple Instance Learning (MIL)'],
 'Material': [],
 'Metric': []
}
</ANSWER>","<ANSWER>
Task: Weakly Supervised Object Detection
Method: Proposal Cluster Learning, Iterative process, Generating proposal clusters, Learning refined instance classifiers
Material: Image-level annotations
Metric: Not explicitly mentioned in the document
</ANSWER>"
2f04ba0f74df046b0080ca78e56898bd4847898b,"document : Aggregate Channel Features for Multi - view Face Detection Face detection has drawn much attention in recent decades since the seminal work by Viola and Jones . While many subsequences have improved the work with more powerful learning algorithms , the feature representation used for face detection still ca n’t meet the demand for effectively and efficiently handling faces with large appearance variance in the wild . To solve this bottleneck , we borrow the concept of channel features to the face detection domain , which extends the image channel to diverse types like gradient magnitude and oriented gradient histograms and therefore encodes rich information in a simple form . We adopt a novel variant called aggregate channel features , make a full exploration of feature design , and discover a multi - scale version of features with better performance .",5,"['Task', 'Method', 'Material', 'Metric']","document : Aggregate Channel Features for Multi - view Face Detection Face detection has drawn much attention in recent decades since the seminal work by Viola and Jones . While many subsequences have improved the work with more powerful learning algorithms , the feature representation used for face detection still ca n’t meet the demand for effectively and efficiently handling faces with large appearance variance in the wild . To solve this bottleneck , we borrow the concept of channel features to the face detection domain , which extends the image channel to diverse types like gradient magnitude and oriented gradient histograms and therefore encodes rich information in a simple form . We adopt a novel variant called aggregate channel features , make a full exploration of feature design , and discover a multi - scale version of features with better performance .","{'Task': ['Multi - view Face Detection', 'detection'], 'Method': ['learning algorithms', 'aggregate channel features', 'feature design', 'feature representation', 'multi - scale version of features', 'Aggregate Channel Features'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['face detection'], 'Method': ['aggregate channel features', 'borrowing the concept of channel features', 'making a full exploration of feature design'], 'Material': ['image channel', 'gradient magnitude', 'oriented gradient histograms'], 'Metric': ['performance']}
</ANSWER>","<ANSWER>
Task: Face detection
Method: Borrowing concept of channel features, Adopting a novel variant called aggregate channel features, Making a full exploration of feature design
Material: Image, Gradient magnitude, Oriented gradient histograms
Metric: Performance
</ANSWER>"
2f56b1ac5b9faac9527b6814778925e9242cf5fd,"document : Training Region - based Object Detectors with Online Hard Example Mining The field of object detection has made significant advances riding on the wave of region - based ConvNets , but their training procedure still includes many heuristics and hyperparameters that are costly to tune . We present a simple yet surprisingly effective online hard example mining ( OHEM ) algorithm for training region - based ConvNet detectors . Our motivation is the same as it has always been – detection datasets contain an overwhelming number of easy examples and a small number of hard examples .",5,"['Task', 'Method', 'Material', 'Metric']","document : Training Region - based Object Detectors with Online Hard Example Mining The field of object detection has made significant advances riding on the wave of region - based ConvNets , but their training procedure still includes many heuristics and hyperparameters that are costly to tune . We present a simple yet surprisingly effective online hard example mining ( OHEM ) algorithm for training region - based ConvNet detectors . Our motivation is the same as it has always been – detection datasets contain an overwhelming number of easy examples and a small number of hard examples .","{'Task': ['object detection'], 'Method': ['Online Hard Example Mining', 'region - based ConvNet detectors', 'online hard example mining', 'region - based ConvNets', 'Region - based Object Detectors', 'OHEM'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['object detection'], 'Method': ['online hard example mining (OHEM) algorithm', 'training procedure'], 'Material': ['detection datasets'], 'Metric': ['easy examples', 'hard examples']}
</ANSWER>","<ANSWER>
Task: Training region-based Object Detectors
Method: Online hard example mining (OHEM) algorithm
Material: Detection datasets
Metric: Number of easy examples and number of hard examples
</ANSWER>"
2f97ee95cad6a1f13596b108072b846c6f747d4e,"document : The Microsoft 2016 Conversational Speech Recognition System We describe Microsoft ’s conversational speech recognition system , in which we combine recent developments in neural - network - based acoustic and language modeling to advance the state of the art on the Switchboard recognition task . Inspired by machine learning ensemble techniques , the system uses a range of convolutional and recurrent neural networks . I - vector modeling and lattice - free MMI training provide significant gains for all acoustic model architectures . Language model rescoring with multiple forward and backward running RNNLMs , and word posterior - based system combination provide a 20 % boost .",5,"['Task', 'Method', 'Material', 'Metric']","document : The Microsoft 2016 Conversational Speech Recognition System We describe Microsoft ’s conversational speech recognition system , in which we combine recent developments in neural - network - based acoustic and language modeling to advance the state of the art on the Switchboard recognition task . Inspired by machine learning ensemble techniques , the system uses a range of convolutional and recurrent neural networks . I - vector modeling and lattice - free MMI training provide significant gains for all acoustic model architectures . Language model rescoring with multiple forward and backward running RNNLMs , and word posterior - based system combination provide a 20 % boost .","{'Task': [], 'Method': ['machine learning ensemble techniques', 'acoustic model architectures', 'Language model rescoring', 'lattice - free MMI training', 'neural - network - based acoustic and language modeling', 'Conversational Speech Recognition System', 'convolutional and recurrent neural networks', 'forward and backward running RNNLMs', 'I - vector modeling', 'Microsoft ’s conversational speech recognition system', 'word posterior - based system combination'], 'Material': ['Switchboard'], 'Metric': []}","<ANSWER>
{'Task': ['Switchboard recognition task'], 'Method': ['neural-network-based acoustic and language modeling', 'machine learning ensemble techniques', 'I-vector modeling', 'lattice-free MMI training', 'language model rescoring', 'multiple forward and backward running RNNLMs', 'word posterior-based system combination'], 'Material': [], 'Metric': ['gains', 'boost']}
</ANSWER>","<ANSWER>
Task:
- Switchboard recognition task

Method:
- Neural-network-based acoustic modeling
- Neural-network-based language modeling
- Machine learning ensemble techniques
- I-vector modeling
- Lattice-free MMI training
- Language model rescoring
- Multiple forward and backward running RNNLMs
- Word posterior-based system combination

Material:
- Neural networks (convolutional and recurrent)

Metric:
- 20% boost
</ANSWER>"
322a7dad274f440a92548faa8f2b2be666b2d01f,"document : Pyramid Scene Parsing Network Scene parsing is challenging for unrestricted open vocabulary and diverse scenes . In this paper , we exploit the capability of global context information by different - region - based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network ( PSPNet ) . Our global prior representation is effective to produce good quality results on the scene parsing task , while PSPNet provides a superior framework for pixel - level prediction . The proposed approach achieves state - of - the - art performance on various datasets .",5,"['Task', 'Method', 'Material', 'Metric']","document : Pyramid Scene Parsing Network Scene parsing is challenging for unrestricted open vocabulary and diverse scenes . In this paper , we exploit the capability of global context information by different - region - based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network ( PSPNet ) . Our global prior representation is effective to produce good quality results on the scene parsing task , while PSPNet provides a superior framework for pixel - level prediction . The proposed approach achieves state - of - the - art performance on various datasets .","{'Task': ['pixel - level prediction', 'Scene parsing', 'scene parsing task'], 'Method': ['Pyramid Scene Parsing Network', 'global prior representation', 'region - based context aggregation', 'pyramid scene parsing network', 'PSPNet', 'pyramid pooling module'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['scene parsing'], 'Method': ['pyramid pooling module', 'pyramid scene parsing network (PSPNet)'], 'Material': [], 'Metric': ['state-of-the-art performance']}
</ANSWER>","<ANSWER>
Task: [""scene parsing""]
Method: [""pyramid pooling module"", ""proposed pyramid scene parsing network (PSPNet)""]
Material: []
Metric: [""state-of-the-art performance""]
</ANSWER>"
325af39d281d5903a269c01fab8f53d7400a4c49,"document : ArtTrack : Articulated Multi - person Tracking in the Wild In this paper we propose an approach for articulated tracking of multiple people in unconstrained videos . Our starting point is a model that resembles existing architectures for single - frame pose estimation but is substantially faster . We achieve this in two ways : ( 1 ) by simplifying and sparsifying the body - part relationship graph and leveraging recent methods for faster inference , and ( 2 ) by offloading a substantial share of computation onto a feed - forward convolutional architecture that is able to detect and associate body joints of the same person even in clutter . We use this model to generate proposals for body joint locations and formulate articulated tracking as spatio - temporal grouping of such proposals .",5,"['Task', 'Method', 'Material', 'Metric']","document : ArtTrack : Articulated Multi - person Tracking in the Wild In this paper we propose an approach for articulated tracking of multiple people in unconstrained videos . Our starting point is a model that resembles existing architectures for single - frame pose estimation but is substantially faster . We achieve this in two ways : ( 1 ) by simplifying and sparsifying the body - part relationship graph and leveraging recent methods for faster inference , and ( 2 ) by offloading a substantial share of computation onto a feed - forward convolutional architecture that is able to detect and associate body joints of the same person even in clutter . We use this model to generate proposals for body joint locations and formulate articulated tracking as spatio - temporal grouping of such proposals .","{'Task': ['single - frame pose estimation', 'faster inference', 'articulated tracking', 'spatio - temporal grouping', 'articulated tracking of multiple people in unconstrained videos'], 'Method': ['body - part relationship graph', 'ArtTrack', 'feed - forward convolutional architecture', 'Articulated Multi - person Tracking'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['articulated tracking of multiple people', 'generating proposals for body joint locations', 'spatio-temporal grouping'],
 'Method': ['model for single-frame pose estimation', 'simplifying and sparsifying the body-part relationship graph', 'faster inference', 'offloading computation onto a feed-forward convolutional architecture', 'detecting and associating body joints'],
 'Material': [],
 'Metric': []
}
</ANSWER>","<ANSWER>
Task:
- Articulated tracking of multiple people
- Spatio-temporal grouping of body joint proposals

Method:
- Simplifying and sparsifying the body-part relationship graph
- Leveraging recent methods for faster inference
- Offloading computation onto a feed-forward convolutional architecture

Material:
- Existing architectures for single-frame pose estimation
- Body joint locations

Metric:
- Speed
</ANSWER>"
33261d252218007147a71e40f8367ed152fa2fe0,Question Answering with Subgraph Embeddings section : Abstract . This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few handcrafted features . Our model learns low - dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .,5,"['Task', 'Method', 'Material', 'Metric']",Question Answering with Subgraph Embeddings section : Abstract . This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few handcrafted features . Our model learns low - dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .,"{'Task': ['Question Answering'], 'Method': ['Subgraph Embeddings'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['Question Answering'], 'Method': ['Learning low-dimensional embeddings of words and knowledge base constituents', 'Scoring natural language questions against candidate answers'], 'Material': ['Knowledge base'], 'Metric': ['Ellipsis']}
</ANSWER>","<ANSWER>
Task: Question Answering
Method: Learning embeddings of words and knowledge base constituents
Material: Few handcrafted features, knowledge base
Metric: Scores of natural language questions against candidate answers
</ANSWER>"
3448e6a5039417dc1ae890efeca3bef5390ace7c,"document : [ Combinatorial features are essential for the success of many commercial models . Manually crafting these features usually comes with high cost due to the variety , volume and velocity of raw data in web - scale systems . Factorization based models , which measure interactions in terms of vector product , can learn patterns of combinatorial features automatically and generalize to unseen features as well . With the great success of deep neural networks ( DNNs ) in various fields , recently researchers have proposed several DNN - based factorization model to learn both low - and high - order feature interactions .",5,"['Task', 'Method', 'Material', 'Metric']","document : [ Combinatorial features are essential for the success of many commercial models . Manually crafting these features usually comes with high cost due to the variety , volume and velocity of raw data in web - scale systems . Factorization based models , which measure interactions in terms of vector product , can learn patterns of combinatorial features automatically and generalize to unseen features as well . With the great success of deep neural networks ( DNNs ) in various fields , recently researchers have proposed several DNN - based factorization model to learn both low - and high - order feature interactions .","{'Task': ['web - scale systems'], 'Method': ['deep neural networks', 'Factorization based models', 'DNNs', 'DNN'], 'Material': [], 'Metric': ['cost']}","<ANSWER>
{'Task': ['success of many commercial models'], 'Method': ['manually crafting features', 'factorization based models', 'learning both low- and high-order feature interactions', 'proposed several DNN-based factorization models'], 'Material': ['raw data in web-scale systems'], 'Metric': ['cost', 'patterns of combinatorial features', 'generalize to unseen features']}
</ANSWER>","<ANSWER>
Task:
- Success of commercial models
- Learning patterns of combinatorial features
- Generalizing to unseen features

Method:
- Manually crafting features
- Factorization based models
- Vector product measurement
- Deep neural networks (DNNs)
- DNN-based factorization models

Material:
- Combinatorial features
- Raw data
- Web-scale systems

Metric:
- Interactions
- Patterns
- Low-order feature interactions
- High-order feature interactions
</ANSWER>"
35734e8724559fb0d494e5cba6a28ad7a3d5dd4d,"document : Explaining and Harnessing Adversarial Examples Several machine learning models , including neural networks , consistently misclassify adversarial examples —inputs formed by applying small but intentionally worst - case perturbations to examples from the dataset , such that the perturbed input results in the model outputting an incorrect answer with high confidence . Early attempts at explaining this phenomenon focused on nonlinearity and overfitting . We argue instead that the primary cause of neural networks ’ vulnerability to adversarial perturbation is their linear nature . This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them : their generalization across architectures and training sets .",5,"['Task', 'Method', 'Material', 'Metric']","document : Explaining and Harnessing Adversarial Examples Several machine learning models , including neural networks , consistently misclassify adversarial examples —inputs formed by applying small but intentionally worst - case perturbations to examples from the dataset , such that the perturbed input results in the model outputting an incorrect answer with high confidence . Early attempts at explaining this phenomenon focused on nonlinearity and overfitting . We argue instead that the primary cause of neural networks ’ vulnerability to adversarial perturbation is their linear nature . This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them : their generalization across architectures and training sets .","{'Task': ['adversarial perturbation'], 'Method': ['neural networks', 'machine learning models', 'architectures'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['explaining', 'harnessing', 'neural networks's vulnerability to adversarial perturbation'],
 'Method': ['applying small but intentionally worst-case perturbations', 'focused on nonlinearity and overfitting', 'arguing that the primary cause is neural networks ’ linear nature'],
 'Material': ['machine learning models', 'neural networks', 'adversarial examples', 'inputs'],
 'Metric': ['misclassification', 'incorrect answer with high confidence', 'quantitative results']
}
</ANSWER>","<ANSWER>
Task: Explaining the cause of neural networks' vulnerability to adversarial perturbations
Method: Analyzing the primary cause of neural networks' vulnerability to adversarial perturbations
Material: Neural networks, machine learning models, adversarial examples
Metric: Misclassification rate, confidence level

Explanation: The task is to explain the cause of neural networks' vulnerability to adversarial perturbations. The method used to accomplish this task is analyzing the primary cause of neural networks' vulnerability to adversarial perturbations. The materials involved are neural networks, machine learning models, and adversarial examples. The metrics used to measure the effectiveness of the analysis are the misclassification rate and confidence level.
</ANSWER>"
364c1a3df58d87cb40ab33fdf3831cf2862f3570,"document : aNMM : Ranking Short Answer Texts with Attention - Based Neural Matching Model As an alternative to question answering methods based on feature engineering , deep learning approaches such as convolutional neural networks ( CNNs ) and Long Short - Term Memory Models ( LSTMs ) have recently been proposed for semantic matching of questions and answers . To achieve good results , however , these models have been combined with additional features such as word overlap or BM25 scores . Without this combination , these models perform significantly worse than methods based on linguistic feature engineering . In this paper , we propose an attention based neural matching model for ranking short answer text .",5,"['Task', 'Method', 'Material', 'Metric']","document : aNMM : Ranking Short Answer Texts with Attention - Based Neural Matching Model As an alternative to question answering methods based on feature engineering , deep learning approaches such as convolutional neural networks ( CNNs ) and Long Short - Term Memory Models ( LSTMs ) have recently been proposed for semantic matching of questions and answers . To achieve good results , however , these models have been combined with additional features such as word overlap or BM25 scores . Without this combination , these models perform significantly worse than methods based on linguistic feature engineering . In this paper , we propose an attention based neural matching model for ranking short answer text .","{'Task': ['semantic matching of questions and answers', 'Ranking Short Answer Texts', 'ranking short answer text', 'question answering'], 'Method': ['Attention - Based Neural Matching Model', 'attention based neural matching model', 'LSTMs', 'linguistic feature engineering', 'feature engineering', 'convolutional neural networks', 'Long Short - Term Memory Models', 'CNNs', 'aNMM', 'deep learning approaches'], 'Material': [], 'Metric': ['BM25 scores']}","<ANSWER>
{'Task': ['ranking short answer texts'],
 'Method': ['attention-based neural matching model'],
 'Material': ['deep learning approaches', 'convolutional neural networks (CNNs)', 'Long Short-Term Memory Models (LSTMs)', 'additional features', 'word overlap', 'BM25 scores'],
 'Metric': ['good results']}
</ANSWER>","<ANSWER>
Task: Ranking short answer texts
Method: Attention-based neural matching model
Material: Convolutional neural networks (CNNs)
Material: Long Short-Term Memory Models (LSTMs)
Metric: Word overlap
Metric: BM25 scores
</ANSWER>

The task is to rank short answer texts. The proposed method is an attention-based neural matching model. The materials used are Convolutional Neural Networks (CNNs) and Long Short-Term Memory Models (LSTMs). The metrics used for evaluation are word overlap and BM25 scores."
3842ee1e0fdfeff936b5c49973ff21adfaaf3929,"document : Adversarial Discriminative Domain Adaptation Adversarial learning methods are a promising approach to training robust deep networks , and can generate complex samples across diverse domains . They also can improve recognition despite the presence of domain shift or dataset bias : several adversarial approaches to unsupervised domain adaptation have recently been introduced , which reduce the difference between the training and test domain distributions and thus improve generalization performance . Prior generative approaches show compelling visualizations , but are not optimal on discriminative tasks and can be limited to smaller shifts . Prior discriminative approaches could handle larger domain shifts , but imposed tied weights on the model and did not exploit a GAN - based loss .",5,"['Task', 'Method', 'Material', 'Metric']","document : Adversarial Discriminative Domain Adaptation Adversarial learning methods are a promising approach to training robust deep networks , and can generate complex samples across diverse domains . They also can improve recognition despite the presence of domain shift or dataset bias : several adversarial approaches to unsupervised domain adaptation have recently been introduced , which reduce the difference between the training and test domain distributions and thus improve generalization performance . Prior generative approaches show compelling visualizations , but are not optimal on discriminative tasks and can be limited to smaller shifts . Prior discriminative approaches could handle larger domain shifts , but imposed tied weights on the model and did not exploit a GAN - based loss .","{'Task': ['robust deep networks', 'generalization', 'discriminative tasks', 'unsupervised domain adaptation', 'recognition'], 'Method': ['Adversarial learning methods', 'adversarial approaches', 'Adversarial Discriminative Domain Adaptation', 'GAN', 'generative approaches', 'discriminative approaches'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['training robust deep networks', 'improve recognition', 'unsupervised domain adaptation', 'reduce the difference between the training and test domain distributions', 'improve generalization performance'],
 'Method': ['adversarial learning methods', 'several adversarial approaches to unsupervised domain adaptation', 'prior generative approaches', 'prior discriminative approaches', 'handle larger domain shifts', 'impose tied weights', 'did not exploit a GAN-based loss'],
 'Material': [],
 'Metric': ['recognition performance', 'generalization performance']}
</ANSWER>","<ANSWER>
Task:
- unsupervised domain adaptation
- recognition
- improving generalization performance

Method:
- adversarial learning methods
- several adversarial approaches to unsupervised domain adaptation
- prior generative approaches
- prior discriminative approaches
- reducing the difference between the training and test domain distributions
- handling larger domain shifts
- imposing tied weights on the model
- not exploiting a GAN-based loss

Material:
- deep networks
- complex samples
- diverse domains
- domain shift
- dataset bias

Metric:
- generalization performance
</ANSWER>"
38cc89399dd6f5aaab1654f27ab3c9eeade12a36,"document : Exploiting temporal information for 3D human pose estimation In this work , we address the problem of 3D human pose estimation from a sequence of 2D human poses . Although the recent success of deep networks has led many state - of - the - art methods for 3D pose estimation to train deep networks end - to - end to predict from images directly , the top - performing approaches have shown the effectiveness of dividing the task of 3D pose estimation into two steps : using a state - of - the - art 2D pose estimator to estimate the 2D pose from images and then mapping them into 3D space . They also showed that a low - dimensional representation like 2D locations of a set of joints can be discriminative enough to estimate 3D pose with high accuracy . However , estimation of 3D pose for individual frames leads to temporally incoherent estimates due to independent error in each frame causing jitter .",5,"['Task', 'Method', 'Material', 'Metric']","document : Exploiting temporal information for 3D human pose estimation In this work , we address the problem of 3D human pose estimation from a sequence of 2D human poses . Although the recent success of deep networks has led many state - of - the - art methods for 3D pose estimation to train deep networks end - to - end to predict from images directly , the top - performing approaches have shown the effectiveness of dividing the task of 3D pose estimation into two steps : using a state - of - the - art 2D pose estimator to estimate the 2D pose from images and then mapping them into 3D space . They also showed that a low - dimensional representation like 2D locations of a set of joints can be discriminative enough to estimate 3D pose with high accuracy . However , estimation of 3D pose for individual frames leads to temporally incoherent estimates due to independent error in each frame causing jitter .","{'Task': ['3D human pose estimation', '3D pose'], 'Method': ['low - dimensional representation', 'deep networks', '2D pose estimator'], 'Material': [], 'Metric': ['accuracy', 'error']}","<ANSWER>
{'Task': ['3D human pose estimation', 'dividing the task of 3D pose estimation into two steps'],
 'Method': ['using a state-of-the-art 2D pose estimator', 'mapping 2D poses into 3D space'],
 'Material': [],
 'Metric': ['high accuracy']}
</ANSWER>","<ANSWER>
Task:
- 3D human pose estimation
- Estimation of 3D pose for individual frames

Method:
- Training deep networks end-to-end to predict 3D pose from images directly
- Dividing the task of 3D pose estimation into two steps: using a state-of-the-art 2D pose estimator to estimate 2D pose from images and then mapping them into 3D space

Material:
- Sequence of 2D human poses
- Images
- Low-dimensional representation (2D locations of a set of joints)

Metric:
- High accuracy
- Temporal coherence
</ANSWER>"
38e2f851b705faa0d0a698ed9885bd6834440073,"Probabilistic Model - Agnostic Meta - Learning section : Abstract Meta - learning for few - shot learning entails acquiring a prior over previous tasks and experiences , such that new tasks be learned from small amounts of data . However , a critical challenge in few - shot learning is task ambiguity : even when a powerful prior can be meta - learned from a large number of prior tasks , a small dataset for a new task can simply be too ambiguous to acquire a single model ( e.g. , a classifier ) for that task that is accurate . In this paper , we propose a probabilistic meta - learning algorithm that can sample models for a new task from a model distribution .",5,"['Task', 'Method', 'Material', 'Metric']","Probabilistic Model - Agnostic Meta - Learning section : Abstract Meta - learning for few - shot learning entails acquiring a prior over previous tasks and experiences , such that new tasks be learned from small amounts of data . However , a critical challenge in few - shot learning is task ambiguity : even when a powerful prior can be meta - learned from a large number of prior tasks , a small dataset for a new task can simply be too ambiguous to acquire a single model ( e.g. , a classifier ) for that task that is accurate . In this paper , we propose a probabilistic meta - learning algorithm that can sample models for a new task from a model distribution .","{'Task': ['task ambiguity', 'few - shot learning'], 'Method': ['classifier', 'probabilistic meta - learning algorithm', 'Meta - learning', 'Probabilistic Model - Agnostic Meta - Learning'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['few-shot learning', 'new task'], 'Method': ['probabilistic meta-learning', 'meta-learning', 'sampling models for a new task from a model distribution'], 'Material': [], 'Metric': ['accuracy']}
</ANSWER>","<ANSWER>
Task:
- Few-shot learning
- Acquiring a model for a new task

Method:
- Meta learning
- Probabilistic meta learning
- Sampling models for a new task from a model distribution

Material:
- Previous tasks and experiences
- Small datasets for new tasks
- Powerful prior

Metric:
- Accuracy
</ANSWER>"
3aa21de1a7c97e0458e10ed5730ce160bb436caa,"document : Pixel2Mesh : Generating 3D Mesh Models from Single RGB Images We propose an end - to - end deep learning architecture that produces a 3D shape in triangular mesh from a single color image . Limited by the nature of deep neural network , previous methods usually represent a 3D shape in volume or point cloud , and it is non - trivial to convert them to the more ready - to - use mesh model . Unlike the existing methods , our network represents 3D mesh in a graph - based convolutional neural network and produces correct geometry by progressively deforming an ellipsoid , leveraging perceptual features extracted from the input image . We adopt a coarse - to - fine strategy to make the whole deformation procedure stable , and define various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3D geometry .",5,"['Task', 'Method', 'Material', 'Metric']","document : Pixel2Mesh : Generating 3D Mesh Models from Single RGB Images We propose an end - to - end deep learning architecture that produces a 3D shape in triangular mesh from a single color image . Limited by the nature of deep neural network , previous methods usually represent a 3D shape in volume or point cloud , and it is non - trivial to convert them to the more ready - to - use mesh model . Unlike the existing methods , our network represents 3D mesh in a graph - based convolutional neural network and produces correct geometry by progressively deforming an ellipsoid , leveraging perceptual features extracted from the input image . We adopt a coarse - to - fine strategy to make the whole deformation procedure stable , and define various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3D geometry .","{'Task': [], 'Method': ['Pixel2Mesh', '3D Mesh Models', 'coarse - to - fine strategy', 'graph - based convolutional neural network', 'end deep learning architecture', '3D mesh', 'mesh model', 'deep neural network', 'deformation procedure'], 'Material': ['RGB Images'], 'Metric': []}","<ANSWER>
{'Task': ['Generating 3D shape in triangular mesh from a single color image'],
 'Method': ['end-to-end deep learning architecture', 'represents 3D mesh in a graph-based convolutional neural network', 'produces correct geometry by progressively deforming an ellipsoid', 'adopts a coarse-to-fine strategy', 'defines various mesh related losses'],
 'Material': [Ellipsis],
 'Metric': ['visually appealing and physically accurate 3D geometry']
}
</ANSWER>","<ANSWER>
Task: Generating 3D mesh models from single RGB images
Method:
- Proposing an end-to-end deep learning architecture
- Representing 3D mesh in a graph-based convolutional neural network
- Progressively deforming an ellipsoid
- Adopting a coarse-to-fine strategy
Material: Single color image
Metric:
- Visually appealing 3D geometry
- Physically accurate 3D geometry
</ANSWER>"
3acc07f7f8951617276cf99483ed02aeb0a6eeac,"document : Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes During the last half decade , convolutional neural networks ( CNNs ) have triumphed over semantic segmentation , which is a core task of various emerging industrial applications such as autonomous driving and medical imaging . However , to train CNNs requires a huge amount of data , which is difficult to collect and laborious to annotate . Recent advances in computer graphics make it possible to train CNN models on photo - realistic synthetic data with computer - generated annotations . Despite this , the domain mismatch between the real images and the synthetic data significantly decreases the models ’ performance .",5,"['Task', 'Method', 'Material', 'Metric']","document : Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes During the last half decade , convolutional neural networks ( CNNs ) have triumphed over semantic segmentation , which is a core task of various emerging industrial applications such as autonomous driving and medical imaging . However , to train CNNs requires a huge amount of data , which is difficult to collect and laborious to annotate . Recent advances in computer graphics make it possible to train CNN models on photo - realistic synthetic data with computer - generated annotations . Despite this , the domain mismatch between the real images and the synthetic data significantly decreases the models ’ performance .","{'Task': ['semantic segmentation', 'autonomous driving', 'medical imaging', 'computer graphics', 'industrial applications', 'Semantic Segmentation of Urban Scenes'], 'Method': ['CNN', 'convolutional neural networks', 'CNNs', 'Curriculum Domain Adaptation'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['semantic segmentation', 'core task of various emerging industrial applications such as autonomous driving and medical imaging'],
 'Method': ['convolutional neural networks ( CNNs )', 'training CNN models', 'making it possible to train CNN models on photo-realistic synthetic data with computer-generated annotations'],
 'Material': ['huge amount of data', 'real images', 'synthetic data'],
 'Metric': ['models ’ performance']}
</ANSWER>","<ANSWER>
Task: [""semantic segmentation"", ""autonomous driving"", ""medical imaging""]
Method: [""convolutional neural networks (CNNs)"", ""training CNN models on photo-realistic synthetic data"", ""computer-generated annotations""]
Material: [""huge amount of data"", ""real images"", ""synthetic data""]
Metric: [""models’ performance"", ""domain mismatch""]
</ANSWER>"
3ca3993b1f3536b15112f759067f62e999c5d38f,"BB8 : A Scalable , Accurate , Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth section : Abstract We introduce a novel method for 3D object detection and pose estimation from color images only . We first use segmentation to detect the objects of interest in 2D even in presence of partial occlusions and cluttered background .",5,"['Task', 'Method', 'Material', 'Metric']","BB8 : A Scalable , Accurate , Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth section : Abstract We introduce a novel method for 3D object detection and pose estimation from color images only . We first use segmentation to detect the objects of interest in 2D even in presence of partial occlusions and cluttered background .","{'Task': ['3D object detection', 'objects of interest in 2D even', 'Predicting the 3D Poses of Challenging Objects', 'pose estimation'], 'Method': ['segmentation', 'Partial Occlusion Method', 'BB8'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['3D object detection', 'pose estimation'], 'Method': ['BB8', 'segmentation'], 'Material': ['color images'], 'Metric': ['scalable', 'accurate', 'robust to partial occlusion']}
</ANSWER>","<ANSWER>
Task: 3D object detection and pose estimation
Method: A novel method using segmentation for 2D object detection in presence of partial occlusions and cluttered background, and estimation of 3D poses from color images only
Material: Color images
Metric: Scalability, Accuracy, Robustness to Partial Occlusion
</ANSWER>"
3cf31ecb2724b5088783d7c96a5fc0d5604cbf41,"document : Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations We present a simple and effective scheme for dependency parsing which is based on bidirectional - LSTMs ( BiLSTMs ) . Each sentence token is associated with a BiLSTM vector representing the token in its sentential context , and feature vectors are constructed by concatenating a few BiLSTM vectors . The BiLSTM is trained jointly with the parser objective , resulting in very effective feature extractors for parsing . We demonstrate the effectiveness of the approach by applying it to a greedy transition - based parser as well as to a globally optimized graph - based parser .",5,"['Task', 'Method', 'Material', 'Metric']","document : Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations We present a simple and effective scheme for dependency parsing which is based on bidirectional - LSTMs ( BiLSTMs ) . Each sentence token is associated with a BiLSTM vector representing the token in its sentential context , and feature vectors are constructed by concatenating a few BiLSTM vectors . The BiLSTM is trained jointly with the parser objective , resulting in very effective feature extractors for parsing . We demonstrate the effectiveness of the approach by applying it to a greedy transition - based parser as well as to a globally optimized graph - based parser .","{'Task': ['Dependency Parsing', 'dependency parsing', 'parsing'], 'Method': ['BiLSTMs', 'greedy transition - based parser', 'feature extractors', 'Bidirectional LSTM Feature Representations', 'BiLSTM', 'bidirectional - LSTMs', 'globally optimized graph - based parser'], 'Material': [], 'Metric': ['parser objective']}","<ANSWER>
{'Task': ['dependency parsing'], 'Method': ['bidirectional LSTMs (BiLSTMs)', 'greedy transition-based parser', 'globally optimized graph-based parser'], 'Material': ['sentence tokens', 'BiLSTM vectors', 'parser objective'], 'Metric': ['effectiveness']}
</ANSWER>","<ANSWER>
Task: Dependency Parsing
Method: Bidirectional Long Short-Term Memory (BiLSTM) model
Material: Sentence tokens, BiLSTM vectors, feature vectors, parser objective
Metric: Effectiveness of the approach (demonstrated through application to a greedy transition-based parser and a globally optimized graph-based parser)
</ANSWER>"
3daa086acd367dc971a2dc1382caba2031294233,"Holistic , Instance - level Human Parsing section : Abstract Object parsing - the task of decomposing an object into its semantic parts - has traditionally been formulated as a category - level segmentation problem . Consequently , when there are multiple objects in an image , current methods can not count the number of objects in the scene , nor can they determine which part belongs to which object . We address this problem by segmenting the parts of objects at an instance - level , such that each pixel in the image is assigned a part label , as well as the identity of the object it belongs to .",5,"['Task', 'Method', 'Material', 'Metric']","Holistic , Instance - level Human Parsing section : Abstract Object parsing - the task of decomposing an object into its semantic parts - has traditionally been formulated as a category - level segmentation problem . Consequently , when there are multiple objects in an image , current methods can not count the number of objects in the scene , nor can they determine which part belongs to which object . We address this problem by segmenting the parts of objects at an instance - level , such that each pixel in the image is assigned a part label , as well as the identity of the object it belongs to .","{'Task': ['category - level segmentation problem', 'Holistic , Instance - level Human Parsing', 'decomposing an object', 'Object parsing'], 'Method': [], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['Abstract Object parsing', 'decomposing an object into its semantic parts', 'segmentation problem', 'counting the number of objects in the scene', 'determining which part belongs to which object'],
 'Method': ['Instance-level segmenting', 'segmenting the parts of objects'],
 'Material': [],
 'Metric': ['pixel in the image', 'part label', 'identity of the object']
}
</ANSWER>","<ANSWER>
Task:
- Object parsing
- Instance-level segmentation

Method:
- Holistic approach
- Abstract Object parsing
- Instance-level part segmentation

Material:
- Pixels in the image

Metric:
- Part label
- Object identity
</ANSWER>"
408e8eecc14c5cc60bbdfc486ba7a7fc97031788,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training . In this paper we present an approach for training a convolutional neural network using only unlabeled data . We train the network to discriminate between a set of surrogate classes . Each surrogate class is formed by applying a variety of transformations to a randomly sampled ’ seed ’ image patch . We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition .,5,"['Task', 'Method', 'Material', 'Metric']",Discriminative Unsupervised Feature Learning with Convolutional Neural Networks Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training . In this paper we present an approach for training a convolutional neural network using only unlabeled data . We train the network to discriminate between a set of surrogate classes . Each surrogate class is formed by applying a variety of transformations to a randomly sampled ’ seed ’ image patch . We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition .,"{'Task': ['visual object recognition', 'supervised training'], 'Method': ['network', 'convolutional neural networks', 'feature learning algorithm', 'Discriminative Unsupervised Feature Learning with Convolutional Neural Networks'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['visual object recognition'], 'Method': ['discriminative unsupervised feature learning', 'training a convolutional neural network using only unlabeled data', 'forming surrogate classes by applying a variety of transformations to a randomly sampled seed image patch'], 'Material': ['convolutional neural network', 'surrogate classes', 'seed image patch'], 'Metric': ['successfully']}
</ANSWER>","<ANSWER>
Task: Visual object recognition
Method: Discriminative unsupervised feature learning with Convolutional Neural Networks (CNNs)
Material: A set of surrogate classes, each formed by applying a variety of transformations to a randomly sampled 'seed' image patch
Metric: Not explicitly mentioned in the document
</ANSWER>"
4365eb43a635bc6431dfaf3af1f7bf7bf55522cc,"document : CoupleNet : Coupling Global Structure with Local Parts for Object Detection The region - based Convolutional Neural Network ( CNN ) detectors such as Faster R - CNN or R - FCN have already shown promising results for object detection by combining the region proposal subnetwork and the classification subnetwork together . Although R - FCN has achieved higher detection speed while keeping the detection performance , the global structure information is ignored by the position - sensitive score maps . To fully explore the local and global properties , in this paper , we propose a novel fully convolutional network , named as CoupleNet , to couple the global structure with local parts for object detection . Specifically , the object proposals obtained by the Region Proposal Network ( RPN ) are fed into the the coupling module which consists of two branches .",5,"['Task', 'Method', 'Material', 'Metric']","document : CoupleNet : Coupling Global Structure with Local Parts for Object Detection The region - based Convolutional Neural Network ( CNN ) detectors such as Faster R - CNN or R - FCN have already shown promising results for object detection by combining the region proposal subnetwork and the classification subnetwork together . Although R - FCN has achieved higher detection speed while keeping the detection performance , the global structure information is ignored by the position - sensitive score maps . To fully explore the local and global properties , in this paper , we propose a novel fully convolutional network , named as CoupleNet , to couple the global structure with local parts for object detection . Specifically , the object proposals obtained by the Region Proposal Network ( RPN ) are fed into the the coupling module which consists of two branches .","{'Task': ['Object Detection', 'detection', 'object detection'], 'Method': ['region proposal subnetwork', 'fully convolutional network', 'RPN', 'CNN', 'CoupleNet', 'coupling module', 'Region Proposal Network', 'classification subnetwork', 'R - FCN', 'Convolutional Neural Network'], 'Material': [], 'Metric': ['detection speed']}","<ANSWER>
{'Task': ['object detection'], 'Method': ['region-based Convolutional Neural Network (CNN)', 'Faster R-CNN', 'R-FCN', 'CoupleNet', 'Region Proposal Network (RPN)', 'coupling module'], 'Material': [], 'Metric': ['detection speed', 'detection performance']}
</ANSWER>","<ANSWER>
Task: Object Detection
Method:
- Region-based Convolutional Neural Network (R-CNN)
- Faster R-CNN
- R-FCN
- Proposing a novel fully convolutional network named CoupleNet
Material:
- Region proposal subnetwork
- Classification subnetwork
- Position-sensitive score maps
- Object proposals obtained by the Region Proposal Network (RPN)
Metric:
- Detection speed
- Detection performance
</ANSWER>"
436b07bebaa1d1f05ef85415e10374048d25334d,"OUTRAGEOUSLY LARGE NEURAL NETWORKS : THE SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER section : ABSTRACT The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per - example basis , has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation .",5,"['Task', 'Method', 'Material', 'Metric']","OUTRAGEOUSLY LARGE NEURAL NETWORKS : THE SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER section : ABSTRACT The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per - example basis , has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation .","{'Task': ['Conditional computation', 'computation', 'model capacity'], 'Method': ['OUTRAGEOUSLY LARGE NEURAL NETWORKS', 'SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER', 'neural network'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['absorbing information', 'dramatically increasing model capacity'],
 'Method': ['conditional computation', 'parts of the network are active on a per - example basis'],
 'Material': [],
 'Metric': ['number of parameters']}
</ANSWER>","<ANSWER>
Task: Increasing model capacity in neural networks
Method:
- Conditional computation
- Parts of the network are active on a per-example basis
Material: Neural networks
Metric: Model capacity
</ANSWER>"
44078d0daed8b13114cffb15b368acc467f96351,"document : Triplet Probabilistic Embedding for Face Verification and Clustering Despite significant progress made over the past twenty five years , unconstrained face verification remains a challenging problem . This paper proposes an approach that couples a deep CNN - based approach with a low - dimensional discriminative embedding step , learned using triplet probability constraints to address the unconstrained face verification problem . Aside from yielding performance improvements , this embedding provides significant advantages in terms of memory and for post - processing operations like subject specific clustering . Experiments on the challenging IJB - A dataset show that the proposed algorithm performs close to the state of the art methods in verification and identification metrics , while requiring much less training data and training / test time .",5,"['Task', 'Method', 'Material', 'Metric']","document : Triplet Probabilistic Embedding for Face Verification and Clustering Despite significant progress made over the past twenty five years , unconstrained face verification remains a challenging problem . This paper proposes an approach that couples a deep CNN - based approach with a low - dimensional discriminative embedding step , learned using triplet probability constraints to address the unconstrained face verification problem . Aside from yielding performance improvements , this embedding provides significant advantages in terms of memory and for post - processing operations like subject specific clustering . Experiments on the challenging IJB - A dataset show that the proposed algorithm performs close to the state of the art methods in verification and identification metrics , while requiring much less training data and training / test time .","{'Task': ['post - processing operations', 'unconstrained face verification problem', 'Clustering', 'Face Verification', 'unconstrained face verification'], 'Method': ['Triplet Probabilistic Embedding', 'CNN', 'subject specific clustering', 'low - dimensional discriminative embedding step'], 'Material': ['IJB - A dataset'], 'Metric': ['verification and identification metrics', 'training / test time', 'memory']}","<ANSWER>
{'Task': ['unconstrained face verification', 'subject specific clustering'],
 'Method': ['deep CNN - based approach', 'low-dimensional discriminative embedding step', 'triplet probability constraints'],
 'Material': [],
 'Metric': ['performance improvements', 'memory', 'verification and identification metrics']
}
</ANSWER>","<ANSWER>
Task:
- unconstrained face verification
- subject specific clustering

Method:
- deep CNN-based approach
- low-dimensional discriminative embedding
- triplet probability constraints

Material:
- IJB-A dataset

Metric:
- verification metrics
- identification metrics
- training data
- training / test time
</ANSWER>"
45429c281e30f9e87ebcd1ae42e0656d2ead24d1,"document : High - Resolution Image Synthesis and Semantic Manipulation with Conditional GANs We present a new method for synthesizing high - resolution photo - realistic images from semantic label maps using conditional generative adversarial networks ( conditional GANs ) . Conditional GANs have enabled a variety of applications , but the results are often limited to low - resolution and still far from realistic . In this work , we generate visually appealing results with a novel adversarial loss , as well as new multi - scale generator and discriminator architectures . Furthermore , we extend our framework to interactive visual manipulation with two additional features .",5,"['Task', 'Method', 'Material', 'Metric']","document : High - Resolution Image Synthesis and Semantic Manipulation with Conditional GANs We present a new method for synthesizing high - resolution photo - realistic images from semantic label maps using conditional generative adversarial networks ( conditional GANs ) . Conditional GANs have enabled a variety of applications , but the results are often limited to low - resolution and still far from realistic . In this work , we generate visually appealing results with a novel adversarial loss , as well as new multi - scale generator and discriminator architectures . Furthermore , we extend our framework to interactive visual manipulation with two additional features .","{'Task': ['High - Resolution Image Synthesis', 'semantic label maps', 'Semantic Manipulation', 'synthesizing high - resolution photo - realistic images', 'interactive visual manipulation'], 'Method': ['multi - scale generator and discriminator architectures', 'conditional GANs', 'adversarial loss', 'Conditional GANs', 'conditional generative adversarial networks'], 'Material': [], 'Metric': []}","<ANSWER>
{'Task': ['High-Resolution Image Synthesis', 'Interactive Visual Manipulation'],
 'Method': ['Conditional Generative Adversarial Networks (conditional GANs)', 'Novel adversarial loss', 'New multi-scale generator and discriminator architectures'],
 'Material': [],
 'Metric': ['visually appealing results']}
</ANSWER>","<ANSWER>
Task:
- High-Resolution Image Synthesis
- Semantic Manipulation

Method:
- Conditional Generative Adversarial Networks (conditional GANs)
- Novel adversarial loss
- New multi-scale generator architectures
- New discriminator architectures

Material:
- Semantic label maps

Metric:
- Visual appeal
</ANSWER>"
455da02e5048dffb51fb6ab5eb8aeca5926c9d9a,"document : Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition Existing deep convolutional neural networks ( CNNs ) require a fixed - size ( , 224 224 ) input image . This requirement is “ artificial ” and may reduce the recognition accuracy for the images or sub - images of an arbitrary size / scale . In this work , we equip the networks with another pooling strategy , “ spatial pyramid pooling ” , to eliminate the above requirement .",5,"['Task', 'Method', 'Material', 'Metric']","document : Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition Existing deep convolutional neural networks ( CNNs ) require a fixed - size ( , 224 224 ) input image . This requirement is “ artificial ” and may reduce the recognition accuracy for the images or sub - images of an arbitrary size / scale . In this work , we equip the networks with another pooling strategy , “ spatial pyramid pooling ” , to eliminate the above requirement .","{'Task': ['Visual Recognition'], 'Method': ['Spatial Pyramid Pooling', 'Deep Convolutional Networks', 'pooling strategy', 'CNNs', 'deep convolutional neural networks', 'spatial pyramid pooling'], 'Material': [], 'Metric': ['recognition accuracy']}","<ANSWER>
{'Task': ['visual recognition'], 'Method': ['spatial pyramid pooling'], 'Material': ['deep convolutional neural networks (CNNs)'], 'Metric': ['recognition accuracy']}
</ANSWER>","<ANSWER>
Task: Visual Recognition
Method:
- Deep Convolutional Neural Networks (CNNs)
- Spatial Pyramid Pooling
Material: Images
Metric: Recognition accuracy
</ANSWER>"

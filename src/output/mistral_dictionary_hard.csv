doc_id,document,top_k,category,input_data,answer,python_output,yaml_output
02567fd428a675ca91a0c6786f47f3e35881bcbd,"document : Deep Label Distribution Learning With Label Ambiguity Convolutional Neural Networks ( ConvNets ) have achieved excellent recognition performance in various visual recognition tasks . A large labeled training set is one of the most important factors for its success . However , it is difficult to collect sufficient training images with precise labels in some domains such as apparent age estimation , head pose estimation , multi - label classification and semantic segmentation . Fortunately , there is ambiguous information among labels , which makes these tasks different from traditional classification .",5,"['Task', 'Method', 'Material', 'Metric']","document : Deep Label Distribution Learning With Label Ambiguity Convolutional Neural Networks ( ConvNets ) have achieved excellent recognition performance in various visual recognition tasks . A large labeled training set is one of the most important factors for its success . However , it is difficult to collect sufficient training images with precise labels in some domains such as apparent age estimation , head pose estimation , multi - label classification and semantic segmentation . Fortunately , there is ambiguous information among labels , which makes these tasks different from traditional classification .","{'Task': ['semantic segmentation', 'age estimation', 'head pose estimation', 'Label Ambiguity', 'classification', 'multi - label classification', 'recognition', 'visual recognition tasks'], 'Method': ['Deep Label Distribution Learning', 'Convolutional Neural Networks', 'ConvNets'], 'Material': [], 'Metric': []}","{'Task': ['apparent age estimation', 'head pose estimation', 'multi-label classification', 'semantic segmentation', 'visual recognition tasks'],
 'Method': ['Deep Label Distribution Learning', 'Convolutional Neural Networks (ConvNets)'],
 'Material': ['sufficient training images', 'precise labels'],
 'Metric': ['recognition performance']}","tasks:
- name: Apparent age estimation
- name: Head pose estimation
- name: Multi-label classification
- name: Semantic segmentation

methods:
- name: Deep Label Distribution Learning
- name: Convolutional Neural Networks (ConvNets)

materials:

metrics:
- name: Recognition performance
- name: Precise labels
- name: Sufficient training images
- name: Ambiguous information among labels"
02b3d1d162080d9aefd3fc30a0bcc9a843073b5d,"In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling , a task central to language understanding . We extend current models to deal with two key challenges present in this task : corpora and vocabulary sizes , and complex , long term structure of language . We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long - Short Term Memory , on the One Billion Word Benchmark . Our best single model significantly improves state - of - the - art perplexity from 51.3 down to 30.0 ( whilst reducing the number of parameters by a factor of 20 ) , while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7 . We also release these models for the NLP and ML community to study and improve upon .",5,"['Task', 'Method', 'Material', 'Metric']","In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling , a task central to language understanding . We extend current models to deal with two key challenges present in this task : corpora and vocabulary sizes , and complex , long term structure of language . We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long - Short Term Memory , on the One Billion Word Benchmark . Our best single model significantly improves state - of - the - art perplexity from 51.3 down to 30.0 ( whilst reducing the number of parameters by a factor of 20 ) , while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7 . We also release these models for the NLP and ML community to study and improve upon .","{'Task': ['language understanding', 'Language Modeling', 'NLP and ML community'], 'Method': ['character Convolutional Neural Networks', 'Recurrent Neural Networks', 'ensemble of models', 'Long - Short Term Memory'], 'Material': ['One Billion Word Benchmark'], 'Metric': ['perplexity']}","{'Task': ['large scale Language Modeling', 'dealing with corpora and vocabulary sizes, and complex, long term structure of language'],
 'Method': ['Recurrent Neural Networks', 'character Convolutional Neural Networks', 'Long - Short Term Memory'],
 'Material': ['One Billion Word Benchmark'],
 'Metric': ['perplexity']}","```yaml
Task: [ ""large scale Language Modeling"", ""exploring recent advances in Recurrent Neural Networks"" ]
Method: [ ""character Convolutional Neural Networks"", ""Long - Short Term Memory"" ]
Material: [ ""One Billion Word Benchmark"" ]
Metric: [ ""perplexity"" ]
```"
0398552184f80db111e9c28bf533b395f233ac00,"Bridging Saliency Detection to Weakly Supervised Object Detection Based on Self - paced Curriculum Learning section : Abstract Weakly - supervised object detection ( WOD ) is a challenging problems in computer vision . The key problem is to simultaneously infer the exact object locations in the training images and train the object detectors , given only the training images with weak image - level labels . Intuitively , by simulating the selective attention mechanism of human visual system , saliency detection technique can select attractive objects in scenes and thus is a potential way to provide useful priors for WOD .",5,"['Task', 'Method', 'Material', 'Metric']","Bridging Saliency Detection to Weakly Supervised Object Detection Based on Self - paced Curriculum Learning section : Abstract Weakly - supervised object detection ( WOD ) is a challenging problems in computer vision . The key problem is to simultaneously infer the exact object locations in the training images and train the object detectors , given only the training images with weak image - level labels . Intuitively , by simulating the selective attention mechanism of human visual system , saliency detection technique can select attractive objects in scenes and thus is a potential way to provide useful priors for WOD .","{'Task': ['WOD', 'saliency detection', 'Weakly Supervised Object Detection', 'Bridging Saliency Detection', 'Weakly - supervised object detection', 'computer vision'], 'Method': ['object detectors', 'Self - paced Curriculum Learning', 'human visual system', 'selective attention mechanism'], 'Material': [], 'Metric': []}","{'Task': ['weakly-supervised object detection (WOD)'], 'Method': ['saliency detection'], 'Material': ['training images', 'weak image-level labels'], 'Metric': ['exact object locations']}","task:
- name: Weakly-supervised object detection (WOD)

method:
- name: Self-paced Curriculum Learning
- name: Simulating the selective attention mechanism of human visual system
- name: Saliency detection technique

material:
- name: Training images
- name: Weak image-level labels

metric:
- name: Exact object locations
- name: Useful priors for WOD
- name: Attractive objects in scenes"
05d2700846c0323f79c1344aca5333994c7c03a5,"document : The IBM 2016 English Conversational Telephone Speech Recognition System We describe a collection of acoustic and language modeling techniques that lowered the word error rate of our English conversational telephone LVCSR system to a record 6.6 % on the Switchboard subset of the Hub5 2000 evaluation testset . On the acoustic side , we use a score fusion of three strong models : recurrent nets with maxout activations , very deep convolutional nets with 3x3 kernels , and bidirectional long short - term memory nets which operate on FMLLR and i - vector features . On the language modeling side , we use an updated model “ M ” and hierarchical neural network LMs . GeorgeSaon , TomSercu , StevenRennieandHong - KwangJ.Kuo IBMT.J.WatsonResearchCenter , YorktownHeights , NY , 10598 gsaon@us.ibm.com",5,"['Task', 'Method', 'Material', 'Metric']","document : The IBM 2016 English Conversational Telephone Speech Recognition System We describe a collection of acoustic and language modeling techniques that lowered the word error rate of our English conversational telephone LVCSR system to a record 6.6 % on the Switchboard subset of the Hub5 2000 evaluation testset . On the acoustic side , we use a score fusion of three strong models : recurrent nets with maxout activations , very deep convolutional nets with 3x3 kernels , and bidirectional long short - term memory nets which operate on FMLLR and i - vector features . On the language modeling side , we use an updated model “ M ” and hierarchical neural network LMs . GeorgeSaon , TomSercu , StevenRennieandHong - KwangJ.Kuo IBMT.J.WatsonResearchCenter , YorktownHeights , NY , 10598 gsaon@us.ibm.com","{'Task': ['language modeling side', 'IBM 2016 English Conversational Telephone Speech Recognition System'], 'Method': ['3x3 kernels', 'maxout activations', 'strong models', 'recurrent nets', 'bidirectional long short - term memory nets', 'deep convolutional nets', 'updated model', 'acoustic and language modeling techniques', 'English conversational telephone LVCSR system', 'M ”', 'hierarchical neural network LMs'], 'Material': ['Switchboard subset'], 'Metric': ['word error rate']}","{'Task': ['English conversational telephone LVCSR system evaluation'],
 'Method': ['score fusion', 'recurrent nets with maxout activations', 'very deep convolutional nets with 3x3 kernels', 'bidirectional long short-term memory nets', 'updated model “ M ”', 'hierarchical neural network LMs'],
 'Material': ['IBM 2016 English Conversational Telephone Speech Recognition System', 'Switchboard subset of the Hub5 2000 evaluation testset', 'FMLLR and i-vector features'],
 'Metric': ['word error rate']}","task: Speech Recognition
method:
- Recurrent Neural Networks with maxout activations
- Very Deep Convolutional Nets with 3x3 kernels
- Bidirectional Long Short-Term Memory Nets
- Score fusion of three strong models
- Updated model ""M""
- Hierarchical neural network LMs
material: FMLLR features, i-vector features
metric: Word Error Rate
value: 6.6 %

entities:
- task: Speech Recognition
- method:
  - Recurrent Neural Networks with maxout activations
  - Very Deep Convolutional Nets with 3x3 kernels
  - Bidirectional Long Short-Term Memory Nets
  - Score fusion of three strong models
  - Updated model ""M""
  - Hierarchical neural network LMs
- material: FMLLR features, i-vector features
- metric: Word Error Rate
- value: 6.6 %
- author:
  - name: GeorgeSaon
  - email: gsaon@us.ibm.com
  - affiliation: IBMT.J.WatsonResearchCenter
  - location: YorktownHeights, NY, 10598
- co_author:
  - name: TomSercu
  - affiliation: IBMT.J.WatsonResearchCenter
  - location: YorktownHeights, NY, 10598
  - email: N/A
- co_author:
  - name: StevenRennie
  - affiliation: IBMT.J.WatsonResearchCenter
  - location: YorktownHeights, NY, 10598
  - email: N/A
- co_author:
  - name: Hong - KwangJ.Kuo
  - affiliation: IBMT.J.WatsonResearchCenter
  - location: YorktownHeights, NY, 10598
  - email: N/A"
0626908dd710b91aece1a81f4ca0635f23fc47f3,"document : Rethinking the Inception Architecture for Computer Vision Convolutional networks are at the core of most state - of - the - art computer vision solutions for a wide variety of tasks . Since 2014 very deep convolutional networks started to become mainstream , yielding substantial gains in various benchmarks . Although increased model size and computational cost tend to translate to immediate quality gains for most tasks ( as long as enough labeled data is provided for training ) , computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big - data scenarios . Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization .",5,"['Task', 'Method', 'Material', 'Metric']","document : Rethinking the Inception Architecture for Computer Vision Convolutional networks are at the core of most state - of - the - art computer vision solutions for a wide variety of tasks . Since 2014 very deep convolutional networks started to become mainstream , yielding substantial gains in various benchmarks . Although increased model size and computational cost tend to translate to immediate quality gains for most tasks ( as long as enough labeled data is provided for training ) , computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big - data scenarios . Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization .","{'Task': ['big - data scenarios', 'Computer Vision', 'mobile vision'], 'Method': ['deep convolutional networks', 'factorized convolutions', 'aggressive regularization', 'computer vision solutions', 'Inception Architecture', 'Convolutional networks'], 'Material': [], 'Metric': ['computational cost', 'computational efficiency', 'model size', 'low parameter count']}","{'Task': ['state-of-the-art computer vision solutions', 'various tasks', 'use cases such as mobile vision and big-data scenarios'],
 'Method': ['very deep convolutional networks', 'increased model size and computational cost', 'suitably factorized convolutions', 'aggressive regularization'],
 'Material': ['convolutional networks', 'enough labeled data'],
 'Metric': ['immediate quality gains', 'computational efficiency', 'low parameter count']}","task: computer vision solutions, various tasks (e.g., mobile vision, big data scenarios)
method: very deep convolutional networks, increased model size, computational cost, suitably factorized convolutions, aggressive regularization
material: convolutional networks, labeled data
metric: quality gains, computational efficiency, low parameter count, benchmarks"
0678a8abea82793993cd89383319da75f6dc4be3,"document : ProNet : Learning to Propose Object - specific Boxes for Cascaded Neural Networks This paper aims to classify and locate objects accurately and efficiently , without using bounding box annotations . It is challenging as objects in the wild could appear at arbitrary locations and in different scales . In this paper , we propose a novel classification architecture ProNet based on convolutional neural networks . It uses computationally efficient neural networks to propose image regions that are likely to contain objects , and applies more powerful but slower networks on the proposed regions .",5,"['Task', 'Method', 'Material', 'Metric']","document : ProNet : Learning to Propose Object - specific Boxes for Cascaded Neural Networks This paper aims to classify and locate objects accurately and efficiently , without using bounding box annotations . It is challenging as objects in the wild could appear at arbitrary locations and in different scales . In this paper , we propose a novel classification architecture ProNet based on convolutional neural networks . It uses computationally efficient neural networks to propose image regions that are likely to contain objects , and applies more powerful but slower networks on the proposed regions .","{'Task': [], 'Method': ['convolutional neural networks', 'classification architecture ProNet', 'neural networks', 'ProNet', 'Learning to Propose Object - specific Boxes for Cascaded Neural Networks'], 'Material': [], 'Metric': []}","{'Task': ['classify', 'locate', 'objects', 'accurately', 'efficiently', 'without using bounding box annotations'],
 'Method': ['ProNet', 'convolutional neural networks', 'computationally efficient neural networks', 'applies more powerful but slower networks', 'on the proposed regions'],
 'Material': [],
 'Metric': []}","task: Object classification and localization
method:
- Proposing image regions using computationally efficient neural networks (ProNet)
- Applying more powerful but slower networks on the proposed regions
material: Convolutional neural networks
metric: Accuracy and efficiency"
081531984770a74e87dbd68907061b4b0f3631bf,"document : Real - Time Video Super - Resolution with Spatio - Temporal Networks and Motion Compensation Convolutional neural networks have enabled accurate image super - resolution in real - time . However , recent attempts to benefit from temporal correlations in video super - resolution have been limited to naive or inefficient architectures . In this paper , we introduce spatio - temporal sub - pixel convolution networks that effectively exploit temporal redundancies and improve reconstruction accuracy while maintaining real - time speed . Specifically , we discuss the use of early fusion , slow fusion and 3D convolutions for the joint processing of multiple consecutive video frames .",5,"['Task', 'Method', 'Material', 'Metric']","document : Real - Time Video Super - Resolution with Spatio - Temporal Networks and Motion Compensation Convolutional neural networks have enabled accurate image super - resolution in real - time . However , recent attempts to benefit from temporal correlations in video super - resolution have been limited to naive or inefficient architectures . In this paper , we introduce spatio - temporal sub - pixel convolution networks that effectively exploit temporal redundancies and improve reconstruction accuracy while maintaining real - time speed . Specifically , we discuss the use of early fusion , slow fusion and 3D convolutions for the joint processing of multiple consecutive video frames .","{'Task': ['Real - Time Video Super - Resolution', 'video super - resolution', 'image super - resolution in real - time', 'video'], 'Method': ['Convolutional neural networks', 'early fusion', 'spatio - temporal sub - pixel convolution networks', 'slow fusion', 'Motion Compensation', 'Spatio - Temporal Networks', '3D convolutions'], 'Material': [], 'Metric': ['real - time speed', 'reconstruction accuracy']}","{'Task': ['Real-time Video Super-Resolution'], 'Method': ['Spatio-temporal sub-pixel convolution networks', 'Early fusion', 'Slow fusion', '3D convolutions'], 'Material': ['Video frames'], 'Metric': ['Reconstruction accuracy']}","task: Real-time Video Super-Resolution
method:
- Spatio-Temporal Sub-Pixel Convolution Networks
- Early Fusion
- Slow Fusion
- 3D Convolusions
material:
- Convolutional neural networks
- Spatio-Temporal Networks
metric: Reconstruction Accuracy
- Real-time speed"
0834e74304b547c9354b6d7da6fa78ef47a48fa8,"document : LINE : Large - scale Information Network Embedding This paper studies the problem of embedding very large information networks into low - dimensional vector spaces , which is useful in many tasks such as visualization , node classification , and link prediction . Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes . In this paper , we propose a novel network embedding method called the “ LINE , ” which is suitable for arbitrary types of information networks : undirected , directed , and / or weighted . The method optimizes a carefully designed objective function that preserves both the local and global network structures .",5,"['Task', 'Method', 'Material', 'Metric']","document : LINE : Large - scale Information Network Embedding This paper studies the problem of embedding very large information networks into low - dimensional vector spaces , which is useful in many tasks such as visualization , node classification , and link prediction . Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes . In this paper , we propose a novel network embedding method called the “ LINE , ” which is suitable for arbitrary types of information networks : undirected , directed , and / or weighted . The method optimizes a carefully designed objective function that preserves both the local and global network structures .","{'Task': ['low - dimensional vector spaces', 'link prediction', 'node classification', 'real world information networks', 'information networks', 'embedding very large information networks', 'visualization'], 'Method': ['graph embedding methods', 'objective function', 'LINE', 'Large - scale Information Network Embedding', 'network embedding method'], 'Material': [], 'Metric': []}","{'Task': ['visualization', 'node classification', 'link prediction'], 'Method': ['LINE'], 'Material': ['information networks'], 'Metric': ['local network structures', 'global network structures']}","task: [""visualization"", ""node classification"", ""link prediction""]
method: [""LINE (Large-scale Information Network Embedding)""]
material: []
metric: [""local network structures"", ""global network structures""]"
0a3381f0432c5cfe491c718349d7a44e5814592c,"document : Compositional Sequence Labeling Models for Error Detection in Learner Writing In this paper , we present the first experiments using neural network models for the task of error detection in learner writing . We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs . Experiments on the CoNLL - 14 shared task dataset show the model is able to outperform other participants on detecting errors in learner writing . Finally , the model is integrated with a publicly deployed self - assessment system , leading to performance comparable to human annotators .",5,"['Task', 'Method', 'Material', 'Metric']","document : Compositional Sequence Labeling Models for Error Detection in Learner Writing In this paper , we present the first experiments using neural network models for the task of error detection in learner writing . We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs . Experiments on the CoNLL - 14 shared task dataset show the model is able to outperform other participants on detecting errors in learner writing . Finally , the model is integrated with a publicly deployed self - assessment system , leading to performance comparable to human annotators .","{'Task': ['Learner Writing', 'detecting errors in learner writing', 'error detection', 'learner writing', 'Error Detection'], 'Method': ['LSTMs', 'neural network models', 'compositional architectures', 'self - assessment', 'Compositional Sequence Labeling Models'], 'Material': ['CoNLL - 14 shared task dataset'], 'Metric': []}","{'Task': ['error detection in learner writing', 'detecting errors'], 'Method': ['neural network models', 'compositional architectures', 'framework for error detection based on bidirectional LSTMs', 'integrated with a publicly deployed self-assessment system'], 'Material': ['CoNLL-14 shared task dataset'], 'Metric': ['performance comparable to human annotators']}","task: Error Detection in Learner Writing
method:
- Neural Network Models
- Compositional Architectures
- Framework based on bidirectional LSTMs
material:
- CoNLL-14 shared task dataset
metric:
- Performance compared to human annotators"
0a6c36de8726b6feaab586046ddc1d1a008f44f9,"document : Filtered Channel Features for Pedestrian Detection This paper starts from the observation that multiple top performing pedestrian detectors can be modelled by using an intermediate layer filtering low - level features in combination with a boosted decision forest . Based on this observation we propose a unifying framework and experimentally explore different filter families . We report extensive results enabling a systematic analysis . Using filtered channel features we obtain top performance on the challenging Caltech and KITTI datasets , while using only HOG + LUV as low - level features .",5,"['Task', 'Method', 'Material', 'Metric']","document : Filtered Channel Features for Pedestrian Detection This paper starts from the observation that multiple top performing pedestrian detectors can be modelled by using an intermediate layer filtering low - level features in combination with a boosted decision forest . Based on this observation we propose a unifying framework and experimentally explore different filter families . We report extensive results enabling a systematic analysis . Using filtered channel features we obtain top performance on the challenging Caltech and KITTI datasets , while using only HOG + LUV as low - level features .","{'Task': ['Pedestrian Detection'], 'Method': ['boosted decision forest', 'unifying framework', 'Filtered Channel Features', 'top performing pedestrian detectors', 'intermediate layer filtering low - level features', 'filter families'], 'Material': ['Caltech', 'KITTI'], 'Metric': []}","{'Task': ['pedestrian detection'], 'Method': ['intermediate layer filtering', 'using a boosted decision forest', 'proposing a unifying framework', 'experimentally exploring different filter families'], 'Material': ['multiple top performing pedestrian detectors', 'low-level features', 'HOG + LUV'], 'Metric': ['extensive results', 'top performance', 'challenging Caltech and KITTI datasets']}","task: Pedestrian Detection
method: Intermediate layer filtering, Boosted decision forest
material: Multiple top performing pedestrian detectors, Low-level features (Filtered channel features, HOG + LUV)
metric: Extensive results, Top performance on Caltech and KITTI datasets"
0c47cad9729c38d9db1f75491b1ee4bd883a5d4e,"document : Semi - Supervised Sequence Modeling with Cross - View Training kevclark@cs.stanford.edu , thangluong@google.com , manning@cs.stanford.edu , qvl@google.com Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models , mainly because they can take advantage of large amounts of unlabeled text . However , the supervised models only learn from task - specific labeled data during the main training phase . We therefore propose Cross - View Training ( CVT ) , a semi - supervised learning algorithm that improves the representations of a Bi - LSTM sentence encoder using a mix of labeled and unlabeled data . On labeled examples , standard supervised learning is used .",5,"['Task', 'Method', 'Material', 'Metric']","document : Semi - Supervised Sequence Modeling with Cross - View Training kevclark@cs.stanford.edu , thangluong@google.com , manning@cs.stanford.edu , qvl@google.com Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models , mainly because they can take advantage of large amounts of unlabeled text . However , the supervised models only learn from task - specific labeled data during the main training phase . We therefore propose Cross - View Training ( CVT ) , a semi - supervised learning algorithm that improves the representations of a Bi - LSTM sentence encoder using a mix of labeled and unlabeled data . On labeled examples , standard supervised learning is used .","{'Task': [], 'Method': ['Semi - Supervised Sequence Modeling', 'Unsupervised representation learning algorithms', 'Bi - LSTM sentence encoder', 'semi - supervised learning algorithm', 'CVT', 'supervised models', 'supervised learning', 'Cross - View Training', 'supervised NLP models', 'word2vec', 'ELMo'], 'Material': [], 'Metric': ['accuracy']}","{'Task': ['improving the accuracy of many supervised NLP models', 'using a mix of labeled and unlabeled data to improve the representations of a Bi- LSTM sentence encoder'],
 'Method': ['Semi- Supervised Learning', 'Cross- View Training (CVT)', 'standard supervised learning'],
 'Material': ['large amounts of unlabeled text', 'task- specific labeled data', 'Bi- LSTM sentence encoder'],
 'Metric': ['accuracy']}","task:
- Semi-supervised Sequence Modeling
- Improving representations of a Bi-LSTM sentence encoder

method:
- Cross-View Training (CVT)
- Unsupervised representation learning (word2vec, ELMo)
- Standard supervised learning

material:
- Large amounts of unlabeled text
- Labeled text
- Bi-LSTM sentence encoder

metric:
- Accuracy (of NLP models)
- Improvement in representations (of Bi-LSTM sentence encoder)"
0dc9eb7d17f2def56ad930945f2521653f04c3fa,Skip - gram Language Modeling Using Sparse Non - negative Matrix Probability Estimation section : Abstract We present a novel family of language model ( LM ) estimation techniques named Sparse Non - negative Matrix ( SNM ) estimation . A first set of experiments empirically evaluating it on the One Billion Word Benchmark [ reference ] shows that SNM n - gram LMs perform almost as well as the well - established Kneser - Ney ( KN ) models .,5,"['Task', 'Method', 'Material', 'Metric']",Skip - gram Language Modeling Using Sparse Non - negative Matrix Probability Estimation section : Abstract We present a novel family of language model ( LM ) estimation techniques named Sparse Non - negative Matrix ( SNM ) estimation . A first set of experiments empirically evaluating it on the One Billion Word Benchmark [ reference ] shows that SNM n - gram LMs perform almost as well as the well - established Kneser - Ney ( KN ) models .,"{'Task': ['Skip - gram Language Modeling'], 'Method': ['SNM n - gram LMs', 'KN', 'LM ) estimation techniques', 'Sparse Non - negative Matrix Probability Estimation', 'Sparse Non - negative Matrix', 'language model', 'Kneser - Ney', 'SNM'], 'Material': ['One Billion Word Benchmark'], 'Metric': []}","{'Task': ['language model estimation'], 'Method': ['Sparse Non-negative Matrix (SNM) estimation'], 'Material': ['One Billion Word Benchmark'], 'Metric': ['performance']}","task: Language Modeling
method: [Sparse Non-negative Matrix (SNM) estimation]
material: [One Billion Word Benchmark]
metric: [Performance evaluation against Kneser-Ney (KN) models]"
107010b7f2abe3c0c9df62bcef35eb77f6fc76df,"Domain - Adversarial Training of Neural Networks section : Abstract We introduce a new representation learning approach for domain adaptation , in which data at training and test time come from similar but different distributions . Our approach is directly inspired by the theory on domain adaptation suggesting that , for effective domain transfer to be achieved , predictions must be made based on features that can not discriminate between the training ( source ) and test ( target ) domains . The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain ( no labeled target - domain data is necessary ) .",5,"['Task', 'Method', 'Material', 'Metric']","Domain - Adversarial Training of Neural Networks section : Abstract We introduce a new representation learning approach for domain adaptation , in which data at training and test time come from similar but different distributions . Our approach is directly inspired by the theory on domain adaptation suggesting that , for effective domain transfer to be achieved , predictions must be made based on features that can not discriminate between the training ( source ) and test ( target ) domains . The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain ( no labeled target - domain data is necessary ) .","{'Task': ['domain transfer', 'predictions'], 'Method': ['neural network architectures', 'Domain - Adversarial Training of Neural Networks', 'representation learning approach', 'domain adaptation'], 'Material': [], 'Metric': []}","{'Task': ['domain adaptation'], 'Method': ['representation learning approach', 'adversarial training of neural networks', 'effective domain transfer'], 'Material': ['data', 'labeled data from the source domain', 'unlabeled data from the target domain'], 'Metric': ['similar but different distributions', 'discriminate between the training ( source ) and test ( target ) domains']}","task: Domain Adaptation
method: Representation Learning Approach for Domain Adaptation, Making predictions based on features that can't discriminate between source and target domains
material: Neural Network Architectures, Labeled data from source domain, Unlabeled data from target domain
metric: Effective domain transfer, Similar but different distributions between training and test data."
1130d8fdd931225c2d7563c3808367726cfa1c3a,"document : PixelGAN Autoencoders In this paper , we describe the ‘ ‘ PixelGAN autoencoder ’ ’ , a generative autoencoder in which the generative path is a convolutional autoregressive neural network on pixels ( PixelCNN ) that is conditioned on a latent code , and the recognition path uses a generative adversarial network ( GAN ) to impose a prior distribution on the latent code . We show that different priors result in different decompositions of information between the latent code and the autoregressive decoder . For example , by imposing a Gaussian distribution as the prior , we can achieve a global vs. local decomposition , or by imposing a categorical distribution as the prior , we can disentangle the style and content information of images in an unsupervised fashion . We further show how the PixelGAN autoencoder with a categorical prior can be directly used in semi - supervised settings and achieve competitive semi - supervised classification results on the MNIST , SVHN and NORB datasets .",5,"['Task', 'Method', 'Material', 'Metric']","document : PixelGAN Autoencoders In this paper , we describe the ‘ ‘ PixelGAN autoencoder ’ ’ , a generative autoencoder in which the generative path is a convolutional autoregressive neural network on pixels ( PixelCNN ) that is conditioned on a latent code , and the recognition path uses a generative adversarial network ( GAN ) to impose a prior distribution on the latent code . We show that different priors result in different decompositions of information between the latent code and the autoregressive decoder . For example , by imposing a Gaussian distribution as the prior , we can achieve a global vs. local decomposition , or by imposing a categorical distribution as the prior , we can disentangle the style and content information of images in an unsupervised fashion . We further show how the PixelGAN autoencoder with a categorical prior can be directly used in semi - supervised settings and achieve competitive semi - supervised classification results on the MNIST , SVHN and NORB datasets .","{'Task': ['competitive semi - supervised classification', 'semi - supervised settings'], 'Method': ['unsupervised fashion', 'recognition path', 'autoregressive decoder', 'GAN', 'Gaussian distribution', 'convolutional autoregressive neural network on pixels', 'autoencoder', 'PixelGAN Autoencoders', 'generative path', 'PixelCNN', 'generative adversarial network', 'generative autoencoder'], 'Material': ['MNIST'], 'Metric': []}","{'Task': ['generative autoencoder', 'semi-supervised settings', 'semi-supervised classification'],
 'Method': ['PixelGAN autoencoder', 'convolutional autoregressive neural network (PixelCNN)', 'generative adversarial network (GAN)', 'imposing a prior distribution'],
 'Material': ['latent code', 'autoregressive decoder', 'MNIST', 'SVHN', 'NORB'],
 'Metric': ['competitive results']}","task:
- Generative model training
- Semi-supervised classification
method:
- PixelGAN autoencoder
  - Generative path: Convolutional autoregressive neural network (PixelCNN)
  - Recognition path: Generative adversarial network (GAN)
material:
- Latent code
- Pixel data
metric:
- Classification accuracy (MNIST, SVHN, NORB datasets)
- Decomposition of information (global vs. local, style and content)"
11356cd6bb0f2776a88cd584ff108470414c6594,"document : Submanifold Sparse Convolutional Networks Convolutional network are the de - facto standard for analysing spatio - temporal data such as images , videos , 3D shapes , etc . Whilst some of this data is naturally dense ( for instance , photos ) , many other data sources are inherently sparse . Examples include pen - strokes forming on a piece of paper , or ( colored ) 3D point clouds that were obtained using a LiDAR scanner or RGB - D camera . Standard",5,"['Task', 'Method', 'Material', 'Metric']","document : Submanifold Sparse Convolutional Networks Convolutional network are the de - facto standard for analysing spatio - temporal data such as images , videos , 3D shapes , etc . Whilst some of this data is naturally dense ( for instance , photos ) , many other data sources are inherently sparse . Examples include pen - strokes forming on a piece of paper , or ( colored ) 3D point clouds that were obtained using a LiDAR scanner or RGB - D camera . Standard","{'Task': [], 'Method': ['Submanifold Sparse Convolutional Networks', 'RGB - D camera', 'LiDAR scanner', 'Convolutional network'], 'Material': [], 'Metric': []}","Based on the given document, here's the Python dictionary with extracted entities:

```python
{'Task': ['analysing spatio-temporal data', 'obtaining 3D point clouds using LiDAR scanner or RGB-D camera'],
 'Method': ['Convolutional network'],
 'Material': ['images', 'videos', '3D shapes', 'pen strokes on a piece of paper', 'colored 3D point clouds'],
 'Metric': ['Ellipsis']}
```

The 'Metric' key is left empty as no metric names were mentioned in the document.","```yaml
Tasks:
- Analyzing spatio-temporal data

Methods:
- Convolutional neural networks
- Submanifold sparse convolutional networks

Materials:
- Images
- Videos
- 3D shapes
- Pen strokes on a piece of paper
- Colored 3D point clouds
- LiDAR scanner data
- RGB-D camera data

Metrics:
- Not explicitly mentioned in the document
```"
11da0c54ba904a1cb31a09d10da55f73e8825c61,"document : Natural Language Inference by Tree - Based Convolution and Heuristic Matching In this paper , we propose the TBCNN - pair model to recognize entailment and contradiction between two sentences . In our model , a tree - based convolutional neural network ( TBCNN ) captures sentence - level semantics ; then heuristic matching layers like concatenation , element - wise product / difference combine the information in individual sentences . Experimental results show that our model outperforms existing sentence encoding - based approaches by a large margin . section : Introduction",5,"['Task', 'Method', 'Material', 'Metric']","document : Natural Language Inference by Tree - Based Convolution and Heuristic Matching In this paper , we propose the TBCNN - pair model to recognize entailment and contradiction between two sentences . In our model , a tree - based convolutional neural network ( TBCNN ) captures sentence - level semantics ; then heuristic matching layers like concatenation , element - wise product / difference combine the information in individual sentences . Experimental results show that our model outperforms existing sentence encoding - based approaches by a large margin . section : Introduction","{'Task': ['recognize entailment and contradiction', 'Natural Language Inference'], 'Method': ['sentence encoding - based approaches', 'concatenation', 'Heuristic Matching', 'TBCNN - pair model', 'Tree - Based Convolution', 'TBCNN', 'tree - based convolutional neural network', 'heuristic matching layers'], 'Material': [], 'Metric': []}","{'Task': ['recognize entailment and contradiction between two sentences'],
 'Method': ['Tree-based convolutional neural network (TBCNN)', 'heuristic matching layers'],
 'Material': ['sentences'],
 'Metric': ['experimental results', 'large margin']}","task: Natural Language Inference
method:
- Tree-based Convolution Neural Network (TBCNN)
- Heuristic matching layers (concatenation, element-wise product / difference)
material: None
metric: Experimental results for model performance evaluation against existing sentence encoding-based approaches."
1235dd37312cb20aced0e97d953f6379d8a0c7d4,"document : Grounded Textual Entailment Capturing semantic relations between sentences , such as entailment , is a long - standing challenge for computational semantics . Logic - based models analyse entailment in terms of possible worlds ( interpretations , or situations ) where a premise P entails a hypothesis H iff in all worlds where P is true , H is also true . Statistical models view this relationship probabilistically , addressing it in terms of whether a human would likely infer H from P. In this paper , we wish to bridge these two perspectives , by arguing for a visually - grounded version of the Textual Entailment task .",5,"['Task', 'Method', 'Material', 'Metric']","document : Grounded Textual Entailment Capturing semantic relations between sentences , such as entailment , is a long - standing challenge for computational semantics . Logic - based models analyse entailment in terms of possible worlds ( interpretations , or situations ) where a premise P entails a hypothesis H iff in all worlds where P is true , H is also true . Statistical models view this relationship probabilistically , addressing it in terms of whether a human would likely infer H from P. In this paper , we wish to bridge these two perspectives , by arguing for a visually - grounded version of the Textual Entailment task .","{'Task': ['Grounded Textual Entailment', 'computational semantics', 'Textual Entailment task', 'Capturing semantic relations between sentences'], 'Method': ['Statistical models', 'Logic - based models', 'visually - grounded version'], 'Material': [], 'Metric': []}","{'Task': ['Grounded Textual Entailment', 'Textual Entailment'],
 'Method': ['Logic-based models', 'Probabilistic models', 'Visually-grounded version'],
 'Material': ['sentences', 'premise P', 'hypothesis H', 'interpretations or situations'],
 'Metric': ['entailment']}","Task:
- Textual Entailment

Method:
- Logic-based models
- Statistical models
- Visually-grounded version

Material:
- Sentences (premise P and hypothesis H)
- Interpretations or situations (possible worlds)

Metric:
- Entailment (iff in all worlds where P is true, H is also true)
- Probability (addressed by statistical models)"
14318685b5959b51d0f1e3db34643eb2855dc6d9,"document : Going deeper with convolutions We propose a deep convolutional neural network architecture codenamed âInceptionâ , which was responsible for setting the new state of the art for classification and detection in the ImageNet Large - Scale Visual Recognition Challenge 2014 ( ILSVRCâ14 ) . The main hallmark of this architecture is the improved utilization of the computing resources inside the network . This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant . To optimize quality , the architectural decisions were based on the Hebbian principle and the intuition of multi - scale processing .",5,"['Task', 'Method', 'Material', 'Metric']","document : Going deeper with convolutions We propose a deep convolutional neural network architecture codenamed âInceptionâ , which was responsible for setting the new state of the art for classification and detection in the ImageNet Large - Scale Visual Recognition Challenge 2014 ( ILSVRCâ14 ) . The main hallmark of this architecture is the improved utilization of the computing resources inside the network . This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant . To optimize quality , the architectural decisions were based on the Hebbian principle and the intuition of multi - scale processing .","{'Task': ['classification', 'detection', 'ImageNet Large - Scale Visual Recognition Challenge 2014', 'ILSVRCâ\x80\x9914'], 'Method': ['deep convolutional neural network architecture codenamed â\x80\x9cInceptionâ\x80\x9d', 'intuition of multi - scale processing', 'convolutions', 'Hebbian principle'], 'Material': [], 'Metric': ['quality']}","{'Task': ['ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC''14)'], 'Method': ['deep convolutional neural network architecture', 'improved utilization of computing resources', 'carefully crafted design', 'Hebbian principle', 'intuition of multi-scale processing'], 'Material': [], 'Metric': ['classification', 'detection', 'computational budget']}","task: Image Classification, Object Detection
method: Proposed a deep convolutional neural network architecture named ""Inception""
material: Computing resources inside the network
metric: Computational budget, Depth, Width, Quality (optimized based on Hebbian principle and multi-scale processing)"
16051bbe3a7f7c77a952ebf76722ea655e8906ca,"Image Super - resolution via Feature - augmented Random Forest section : Abstract - Recent random - forest ( RF )- based image super - resolution approaches inherit some properties from dictionary - learning - based algorithms , but the effectiveness of the properties in RF is overlooked in the literature . In this paper , we present a novel feature - augmented random forest ( FARF ) for image super - resolution , where the conventional gradient - based features are augmented with gradient magnitudes and different feature recipes are formulated on different stages in an RF . The advantages of our method are that , firstly , the dictionary - learning - based features are enhanced by adding gradient magnitudes , based on the observation that the non - linear gradient magnitude are with highly discriminative property .",5,"['Task', 'Method', 'Material', 'Metric']","Image Super - resolution via Feature - augmented Random Forest section : Abstract - Recent random - forest ( RF )- based image super - resolution approaches inherit some properties from dictionary - learning - based algorithms , but the effectiveness of the properties in RF is overlooked in the literature . In this paper , we present a novel feature - augmented random forest ( FARF ) for image super - resolution , where the conventional gradient - based features are augmented with gradient magnitudes and different feature recipes are formulated on different stages in an RF . The advantages of our method are that , firstly , the dictionary - learning - based features are enhanced by adding gradient magnitudes , based on the observation that the non - linear gradient magnitude are with highly discriminative property .","{'Task': ['image super - resolution', 'Image Super - resolution'], 'Method': ['Recent random - forest', 'dictionary - learning - based features', 'RF', 'dictionary - learning - based algorithms', 'FARF', 'feature - augmented random forest', 'gradient - based features', 'Feature - augmented Random Forest'], 'Material': [], 'Metric': []}","{'Task': ['image super- resolution'], 'Method': ['random forest', 'feature-augmented random forest (FARF)'], 'Material': [], 'Metric': ['effectiveness', 'discriminative property']}","task: Image Super-resolution
method:
- Feature-augmented Random Forest (FARF)
material:
- Conventional gradient-based features
- Gradient magnitudes
metric:
- Discriminative property"
160563abbd75265b19afc8b4169bab9e1eb33d97,"document : Massively Multilingual Sentence Embeddings for Zero - Shot Cross - Lingual Transfer and Beyond We introduce an architecture to learn joint multilingual sentence representations for 93 languages , belonging to more than 30 different language families and written in 28 different scripts . Our system uses a single BiLSTM encoder with a shared BPE vocabulary for all languages , which is coupled with an auxiliary decoder and trained on publicly available parallel corpora . This enables us to learn a classifier on top of the resulting sentence embeddings using English annotated data only , and transfer it to any of the 93 languages without any modification . Our approach sets a new state - of - the - art on zero - shot cross - lingual natural language inference for all the 14 languages in the XNLI dataset but one .",5,"['Task', 'Method', 'Material', 'Metric']","document : Massively Multilingual Sentence Embeddings for Zero - Shot Cross - Lingual Transfer and Beyond We introduce an architecture to learn joint multilingual sentence representations for 93 languages , belonging to more than 30 different language families and written in 28 different scripts . Our system uses a single BiLSTM encoder with a shared BPE vocabulary for all languages , which is coupled with an auxiliary decoder and trained on publicly available parallel corpora . This enables us to learn a classifier on top of the resulting sentence embeddings using English annotated data only , and transfer it to any of the 93 languages without any modification . Our approach sets a new state - of - the - art on zero - shot cross - lingual natural language inference for all the 14 languages in the XNLI dataset but one .","{'Task': ['Massively Multilingual Sentence Embeddings', 'sentence representations', 'Zero - Shot Cross - Lingual Transfer', 'cross - lingual natural language inference'], 'Method': ['auxiliary decoder', 'sentence embeddings', 'BiLSTM encoder', 'BPE', 'classifier'], 'Material': ['XNLI dataset'], 'Metric': []}","{'Task': ['zero-shot cross-lingual natural language inference'],
 'Method': ['learning joint multilingual sentence representations', 'using a single BiLSTM encoder with a shared BPE vocabulary', 'coupled with an auxiliary decoder', 'trained on publicly available parallel corpora', 'enables us to learn a classifier on top of the resulting sentence embeddings'],
 'Material': ['93 languages', 'belonging to more than 30 different language families and written in 28 different scripts', 'English annotated data'],
 'Metric': ['sets a new state-of-the-art']}","task:
- zero-shot cross-lingual natural language inference
- setting new state-of-the-art

method:
- learning joint multilingual sentence representations
- using a single BiLSTM encoder with a shared BPE vocabulary
- coupling with an auxiliary decoder
- training on publicly available parallel corpora

material:
- 93 languages
- 30 different language families
- 28 different scripts
- English annotated data

metric:
- accuracy on XNLI dataset for 14 languages"
175f74a09241b6cb5101a2a09978095720db7d5f,"document : Image Super - Resolution via Dual - State Recurrent Networks Advances in image super - resolution ( SR ) have recently benefited significantly from rapid developments in deep neural networks . Inspired by these recent discoveries , we note that many state - of - the - art deep SR architectures can be reformulated as a single - state recurrent neural network ( RNN ) with finite unfoldings . In this paper , we explore new structures for SR based on this compact RNN view , leading us to a dual - state design , the Dual - State Recurrent Network ( DSRN ) . Compared to its single - state counterparts that operate at a fixed spatial resolution , DSRN exploits both low - resolution ( LR ) and high - resolution ( HR ) signals jointly .",5,"['Task', 'Method', 'Material', 'Metric']","document : Image Super - Resolution via Dual - State Recurrent Networks Advances in image super - resolution ( SR ) have recently benefited significantly from rapid developments in deep neural networks . Inspired by these recent discoveries , we note that many state - of - the - art deep SR architectures can be reformulated as a single - state recurrent neural network ( RNN ) with finite unfoldings . In this paper , we explore new structures for SR based on this compact RNN view , leading us to a dual - state design , the Dual - State Recurrent Network ( DSRN ) . Compared to its single - state counterparts that operate at a fixed spatial resolution , DSRN exploits both low - resolution ( LR ) and high - resolution ( HR ) signals jointly .","{'Task': ['image super - resolution', 'Image Super - Resolution', 'HR', 'high - resolution', 'SR'], 'Method': ['finite unfoldings', 'deep neural networks', 'single - state recurrent neural network', 'compact RNN view', 'Dual - State Recurrent Network', 'DSRN', 'Dual - State Recurrent Networks', 'dual - state design', 'RNN'], 'Material': [], 'Metric': []}","{'Task': ['image super-resolution'], 'Method': ['single-state recurrent neural network (RNN)', 'finite unfoldings', 'compact RNN view', 'dual-state design', 'Dual-State Recurrent Network (DSRN)'], 'Material': [], 'Metric': ['spatial resolution']}","task: Image Super-Resolution
method:
- Single-State Recurrent Neural Network (RNN)
- Dual-State Design
- Dual-State Recurrent Network (DSRN)
material: Deep Neural Networks
metric:
- Spatial Resolution
- Low-Resolution (LR) signals
- High-Resolution (HR) signals"
1778e32c18bd611169e64c1805a51abff341ca53,"document : Natural Language Inference over Interaction Space Natural Language Inference ( NLI ) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis . We introduce Interactive Inference Network ( IIN ) , a novel class of neural network architectures that is able to achieve high - level understanding of the sentence pair by hierarchically extracting semantic features from interaction space . We show that an interaction tensor ( attention weight ) contains semantic information to solve natural language inference , and a denser interaction tensor contains richer semantic information . One instance of such architecture , Densely Interactive Inference Network ( DIIN ) , demonstrates the state - of - the - art performance on large scale NLI copora and large - scale NLI alike corpus .",5,"['Task', 'Method', 'Material', 'Metric']","document : Natural Language Inference over Interaction Space Natural Language Inference ( NLI ) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis . We introduce Interactive Inference Network ( IIN ) , a novel class of neural network architectures that is able to achieve high - level understanding of the sentence pair by hierarchically extracting semantic features from interaction space . We show that an interaction tensor ( attention weight ) contains semantic information to solve natural language inference , and a denser interaction tensor contains richer semantic information . One instance of such architecture , Densely Interactive Inference Network ( DIIN ) , demonstrates the state - of - the - art performance on large scale NLI copora and large - scale NLI alike corpus .","{'Task': ['Interaction Space', 'NLI', 'natural language inference', 'Natural Language Inference'], 'Method': ['DIIN', 'Densely Interactive Inference Network', 'IIN )', 'Interactive Inference Network', 'neural network architectures'], 'Material': [], 'Metric': []}","{'Task': ['Natural Language Inference'], 'Method': ['Interactive Inference Network', 'Hierarchically extracting semantic features from interaction space', 'Densely Interactive Inference Network'], 'Material': ['sentence pair', 'interaction space', 'interaction tensor', 'attention weight'], 'Metric': ['high-level understanding', 'state-of-the-art performance', 'large scale NLI corpora', 'large-scale NLI alike corpus']}","Task: Natural Language Inference (NLI)
Method:
- Interactive Inference Network (IIN)
- Densely Interactive Inference Network (DIIN)
Material:
- Natural language premise
- Natural language hypothesis
- Interaction tensor (attention weight)
Metric:
- Performance on large scale NLI corpora
- Performance on large-scale NLI alike corpora

```yaml
Task: Natural_Language_Inference (NLI)
Method:
  - Interactive_Inference_Network (IIN)
  - Densely_Interactive_Inference_Network (DIIN)
Material:
  - natural_language_premise
  - natural_language_hypothesis
  - interaction_tensor (attention_weight)
Metric:
  - performance_on_large_scale_NLI_corpora
  - performance_on_large_scale_NLI_alike_corpora
```"
178275dbdcfa267e41a9d5efe386ee5874c6d23f,"document : Fraternal Dropout Recurrent neural networks ( RNNs ) form an important class of architectures among neural networks useful for language modeling and sequential prediction . However , optimizing RNNs is known to be harder compared to feed - forward neural networks . A number of techniques have been proposed in literature to address this problem . In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal .",5,"['Task', 'Method', 'Material', 'Metric']","document : Fraternal Dropout Recurrent neural networks ( RNNs ) form an important class of architectures among neural networks useful for language modeling and sequential prediction . However , optimizing RNNs is known to be harder compared to feed - forward neural networks . A number of techniques have been proposed in literature to address this problem . In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal .","{'Task': ['optimizing RNNs', 'language modeling', 'sequential prediction'], 'Method': ['Recurrent neural networks', 'RNNs', 'feed - forward neural networks', 'neural networks', 'fraternal dropout', 'dropout'], 'Material': [], 'Metric': []}","{'Task': ['language modeling', 'sequential prediction', 'optimizing RNNs'],
 'Method': ['fraternal dropout', 'dropout'],
 'Material': ['RNNs'],
 'Metric': ['goal']}","task: Language modeling and sequential prediction
method:
- Optimizing RNNs
- Proposing a technique called fraternal dropout
material: RNNs (Recurrent neural networks)
metric: Performance of RNNs in language modeling and sequential prediction tasks."
178631e0f0e624b1607c7a7a2507ed30d4e83a42,"document : Speech Recognition with Deep Recurrent Neural Networks Recurrent neural networks ( RNNs ) are a powerful model for sequential data . End - to - end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input - output alignment is unknown . The combination of these methods with the Long Short - term Memory RNN architecture has proved particularly fruitful , delivering state - of - the - art results in cursive handwriting recognition . However RNN performance in speech recognition has so far been disappointing , with better results returned by deep feedforward networks .",5,"['Task', 'Method', 'Material', 'Metric']","document : Speech Recognition with Deep Recurrent Neural Networks Recurrent neural networks ( RNNs ) are a powerful model for sequential data . End - to - end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input - output alignment is unknown . The combination of these methods with the Long Short - term Memory RNN architecture has proved particularly fruitful , delivering state - of - the - art results in cursive handwriting recognition . However RNN performance in speech recognition has so far been disappointing , with better results returned by deep feedforward networks .","{'Task': ['sequence labelling problems', 'Speech Recognition', 'handwriting recognition', 'speech'], 'Method': ['Connectionist Temporal Classification', 'Recurrent neural networks', 'deep feedforward networks', 'RNNs', 'end training methods', 'Deep Recurrent Neural Networks', 'RNN', 'Long Short - term Memory RNN architecture'], 'Material': [], 'Metric': []}","{'Task': ['sequence labelling', 'speech recognition'], 'Method': ['Connectionist Temporal Classification', 'end-to-end training', 'Long Short-term Memory RNN architecture'], 'Material': ['recurrent neural networks (RNNs)'], 'Metric': ['state-of-the-art results']}","task: Speech Recognition, Cursive Handwriting Recognition
method: End-to-end training methods (Connectionist Temporal Classification), Long Short-term Memory RNN architecture
material: Recurrent neural networks (RNNs)
metric: State-of-the-art results"
18168aea48a22f6fe2fe407c0ff70083cba225a7,"document : Image Restoration Using Very Deep Convolutional Encoder - Decoder Networks with Symmetric Skip Connections In this paper , we propose a very deep fully convolutional encoding - decoding framework for image restoration such as denoising and super - resolution . The network is composed of multiple layers of convolution and de - convolution operators , learning end - to - end mappings from corrupted images to the original ones . The convolutional layers act as the feature extractor , which capture the abstraction of image contents while eliminating noises / corruptions . De - convolutional layers are then used to recover the image details .",5,"['Task', 'Method', 'Material', 'Metric']","document : Image Restoration Using Very Deep Convolutional Encoder - Decoder Networks with Symmetric Skip Connections In this paper , we propose a very deep fully convolutional encoding - decoding framework for image restoration such as denoising and super - resolution . The network is composed of multiple layers of convolution and de - convolution operators , learning end - to - end mappings from corrupted images to the original ones . The convolutional layers act as the feature extractor , which capture the abstraction of image contents while eliminating noises / corruptions . De - convolutional layers are then used to recover the image details .","{'Task': ['image restoration', 'super - resolution', 'denoising', 'Image Restoration'], 'Method': ['Symmetric Skip Connections', 'convolutional layers', 'Deep Convolutional Encoder - Decoder Networks', 'deep fully convolutional encoding - decoding framework', 'De - convolutional layers', 'feature extractor', 'layers of convolution and de - convolution operators'], 'Material': [], 'Metric': []}","{'Task': ['image restoration', 'denoising', 'super-resolution'], 'Method': ['very deep fully convolutional encoding-decoding framework', 'convolution and de-convolution operators', 'learning end-to-end mappings'], 'Material': ['corrupted images'], 'Metric': ['original images']}","task: [""Image Restoration"", ""Denoising"", ""Super-Resolution""]
method: [""Very Deep Fully Convolutional Encoding-Decoding Framework""]
material: [""Multiple layers of convolution and de-convolution operators""]
metric: [""End-to-end mappings from corrupted images to the original ones""]"
193089d56758ab88391d846edd08d359b1f9a863,"document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( CNN ) in person re - identification ( re - ID ) , i.e. , verification and identification models . The two models have their respective advantages and limitations due to different loss functions . In this paper , we shed light on how to combine the two models to learn more discriminative pedestrian descriptors . Specifically , we propose a siamese network that simultaneously computes the identification loss and verification loss .",5,"['Task', 'Method', 'Material', 'Metric']","document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( CNN ) in person re - identification ( re - ID ) , i.e. , verification and identification models . The two models have their respective advantages and limitations due to different loss functions . In this paper , we shed light on how to combine the two models to learn more discriminative pedestrian descriptors . Specifically , we propose a siamese network that simultaneously computes the identification loss and verification loss .","{'Task': ['discriminative pedestrian descriptors', 'verification', 'Person Re - identification', 'identification models', 'person re - identification', 're - ID'], 'Method': ['siamese network', 'Discriminatively Learned CNN Embedding', 'CNN', 'convolutional neural networks'], 'Material': [], 'Metric': ['identification loss']}","{'Task': ['person re-identification', 'verification', 'identification'],
 'Method': ['convolutional neural networks (CNN)', 'siamese network', 'discriminatively learned CNN embedding'],
 'Material': [Ellipsis],
 'Metric': ['identification loss', 'verification loss']}","task: [""person re-identification"", ""verification"", ""identification""]
method: [""convolutional neural networks (CNN)"", ""siamese network"", ""discriminatively learned CNN embedding""]
material: []
metric: [""identification loss"", ""verification loss""]"
193b518bc3025804c6d587c74cbc154d91478417,"document : Learning to Adapt Structured Output Space for Semantic Segmentation Convolutional neural network - based approaches for semantic segmentation rely on supervision with pixel - level ground truth , but may not generalize well to unseen image domains . As the labeling process is tedious and labor intensive , developing algorithms that can adapt source ground truth labels to the target domain is of great interest . In this paper , we propose an adversarial learning method for domain adaptation in the context of semantic segmentation . Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains , we adopt adversarial learning in the output space .",5,"['Task', 'Method', 'Material', 'Metric']","document : Learning to Adapt Structured Output Space for Semantic Segmentation Convolutional neural network - based approaches for semantic segmentation rely on supervision with pixel - level ground truth , but may not generalize well to unseen image domains . As the labeling process is tedious and labor intensive , developing algorithms that can adapt source ground truth labels to the target domain is of great interest . In this paper , we propose an adversarial learning method for domain adaptation in the context of semantic segmentation . Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains , we adopt adversarial learning in the output space .","{'Task': ['semantic segmentation', 'Semantic Segmentation', 'labeling process', 'semantic segmentations'], 'Method': ['adversarial learning', 'Convolutional neural network - based approaches', 'adversarial learning method', 'domain adaptation'], 'Material': [], 'Metric': []}","{'Task': ['semantic segmentation'], 'Method': ['adversarial learning', 'domain adaptation'], 'Material': ['convolutional neural network', 'pixel-level ground truth', 'source ground truth labels', 'target domain'], 'Metric': ['generalization', 'performance']}","Task:
- Semantic segmentation
- Domain adaptation

Method:
- Adversarial learning
- Output space adaptation

Material:
- Convolutional neural networks
- Pixel-level ground truth labels
- Source ground truth labels
- Target domain

Metric:
- Spatial similarities between source and target domains"
1d0dcb458aa4d30b51f7c74b159be687f39120a0,"document : Pose - driven Deep Convolutional Model for Person Re - identification Feature extraction and matching are two crucial components in person Re - Identification ( ReID ) . The large pose deformations and the complex view variations exhibited by the captured person images significantly increase the difficulty of learning and matching of the features from person images . To overcome these difficulties , in this work we propose a Pose - driven Deep Convolutional ( PDC ) model to learn improved feature extraction and matching models from end to end . Our deep architecture explicitly leverages the human part cues to alleviate the pose variations and learn robust feature representations from both the global image and different local parts .",5,"['Task', 'Method', 'Material', 'Metric']","document : Pose - driven Deep Convolutional Model for Person Re - identification Feature extraction and matching are two crucial components in person Re - Identification ( ReID ) . The large pose deformations and the complex view variations exhibited by the captured person images significantly increase the difficulty of learning and matching of the features from person images . To overcome these difficulties , in this work we propose a Pose - driven Deep Convolutional ( PDC ) model to learn improved feature extraction and matching models from end to end . Our deep architecture explicitly leverages the human part cues to alleviate the pose variations and learn robust feature representations from both the global image and different local parts .","{'Task': ['learning and matching of the features', 'matching', 'person Re - Identification', 'ReID', 'Person Re - identification', 'Feature extraction'], 'Method': ['deep architecture', 'PDC', 'robust feature representations', 'feature extraction', 'Pose - driven Deep Convolutional', 'matching models'], 'Material': [], 'Metric': []}","{'Task': ['Person Re-Identification'], 'Method': ['Pose-driven Deep Convolutional (PDC) model', 'Feature extraction', 'Feature matching'], 'Material': ['Person images'], 'Metric': ['Difficulty of learning and matching of features', 'Robustness of feature representations']}","task: Person Re-Identification (ReID)
method:
- Pose-driven Deep Convolutional (PDC) model
material:
- Human part cues
metric:
- Feature extraction and matching accuracy
- Robustness of feature representations against pose variations and complex view variations."
1f08598381af9146d0fd9a61b30d0e51a7331689,"document : Distributed Prioritized Experience Replay We propose a distributed architecture for deep reinforcement learning at scale , that enables agents to learn effectively from orders of magnitude more data than previously possible . The algorithm decouples acting from learning : the actors interact with their own instances of the environment by selecting actions according to a shared neural network , and accumulate the resulting experience in a shared experience replay memory ; the learner replays samples of experience and updates the neural network . The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors . Our architecture substantially improves the state of the art on the Arcade Learning Environment , achieving better final performance in a fraction of the wall - clock training time .",5,"['Task', 'Method', 'Material', 'Metric']","document : Distributed Prioritized Experience Replay We propose a distributed architecture for deep reinforcement learning at scale , that enables agents to learn effectively from orders of magnitude more data than previously possible . The algorithm decouples acting from learning : the actors interact with their own instances of the environment by selecting actions according to a shared neural network , and accumulate the resulting experience in a shared experience replay memory ; the learner replays samples of experience and updates the neural network . The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors . Our architecture substantially improves the state of the art on the Arcade Learning Environment , achieving better final performance in a fraction of the wall - clock training time .","{'Task': ['Distributed Prioritized Experience Replay', 'deep reinforcement learning', 'learning', 'prioritized experience replay'], 'Method': ['shared neural network', 'distributed architecture', 'neural network'], 'Material': ['Arcade Learning Environment'], 'Metric': ['wall - clock training time']}","{'Task': ['deep reinforcement learning', 'learning'], 'Method': ['distributed architecture', 'decouples acting from learning', 'prioritized experience replay'], 'Material': ['orders of magnitude more data', 'shared neural network', 'shared experience replay memory'], 'Metric': ['final performance', 'wall-clock training time']}","```yaml
Task: deep reinforcement learning at scale
Method: distributed architecture, decoupled acting from learning, prioritized experience replay
Material: neural network, experience replay memory
Metric: final performance, wall-clock training time
```"
2138a7127429d67746ec78de46d6820fee0e548e,"document : Graph2Seq : Graph to Sequence Learning with Attention - Based Neural Networks The celebrated Sequence to Sequence learning ( Seq2Seq ) technique and its numerous variants achieve excellent performance on many tasks . However , many machine learning tasks have inputs naturally represented as graphs ; existing Seq2Seq models face a significant challenge in achieving accurate conversion from graph form to the appropriate sequence . To address this challenge , we introduce a novel general end - to - end graph - to - sequence neural encoder - decoder model that maps an input graph to a sequence of vectors and uses an attention - based LSTM method to decode the target sequence from these vectors . Our method first generates the node and graph embeddings using an improved graph - based neural network with a novel aggregation strategy to incorporate edge direction information in the node embeddings .",5,"['Task', 'Method', 'Material', 'Metric']","document : Graph2Seq : Graph to Sequence Learning with Attention - Based Neural Networks The celebrated Sequence to Sequence learning ( Seq2Seq ) technique and its numerous variants achieve excellent performance on many tasks . However , many machine learning tasks have inputs naturally represented as graphs ; existing Seq2Seq models face a significant challenge in achieving accurate conversion from graph form to the appropriate sequence . To address this challenge , we introduce a novel general end - to - end graph - to - sequence neural encoder - decoder model that maps an input graph to a sequence of vectors and uses an attention - based LSTM method to decode the target sequence from these vectors . Our method first generates the node and graph embeddings using an improved graph - based neural network with a novel aggregation strategy to incorporate edge direction information in the node embeddings .","{'Task': ['conversion', 'machine learning tasks', 'Graph to Sequence Learning'], 'Method': ['LSTM', 'node and graph embeddings', 'graph - based neural network', 'Seq2Seq', 'end - to - end graph - to - sequence neural encoder - decoder model', 'aggregation strategy', 'Sequence to Sequence learning', 'Attention - Based Neural Networks', 'Graph2Seq'], 'Material': [], 'Metric': []}","{'Task': ['Graph-to-Sequence learning', 'achieving accurate conversion from graph form to the appropriate sequence'],
 'Method': ['Seq2Seq technique', 'numerous variants', 'introduce a novel general end-to-end graph-to-sequence neural encoder-decoder model', 'maps an input graph to a sequence of vectors', 'uses an attention-based LSTM method to decode the target sequence from these vectors', 'improved graph-based neural network', 'novel aggregation strategy'],
 'Material': ['machine learning tasks', 'graph form'],
 'Metric': ['excellent performance']}","task: Graph-to-Sequence Learning
method:
- Sequence to Sequence learning
- End-to-end graph-to-sequence neural encoder-decoder model
- Attention-based LSTM method
- Improved graph-based neural network with a novel aggregation strategy
material: N/A
metric: N/A

# The document does not provide specific information about metrics or materials used in the method."
21a1654b856cf0c64e60e58258669b374cb05539,"document : You Only Look Once : Unified , Real - Time Object Detection We present YOLO , a new approach to object detection . Prior work on object detection repurposes classifiers to perform detection . Instead , we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities . A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation .",5,"['Task', 'Method', 'Material', 'Metric']","document : You Only Look Once : Unified , Real - Time Object Detection We present YOLO , a new approach to object detection . Prior work on object detection repurposes classifiers to perform detection . Instead , we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities . A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation .","{'Task': ['Real - Time Object Detection', 'regression problem', 'detection', 'object detection'], 'Method': ['neural network', 'classifiers', 'YOLO'], 'Material': [], 'Metric': []}","{'Task': ['object detection'], 'Method': ['framing object detection as a regression problem', 'predicting bounding boxes and class probabilities directly from full images in one evaluation'], 'Material': [], 'Metric': ['spatially separated bounding boxes', 'associated class probabilities']}","task: Object Detection
method: Framing object detection as a regression problem, Predicting bounding boxes and class probabilities directly from full images using a single neural network
material: Neural network, Full images
metric: Spatially separated bounding boxes, Associated class probabilities"
232b43584b2236669c0a53702ad89ab10c3886ea,"In this work , we build on recent advances in distributional reinforcement learning to give a generally applicable , flexible , and state - of - the - art distributional variant of DQN . We achieve this by using quantile regression to approximate the full quantile function for the state - action return distribution . By reparameterizing a distribution over the sample space , this yields an implicitly defined return distribution and gives rise to a large class of risk - sensitive policies . We demonstrate improved performance on the 57 Atari 2600 games in the ALE , and use our algorithm ’s implicitly defined distributions to study the effects of risk - sensitive policies in Atari games . ImplicitQuantileNetworksforDistributionalReinforcementLearning",5,"['Task', 'Method', 'Material', 'Metric']","In this work , we build on recent advances in distributional reinforcement learning to give a generally applicable , flexible , and state - of - the - art distributional variant of DQN . We achieve this by using quantile regression to approximate the full quantile function for the state - action return distribution . By reparameterizing a distribution over the sample space , this yields an implicitly defined return distribution and gives rise to a large class of risk - sensitive policies . We demonstrate improved performance on the 57 Atari 2600 games in the ALE , and use our algorithm ’s implicitly defined distributions to study the effects of risk - sensitive policies in Atari games . ImplicitQuantileNetworksforDistributionalReinforcementLearning","{'Task': ['risk - sensitive policies', 'Atari games'], 'Method': ['risk - sensitive policies', 'distributional variant', 'reinforcement learning', 'quantile regression', 'DQN'], 'Material': ['Atari 2600 games'], 'Metric': []}","{'Task': ['Atari 2600 games in the ALE'], 'Method': ['distributional reinforce learning', 'quantile regression', 'ImplicitQuantileNetworks'], 'Material': [], 'Metric': ['state-action return distribution', 'risk sensitivity']}","task: Distributional variant of DQN for Atari 2600 games in the ALE
method: [1] Using quantile regression to approximate the full quantile function for the state-action return distribution, [2] Reparameterizing a distribution over the sample space to yield an implicitly defined return distribution, [3] Studying the effects of risk-sensitive policies in Atari games
material: Recent advances in distributional reinforcement learning
metric: Improved performance on the 57 Atari 2600 games in the ALE, effects of risk-sensitive policies in Atari games"
23d2d3a6ffebfecaa8930307fdcf451c147757c8,"document : SeqGAN : Sequence Generative Adversarial Nets with Policy Gradient As a new way of training generative models , Generative Adversarial Net ( GAN ) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real - valued data . However , it has limitations when the goal is for generating sequences of discrete tokens . A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model . Also , the discriminative model can only assess a complete sequence , while for a partially generated sequence , it is non - trivial to balance its current score and the future one once the entire sequence has been generated .",5,"['Task', 'Method', 'Material', 'Metric']","document : SeqGAN : Sequence Generative Adversarial Nets with Policy Gradient As a new way of training generative models , Generative Adversarial Net ( GAN ) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real - valued data . However , it has limitations when the goal is for generating sequences of discrete tokens . A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model . Also , the discriminative model can only assess a complete sequence , while for a partially generated sequence , it is non - trivial to balance its current score and the future one once the entire sequence has been generated .","{'Task': ['generating sequences of discrete tokens'], 'Method': ['generative models', 'discriminative model', 'Sequence Generative Adversarial Nets', 'generative model', 'Policy Gradient', 'Generative Adversarial Net', 'GAN', 'SeqGAN'], 'Material': [], 'Metric': []}","{'Task': ['generating sequences of discrete tokens', 'training generative models'], 'Method': ['Sequential Generative Adversarial Nets (SeqGAN)', 'Policy Gradient', 'Generative Adversarial Net (GAN)'], 'Material': ['discrete outputs from the generative model', 'complete sequence', 'partially generated sequence'], 'Metric': ['gradient update', 'current score', 'future score']}","task: Sequence generation with Generative Adversarial Nets (GAN)
method:
- Sequential Generative Adversarial Nets (SeqGAN)
- Policy Gradient
material:
- Generative model
- Discriminative model
metric:
- Sequence score
- Gradient update
- Balance of current and future sequence scores"
24730424236724d3f798dec02901e7a1f1c4710e,"Joint Maximum Purity Forest with Application to Image Super - Resolution section : Abstract - In this paper , we propose a novel random - forest scheme , namely Joint Maximum Purity Forest ( JMPF ) , for classification , clustering , and regression tasks . In the JMPF scheme , the original feature space is transformed into a compactly pre - clustered feature space , via a trained rotation matrix . The rotation matrix is obtained through an iterative quantization process , where the input data belonging to different classes are clustered to the respective vertices of the new feature space with maximum purity .",5,"['Task', 'Method', 'Material', 'Metric']","Joint Maximum Purity Forest with Application to Image Super - Resolution section : Abstract - In this paper , we propose a novel random - forest scheme , namely Joint Maximum Purity Forest ( JMPF ) , for classification , clustering , and regression tasks . In the JMPF scheme , the original feature space is transformed into a compactly pre - clustered feature space , via a trained rotation matrix . The rotation matrix is obtained through an iterative quantization process , where the input data belonging to different classes are clustered to the respective vertices of the new feature space with maximum purity .","{'Task': ['Image Super - Resolution', 'classification , clustering , and regression tasks'], 'Method': ['Joint Maximum Purity Forest', 'iterative quantization process', 'random - forest scheme', 'JMPF'], 'Material': [], 'Metric': []}","{'Task': ['classification', 'clustering', 'regression'], 'Method': ['Joint Maximum Purity Forest (JMPF)', 'iterative quantization process', 'trained rotation matrix'], 'Material': [], 'Metric': ['purity']}","task: [""classification"", ""clustering"", ""regression""]
method: [""Joint Maximum Purity Forest (JMPF)""]
material: [""original feature space"", ""compactly pre-clustered feature space"", ""trained rotation matrix""]
metric: [""purity""]"
249b3b7421d3cdb932eecfe4b67203e0e46806b2,"document : Cell - aware Stacked LSTMs for Modeling Sentences We propose a method of stacking multiple long short - term memory ( LSTM ) layers for modeling sentences . In contrast to the conventional stacked LSTMs where only hidden states are fed as input to the next layer , our architecture accepts both hidden and memory cell states of the preceding layer and fuses information from the left and the lower context using the soft gating mechanism of LSTMs . Thus the proposed stacked LSTM architecture modulates the amount of information to be delivered not only in horizontal recurrence but also in vertical connections , from which useful features extracted from lower layers are effectively conveyed to upper layers . We dub this architecture Cell - aware Stacked LSTM ( CAS - LSTM ) and show from experiments that our models achieve state - of - the - art results on benchmark datasets for natural language inference , paraphrase detection , and sentiment classification .",5,"['Task', 'Method', 'Material', 'Metric']","document : Cell - aware Stacked LSTMs for Modeling Sentences We propose a method of stacking multiple long short - term memory ( LSTM ) layers for modeling sentences . In contrast to the conventional stacked LSTMs where only hidden states are fed as input to the next layer , our architecture accepts both hidden and memory cell states of the preceding layer and fuses information from the left and the lower context using the soft gating mechanism of LSTMs . Thus the proposed stacked LSTM architecture modulates the amount of information to be delivered not only in horizontal recurrence but also in vertical connections , from which useful features extracted from lower layers are effectively conveyed to upper layers . We dub this architecture Cell - aware Stacked LSTM ( CAS - LSTM ) and show from experiments that our models achieve state - of - the - art results on benchmark datasets for natural language inference , paraphrase detection , and sentiment classification .","{'Task': ['natural language inference', 'paraphrase detection', 'modeling sentences', 'sentiment classification', 'Modeling Sentences'], 'Method': ['LSTMs', 'Cell - aware Stacked LSTM', 'LSTM', 'CAS - LSTM', 'multiple long short - term memory', 'soft gating mechanism'], 'Material': [], 'Metric': []}","{'Task': ['natural language inference', 'paraphrase detection', 'sentiment classification'],
 'Method': ['stacking multiple long short - term memory (LSTM) layers', 'proposed stacked LSTM architecture', 'Cell - aware Stacked LSTM (CAS - LSTM)'],
 'Material': ['hidden states', 'memory cell states', 'preceding layer', 'soft gating mechanism of LSTMs'],
 'Metric': ['state - of - the - art results']}","task:
- natural language inference
- paraphrase detection
- sentiment classification

method:
- Stacked LSTMs
- Cell-aware Stacked LSTMs (CAS-LSTM)

material:
- long short-term memory (LSTM) layers
- hidden states
- memory cell states

metric:
- state-of-the-art results"
25a784f7f8c94c42821ee078587fc38dffcd00a4,"document : Robust Face Detection via Learning Small Faces on Hard Images Recent anchor - based deep face detectors have achieved promising performance , but they are still struggling to detect hard faces , such as small , blurred and partially occluded faces . A reason is that they treat all images and faces equally , without putting more effort on hard ones ; however , many training images only contain easy faces , which are less helpful to achieve better performance on hard images . In this paper , we propose that the robustness of a face detector against hard faces can be improved by learning small faces on hard images . Our intuitions are ( 1 ) hard images are the images which contain at least one hard face , thus they facilitate training robust face detectors ; ( 2 ) most hard faces are small faces and other types of hard faces can be easily converted to small faces by shrinking .",5,"['Task', 'Method', 'Material', 'Metric']","document : Robust Face Detection via Learning Small Faces on Hard Images Recent anchor - based deep face detectors have achieved promising performance , but they are still struggling to detect hard faces , such as small , blurred and partially occluded faces . A reason is that they treat all images and faces equally , without putting more effort on hard ones ; however , many training images only contain easy faces , which are less helpful to achieve better performance on hard images . In this paper , we propose that the robustness of a face detector against hard faces can be improved by learning small faces on hard images . Our intuitions are ( 1 ) hard images are the images which contain at least one hard face , thus they facilitate training robust face detectors ; ( 2 ) most hard faces are small faces and other types of hard faces can be easily converted to small faces by shrinking .","{'Task': ['Learning Small Faces', 'Robust Face Detection'], 'Method': ['face detectors', 'anchor - based deep face detectors', 'face detector'], 'Material': ['Hard Images', 'hard images'], 'Metric': ['robustness']}","{'Task': ['improving the robustness of a face detector against hard faces', 'learning small faces on hard images'],
 'Method': ['using anchor-based deep face detectors', 'putting more effort on hard images', 'shrinking other types of hard faces to make them small'],
 'Material': ['hard images', 'training images'],
 'Metric': ['performance']}","Task:
- Face Detection
- Robust Face Detection
- Hard Face Detection

Method:
- Anchor-based deep face detection
- Learning small faces on hard images

Material:
- Hard images
- Training images
- Easy faces
- Small faces

Metric:
- Performance
- Robustness against hard faces
- Hardness of images
- Size of faces"
25f5df29342a04936ba0d308b4d1b8245a7e8f5c,Convolutional Pose Machines section : Abstract Pose Machines provide a sequential prediction framework for learning rich implicit spatial models . In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image - dependent spatial models for the task of pose estimation . The contribution of this paper is to implicitly model long - range dependencies between variables in structured prediction tasks such as articulated pose estimation .,5,"['Task', 'Method', 'Material', 'Metric']",Convolutional Pose Machines section : Abstract Pose Machines provide a sequential prediction framework for learning rich implicit spatial models . In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image - dependent spatial models for the task of pose estimation . The contribution of this paper is to implicitly model long - range dependencies between variables in structured prediction tasks such as articulated pose estimation .,"{'Task': ['structured prediction tasks', 'long - range dependencies between variables', 'learning image features', 'pose estimation'], 'Method': ['Convolutional Pose Machines', 'convolutional networks', 'Pose Machines', 'sequential prediction framework', 'image - dependent spatial models', 'pose machine', 'implicit spatial models'], 'Material': [], 'Metric': []}","{'Task': ['pose estimation'], 'Method': ['convolutional networks', 'pose machine framework', 'structured prediction tasks', 'articulated pose estimation'], 'Material': ['image features', 'image-dependent spatial models'], 'Metric': ['implicitly modeling long-range dependencies']}","task: articulated pose estimation
method: [Convolutional Pose Machines, Sequential prediction framework, Learning rich implicit spatial models, Incorporating convolutional networks into pose machine framework, Implicitly modeling long-range dependencies in structured prediction tasks]
material: [Convolutional networks]
metric: [Image features, Image-dependent spatial models]"
269730dbbabed8b8b5ba720e44a4c31b1f51e8f1,"Published as a conference paper at ICLR 2017 QUERY - REDUCTION NETWORKS FOR QUESTION ANSWERING section : ABSTRACT In this paper , we study the problem of question answering when reasoning over multiple facts is required . We propose Query - Reduction Network ( QRN ) , a variant of Recurrent Neural Network ( RNN ) that effectively handles both short - term ( local ) and long - term ( global ) sequential dependencies to reason over multiple facts . QRN considers the context sentences as a sequence of state - changing triggers , and reduces the original query to a more informed query as it observes each trigger ( context sentence ) through time .",5,"['Task', 'Method', 'Material', 'Metric']","Published as a conference paper at ICLR 2017 QUERY - REDUCTION NETWORKS FOR QUESTION ANSWERING section : ABSTRACT In this paper , we study the problem of question answering when reasoning over multiple facts is required . We propose Query - Reduction Network ( QRN ) , a variant of Recurrent Neural Network ( RNN ) that effectively handles both short - term ( local ) and long - term ( global ) sequential dependencies to reason over multiple facts . QRN considers the context sentences as a sequence of state - changing triggers , and reduces the original query to a more informed query as it observes each trigger ( context sentence ) through time .","{'Task': ['QUESTION ANSWERING', 'question answering'], 'Method': ['QUERY - REDUCTION NETWORKS', 'QRN', 'Query - Reduction Network', 'Recurrent Neural Network', 'RNN'], 'Material': [], 'Metric': []}","{'Task': ['question answering'], 'Method': ['Query-Reduction Network (QRN)', 'Recurrent Neural Network (RNN)'], 'Material': ['facts'], 'Metric': ['short-term (local) sequential dependencies', 'long-term (global) sequential dependencies']}","task: Question Answering
method: Query-Reduction Network (QRN)
material: Recurrent Neural Network (RNN)
metric: Short-term (local) sequential dependencies, Long-term (global) sequential dependencies"
270e65acc071b9e4e2a632720130c0e10cb6fa08,"document : Neural Tree Indexers for Text Understanding Recurrent neural networks ( RNNs ) process input text sequentially and model the conditional transition between word tokens . In contrast , the advantages of recursive networks include that they explicitly model the compositionality and the recursive structure of natural language . However , the current recursive architecture is limited by its dependence on syntactic tree . In this paper , we introduce a robust syntactic parsing - independent tree structured model , Neural Tree Indexers ( NTI ) that provides a middle ground between the sequential RNNs and the syntactic tree - based recursive models .",5,"['Task', 'Method', 'Material', 'Metric']","document : Neural Tree Indexers for Text Understanding Recurrent neural networks ( RNNs ) process input text sequentially and model the conditional transition between word tokens . In contrast , the advantages of recursive networks include that they explicitly model the compositionality and the recursive structure of natural language . However , the current recursive architecture is limited by its dependence on syntactic tree . In this paper , we introduce a robust syntactic parsing - independent tree structured model , Neural Tree Indexers ( NTI ) that provides a middle ground between the sequential RNNs and the syntactic tree - based recursive models .","{'Task': ['Text Understanding'], 'Method': ['tree', 'Recurrent neural networks', 'Neural Tree Indexers', 'RNNs', 'NTI', 'recursive networks', 'recursive architecture'], 'Material': [], 'Metric': []}","{'Task': ['text understanding'], 'Method': ['processing input text', 'modeling conditional transition between word tokens', 'explicitly modeling compositionality and recursive structure of natural language', 'introducing Neural Tree Indexers (NTI)'], 'Material': ['recurrent neural networks (RNNs)', 'syntactic tree'], 'Metric': ['Ellipsis']}","task: Text Understanding, Parsing
method: [Recurrent neural networks (RNNs), Recursive networks, Neural Tree Indexers (NTI)]
material: [Input text, Word tokens, Syntactic tree]
metric: [Conditional transition, Compositionality, Recursive structure]"
2777cd26b2c257843273fe41ad4c5b8cdf1b1b75,"document : Understanding Humans in Crowded Scenes : Deep Nested Adversarial Learning and A New Benchmark for Multi - Human Parsing Despite the noticeable progress in perceptual tasks like detection , instance segmentation and human parsing , computers still perform unsatisfactorily on visually understanding humans in crowded scenes , such as group behavior analysis , person re - identification and autonomous driving , etc . To this end , models need to comprehensively perceive the semantic information and the differences between instances in a multi - human image , which is recently defined as the multi - human parsing task . In this paper , we present a new large - scale database “ M ulti - H uman",5,"['Task', 'Method', 'Material', 'Metric']","document : Understanding Humans in Crowded Scenes : Deep Nested Adversarial Learning and A New Benchmark for Multi - Human Parsing Despite the noticeable progress in perceptual tasks like detection , instance segmentation and human parsing , computers still perform unsatisfactorily on visually understanding humans in crowded scenes , such as group behavior analysis , person re - identification and autonomous driving , etc . To this end , models need to comprehensively perceive the semantic information and the differences between instances in a multi - human image , which is recently defined as the multi - human parsing task . In this paper , we present a new large - scale database “ M ulti - H uman","{'Task': ['autonomous driving', 'detection', 'instance segmentation', 'group behavior analysis', 'Understanding Humans in Crowded Scenes', 'human parsing', 'visually understanding humans in crowded scenes', 'perceptual tasks', 'person re - identification', 'Multi - Human Parsing', 'M ulti -'], 'Method': ['Deep Nested Adversarial Learning'], 'Material': [], 'Metric': []}","{'Task': ['multi-human parsing', 'group behavior analysis', 'person re-identification', 'autonomous driving'],
 'Method': ['Deep Nested Adversarial Learning'],
 'Material': ['multi-human image'],
 'Metric': ['semantic information', 'differences between instances']}","```yaml
Tasks:
- multi-human-parsing
- group-behavior-analysis
- person-re-identification
- autonomous-driving

Methods:
- Deep Nested Adversarial Learning

Materials:
- multi-human image

Metrics:
- semantic information perception
- instance differences perception
```"
27a99c21a1324f087b2f144adc119f04137dfd87,"document : Deep Fried Convnets The fully - connected layers of deep convolutional neural networks typically contain over 90 % of the network parameters . Reducing the number of parameters while preserving predictive performance is critically important for training big models in distributed systems and for deployment in embedded devices . In this paper , we introduce a novel Adaptive Fastfood transform to reparameterize the matrix - vector multiplication of fully connected layers . Reparameterizing a fully connected layer with inputs and outputs with the Adaptive Fastfood transform reduces the storage and computational costs costs from to and respectively .",5,"['Task', 'Method', 'Material', 'Metric']","document : Deep Fried Convnets The fully - connected layers of deep convolutional neural networks typically contain over 90 % of the network parameters . Reducing the number of parameters while preserving predictive performance is critically important for training big models in distributed systems and for deployment in embedded devices . In this paper , we introduce a novel Adaptive Fastfood transform to reparameterize the matrix - vector multiplication of fully connected layers . Reparameterizing a fully connected layer with inputs and outputs with the Adaptive Fastfood transform reduces the storage and computational costs costs from to and respectively .","{'Task': ['distributed systems', 'embedded devices'], 'Method': ['Deep Fried Convnets', 'matrix - vector multiplication of fully connected layers', 'big models', 'fully - connected layers of deep convolutional neural networks', 'Adaptive Fastfood transform', 'fully connected layer'], 'Material': [], 'Metric': ['predictive performance', 'storage and computational costs costs']}","{'Task': ['training big models', 'deployment'],
 'Method': ['Adaptive Fastfood transform', 'reparameterizing a fully connected layer'],
 'Material': ['big models', 'distributed systems', 'embedded devices'],
 'Metric': ['predictive performance', 'storage costs', 'computational costs']}","task: Reducing parameters in deep learning models
method: Introducing Adaptive Fastfood transform for reparameterizing fully connected layers
material: Fully connected layers in deep convolutional neural networks
metric: Storage costs, computational costs, predictive performance"
27aa0f3ec934925265f93fac7ff1cd1d70ceb618,"Strong Baselines for Neural Semi - supervised Learning under Domain Shift section : Abstract Novel neural models have been proposed in recent years for learning under domain shift . Most models , however , only evaluate on a single task , on proprietary datasets , or compare to weak baselines , which makes comparison of models difficult . In this paper , we re - evaluate classic general - purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi - task tri - training method that reduces the time and space complexity of classic tri - training .",5,"['Task', 'Method', 'Material', 'Metric']","Strong Baselines for Neural Semi - supervised Learning under Domain Shift section : Abstract Novel neural models have been proposed in recent years for learning under domain shift . Most models , however , only evaluate on a single task , on proprietary datasets , or compare to weak baselines , which makes comparison of models difficult . In this paper , we re - evaluate classic general - purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi - task tri - training method that reduces the time and space complexity of classic tri - training .","{'Task': ['Neural Semi - supervised Learning', 'learning under domain shift', 'Domain Shift'], 'Method': ['tri - training', 'neural models', 'general - purpose bootstrapping approaches', 'neural networks', 'multi - task tri - training method', 'neural approaches'], 'Material': [], 'Metric': ['time and space complexity']}","{'Task': ['learning under domain shift'], 'Method': ['classic general-purpose bootstrapping approaches', 'recent neural approaches', 'tri-training', 'multi-task tri-training'], 'Material': ['neural networks'], 'Metric': ['time complexity', 'space complexity']}","task:
- Neural models for learning under domain shift
- Comparison of models
- Evaluation of classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts
- Proposal and implementation of a novel multi-task tri-training method

method:
- Classic general-purpose bootstrapping approaches
- Tri-training
- Multi-task tri-training

material:
- Neural networks
- Domain shifts

metric:
- Time complexity
- Space complexity"
2a86bcdfb1d817ddb76ba202319f8267a36c0f62,"document : PCL : Proposal Cluster Learning for Weakly Supervised Object Detection Weakly Supervised Object Detection ( WSOD ) , using only image - level annotations to train object detectors , is of growing importance in object recognition . In this paper , we propose a novel deep network for WSOD . Unlike previous networks that transfer the object detection problem to an image classification problem using Multiple Instance Learning ( MIL ) , our strategy generates proposal clusters to learn refined instance classifiers by an iterative process . The proposals in the same cluster are spatially adjacent and associated with the same object .",5,"['Task', 'Method', 'Material', 'Metric']","document : PCL : Proposal Cluster Learning for Weakly Supervised Object Detection Weakly Supervised Object Detection ( WSOD ) , using only image - level annotations to train object detectors , is of growing importance in object recognition . In this paper , we propose a novel deep network for WSOD . Unlike previous networks that transfer the object detection problem to an image classification problem using Multiple Instance Learning ( MIL ) , our strategy generates proposal clusters to learn refined instance classifiers by an iterative process . The proposals in the same cluster are spatially adjacent and associated with the same object .","{'Task': ['WSOD )', 'image classification problem', 'Weakly Supervised Object Detection', 'object recognition', 'object detection', 'WSOD'], 'Method': ['Proposal Cluster Learning', 'PCL', 'MIL', 'object detectors', 'instance classifiers', 'proposal clusters', 'iterative process', 'deep network', 'Multiple Instance Learning'], 'Material': [], 'Metric': []}","{'Task': ['Weakly Supervised Object Detection'], 'Method': ['Proposal Cluster Learning', 'Iterative process'], 'Material': [], 'Metric': []}","task: Weakly Supervised Object Detection
method:
- Proposal Cluster Learning
- Iterative process for generating proposal clusters
- Learning refined instance classifiers
material: Image-level annotations
metric: N/A (The document does not mention any specific metric used in the proposed method.)"
2f04ba0f74df046b0080ca78e56898bd4847898b,"document : Aggregate Channel Features for Multi - view Face Detection Face detection has drawn much attention in recent decades since the seminal work by Viola and Jones . While many subsequences have improved the work with more powerful learning algorithms , the feature representation used for face detection still ca n’t meet the demand for effectively and efficiently handling faces with large appearance variance in the wild . To solve this bottleneck , we borrow the concept of channel features to the face detection domain , which extends the image channel to diverse types like gradient magnitude and oriented gradient histograms and therefore encodes rich information in a simple form . We adopt a novel variant called aggregate channel features , make a full exploration of feature design , and discover a multi - scale version of features with better performance .",5,"['Task', 'Method', 'Material', 'Metric']","document : Aggregate Channel Features for Multi - view Face Detection Face detection has drawn much attention in recent decades since the seminal work by Viola and Jones . While many subsequences have improved the work with more powerful learning algorithms , the feature representation used for face detection still ca n’t meet the demand for effectively and efficiently handling faces with large appearance variance in the wild . To solve this bottleneck , we borrow the concept of channel features to the face detection domain , which extends the image channel to diverse types like gradient magnitude and oriented gradient histograms and therefore encodes rich information in a simple form . We adopt a novel variant called aggregate channel features , make a full exploration of feature design , and discover a multi - scale version of features with better performance .","{'Task': ['Multi - view Face Detection', 'detection'], 'Method': ['learning algorithms', 'aggregate channel features', 'feature design', 'feature representation', 'multi - scale version of features', 'Aggregate Channel Features'], 'Material': [], 'Metric': []}","{'Task': ['face detection'], 'Method': ['aggregate channel features', 'borrowing the concept of channel features', 'making a full exploration of feature design'], 'Material': ['image channel', 'gradient magnitude', 'oriented gradient histograms'], 'Metric': ['performance']}","task: Aggregate Channel Features for Multi-view Face Detection
method: [Borrowing concept of channel features, Adopting a novel variant called aggregate channel features, Making a full exploration of feature design, Discovering a multi-scale version of features]
material: [Image channel, Gradient magnitude, Oriented gradient histograms]
metric: [Performance]"
2f56b1ac5b9faac9527b6814778925e9242cf5fd,"document : Training Region - based Object Detectors with Online Hard Example Mining The field of object detection has made significant advances riding on the wave of region - based ConvNets , but their training procedure still includes many heuristics and hyperparameters that are costly to tune . We present a simple yet surprisingly effective online hard example mining ( OHEM ) algorithm for training region - based ConvNet detectors . Our motivation is the same as it has always been – detection datasets contain an overwhelming number of easy examples and a small number of hard examples .",5,"['Task', 'Method', 'Material', 'Metric']","document : Training Region - based Object Detectors with Online Hard Example Mining The field of object detection has made significant advances riding on the wave of region - based ConvNets , but their training procedure still includes many heuristics and hyperparameters that are costly to tune . We present a simple yet surprisingly effective online hard example mining ( OHEM ) algorithm for training region - based ConvNet detectors . Our motivation is the same as it has always been – detection datasets contain an overwhelming number of easy examples and a small number of hard examples .","{'Task': ['object detection'], 'Method': ['Online Hard Example Mining', 'region - based ConvNet detectors', 'online hard example mining', 'region - based ConvNets', 'Region - based Object Detectors', 'OHEM'], 'Material': [], 'Metric': []}","{'Task': ['training', 'object detection'], 'Method': ['online hard example mining (OHEM)'], 'Material': ['detection datasets'], 'Metric': ['easy examples', 'hard examples']}","task: Object Detection Training
method:
- Online Hard Example Mining (OHEM) algorithm
material:
- Detection datasets
metric:
- Number of easy examples
- Number of hard examples"
2f97ee95cad6a1f13596b108072b846c6f747d4e,"document : The Microsoft 2016 Conversational Speech Recognition System We describe Microsoft ’s conversational speech recognition system , in which we combine recent developments in neural - network - based acoustic and language modeling to advance the state of the art on the Switchboard recognition task . Inspired by machine learning ensemble techniques , the system uses a range of convolutional and recurrent neural networks . I - vector modeling and lattice - free MMI training provide significant gains for all acoustic model architectures . Language model rescoring with multiple forward and backward running RNNLMs , and word posterior - based system combination provide a 20 % boost .",5,"['Task', 'Method', 'Material', 'Metric']","document : The Microsoft 2016 Conversational Speech Recognition System We describe Microsoft ’s conversational speech recognition system , in which we combine recent developments in neural - network - based acoustic and language modeling to advance the state of the art on the Switchboard recognition task . Inspired by machine learning ensemble techniques , the system uses a range of convolutional and recurrent neural networks . I - vector modeling and lattice - free MMI training provide significant gains for all acoustic model architectures . Language model rescoring with multiple forward and backward running RNNLMs , and word posterior - based system combination provide a 20 % boost .","{'Task': [], 'Method': ['machine learning ensemble techniques', 'acoustic model architectures', 'Language model rescoring', 'lattice - free MMI training', 'neural - network - based acoustic and language modeling', 'Conversational Speech Recognition System', 'convolutional and recurrent neural networks', 'forward and backward running RNNLMs', 'I - vector modeling', 'Microsoft ’s conversational speech recognition system', 'word posterior - based system combination'], 'Material': ['Switchboard'], 'Metric': []}","{'Task': ['Switchboard recognition task'], 'Method': ['neural-network-based acoustic and language modeling', 'machine learning ensemble techniques', 'I-vector modeling', 'lattice-free MMI training', 'language model rescoring', 'multiple forward and backward running RNNLMs', 'word posterior-based system combination'], 'Material': ['Microsoft ’s conversational speech recognition system'], 'Metric': ['significant gains', '20 % boost']}","task: Switchboard recognition task
methods:
- neural-network-based acoustic modeling
- neural-network-based language modeling
- machine learning ensemble techniques
- I-vector modeling
- lattice-free MMI training
- language model rescoring
- multiple forward and backward running RNNLMs
- word posterior-based system combination
materials: []
metrics:
- gains for all acoustic model architectures
- 20 % boost"
322a7dad274f440a92548faa8f2b2be666b2d01f,"document : Pyramid Scene Parsing Network Scene parsing is challenging for unrestricted open vocabulary and diverse scenes . In this paper , we exploit the capability of global context information by different - region - based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network ( PSPNet ) . Our global prior representation is effective to produce good quality results on the scene parsing task , while PSPNet provides a superior framework for pixel - level prediction . The proposed approach achieves state - of - the - art performance on various datasets .",5,"['Task', 'Method', 'Material', 'Metric']","document : Pyramid Scene Parsing Network Scene parsing is challenging for unrestricted open vocabulary and diverse scenes . In this paper , we exploit the capability of global context information by different - region - based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network ( PSPNet ) . Our global prior representation is effective to produce good quality results on the scene parsing task , while PSPNet provides a superior framework for pixel - level prediction . The proposed approach achieves state - of - the - art performance on various datasets .","{'Task': ['pixel - level prediction', 'Scene parsing', 'scene parsing task'], 'Method': ['Pyramid Scene Parsing Network', 'global prior representation', 'region - based context aggregation', 'pyramid scene parsing network', 'PSPNet', 'pyramid pooling module'], 'Material': [], 'Metric': []}","{'Task': ['scene parsing'], 'Method': ['pyramid pooling module', 'proposed pyramid scene parsing network (PSPNet)'], 'Material': [], 'Metric': ['state-of-the-art performance']}","task: [""scene parsing""]
method: [""pyramid pooling module"", ""proposed pyramid scene parsing network (PSPNet)""]
material: []
metric: [""state-of-the-art performance""]"
325af39d281d5903a269c01fab8f53d7400a4c49,"document : ArtTrack : Articulated Multi - person Tracking in the Wild In this paper we propose an approach for articulated tracking of multiple people in unconstrained videos . Our starting point is a model that resembles existing architectures for single - frame pose estimation but is substantially faster . We achieve this in two ways : ( 1 ) by simplifying and sparsifying the body - part relationship graph and leveraging recent methods for faster inference , and ( 2 ) by offloading a substantial share of computation onto a feed - forward convolutional architecture that is able to detect and associate body joints of the same person even in clutter . We use this model to generate proposals for body joint locations and formulate articulated tracking as spatio - temporal grouping of such proposals .",5,"['Task', 'Method', 'Material', 'Metric']","document : ArtTrack : Articulated Multi - person Tracking in the Wild In this paper we propose an approach for articulated tracking of multiple people in unconstrained videos . Our starting point is a model that resembles existing architectures for single - frame pose estimation but is substantially faster . We achieve this in two ways : ( 1 ) by simplifying and sparsifying the body - part relationship graph and leveraging recent methods for faster inference , and ( 2 ) by offloading a substantial share of computation onto a feed - forward convolutional architecture that is able to detect and associate body joints of the same person even in clutter . We use this model to generate proposals for body joint locations and formulate articulated tracking as spatio - temporal grouping of such proposals .","{'Task': ['single - frame pose estimation', 'faster inference', 'articulated tracking', 'spatio - temporal grouping', 'articulated tracking of multiple people in unconstrained videos'], 'Method': ['body - part relationship graph', 'ArtTrack', 'feed - forward convolutional architecture', 'Articulated Multi - person Tracking'], 'Material': [], 'Metric': []}","{'Task': ['articulated tracking of multiple people', 'generating proposals for body joint locations', 'spatio-temporal grouping'],
 'Method': ['model for single-frame pose estimation', 'simplifying and sparsifying the body-part relationship graph', 'faster inference', 'offloading computation onto a feed-forward convolutional architecture', 'detecting and associating body joints'],
 'Material': [],
 'Metric': []}","```yaml
Task:
- Articulated tracking of multiple people
- Spatio-temporal grouping of body joint proposals

Method:
- Simplifying and sparsifying body-part relationship graph
- Faster inference using recent methods
- Offloading computation onto a feed-forward convolutional architecture

Material:
- Existing architectures for single-frame pose estimation
- Body joint locations

Metric:
- Speed
```"
33261d252218007147a71e40f8367ed152fa2fe0,Question Answering with Subgraph Embeddings section : Abstract . This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few handcrafted features . Our model learns low - dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .,5,"['Task', 'Method', 'Material', 'Metric']",Question Answering with Subgraph Embeddings section : Abstract . This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few handcrafted features . Our model learns low - dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .,"{'Task': ['Question Answering'], 'Method': ['Subgraph Embeddings'], 'Material': [], 'Metric': []}","{'Task': ['Question Answering'], 'Method': ['Learning low-dimensional embeddings of words and knowledge base constituents', 'Scoring natural language questions against candidate answers'], 'Material': ['Knowledge base'], 'Metric': ['Ellipsis']}","```yaml
Task: Question Answering
Method:
- Learning low-dimensional embeddings of words
- Learning low-dimensional embeddings of knowledge base constituents
- Scoring natural language questions against candidate answers
Material: Knowledge base
Metric: Not explicitly mentioned in the given text
```"
3448e6a5039417dc1ae890efeca3bef5390ace7c,"document : [ Combinatorial features are essential for the success of many commercial models . Manually crafting these features usually comes with high cost due to the variety , volume and velocity of raw data in web - scale systems . Factorization based models , which measure interactions in terms of vector product , can learn patterns of combinatorial features automatically and generalize to unseen features as well . With the great success of deep neural networks ( DNNs ) in various fields , recently researchers have proposed several DNN - based factorization model to learn both low - and high - order feature interactions .",5,"['Task', 'Method', 'Material', 'Metric']","document : [ Combinatorial features are essential for the success of many commercial models . Manually crafting these features usually comes with high cost due to the variety , volume and velocity of raw data in web - scale systems . Factorization based models , which measure interactions in terms of vector product , can learn patterns of combinatorial features automatically and generalize to unseen features as well . With the great success of deep neural networks ( DNNs ) in various fields , recently researchers have proposed several DNN - based factorization model to learn both low - and high - order feature interactions .","{'Task': ['web - scale systems'], 'Method': ['deep neural networks', 'Factorization based models', 'DNNs', 'DNN'], 'Material': [], 'Metric': ['cost']}","{'Task': ['success of many commercial models'], 'Method': ['manually crafting features', 'factorization based models', 'learning both low- and high-order feature interactions', 'proposing DNN-based factorization models'], 'Material': ['raw data in web-scale systems'], 'Metric': ['cost', 'patterns of combinatorial features', 'generalize to unseen features']}","task: Success of commercial models, Learning patterns of combinatorial features, Generalizing to unseen features
method: Manually crafting features, Factorization based models, Vector product, DNN-based factorization models
material: Combinatorial features, Raw data, Web-scale systems
metric: Interactions, Patterns, Low-order feature interactions, High-order feature interactions"
35734e8724559fb0d494e5cba6a28ad7a3d5dd4d,"document : Explaining and Harnessing Adversarial Examples Several machine learning models , including neural networks , consistently misclassify adversarial examples —inputs formed by applying small but intentionally worst - case perturbations to examples from the dataset , such that the perturbed input results in the model outputting an incorrect answer with high confidence . Early attempts at explaining this phenomenon focused on nonlinearity and overfitting . We argue instead that the primary cause of neural networks ’ vulnerability to adversarial perturbation is their linear nature . This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them : their generalization across architectures and training sets .",5,"['Task', 'Method', 'Material', 'Metric']","document : Explaining and Harnessing Adversarial Examples Several machine learning models , including neural networks , consistently misclassify adversarial examples —inputs formed by applying small but intentionally worst - case perturbations to examples from the dataset , such that the perturbed input results in the model outputting an incorrect answer with high confidence . Early attempts at explaining this phenomenon focused on nonlinearity and overfitting . We argue instead that the primary cause of neural networks ’ vulnerability to adversarial perturbation is their linear nature . This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them : their generalization across architectures and training sets .","{'Task': ['adversarial perturbation'], 'Method': ['neural networks', 'machine learning models', 'architectures'], 'Material': [], 'Metric': []}","{'Task': ['explaining', 'harnessing', 'neural networks vulnerability to adversarial perturbation'],
 'Method': ['applying small but intentionally worst-case perturbations', 'forming inputs', 'perturbing examples from the dataset'],
 'Material': ['machine learning models', 'neural networks', 'adversarial examples'],
 'Metric': ['model outputting an incorrect answer', 'high confidence']}","task: Explaining the cause of neural networks' vulnerability to adversarial perturbations
method: Arguing that neural networks' linear nature is the primary cause
material: Machine learning models, neural networks, adversarial examples, inputs, perturbations, dataset
metric: Misclassification rate, high confidence output, quantitative results, generalization across architectures and training sets"
364c1a3df58d87cb40ab33fdf3831cf2862f3570,"document : aNMM : Ranking Short Answer Texts with Attention - Based Neural Matching Model As an alternative to question answering methods based on feature engineering , deep learning approaches such as convolutional neural networks ( CNNs ) and Long Short - Term Memory Models ( LSTMs ) have recently been proposed for semantic matching of questions and answers . To achieve good results , however , these models have been combined with additional features such as word overlap or BM25 scores . Without this combination , these models perform significantly worse than methods based on linguistic feature engineering . In this paper , we propose an attention based neural matching model for ranking short answer text .",5,"['Task', 'Method', 'Material', 'Metric']","document : aNMM : Ranking Short Answer Texts with Attention - Based Neural Matching Model As an alternative to question answering methods based on feature engineering , deep learning approaches such as convolutional neural networks ( CNNs ) and Long Short - Term Memory Models ( LSTMs ) have recently been proposed for semantic matching of questions and answers . To achieve good results , however , these models have been combined with additional features such as word overlap or BM25 scores . Without this combination , these models perform significantly worse than methods based on linguistic feature engineering . In this paper , we propose an attention based neural matching model for ranking short answer text .","{'Task': ['semantic matching of questions and answers', 'Ranking Short Answer Texts', 'ranking short answer text', 'question answering'], 'Method': ['Attention - Based Neural Matching Model', 'attention based neural matching model', 'LSTMs', 'linguistic feature engineering', 'feature engineering', 'convolutional neural networks', 'Long Short - Term Memory Models', 'CNNs', 'aNMM', 'deep learning approaches'], 'Material': [], 'Metric': ['BM25 scores']}","{'Task': ['ranking short answer texts'], 'Method': ['attention-based neural matching model'], 'Material': ['deep learning approaches', 'convolutional neural networks (CNNs)', 'Long Short-Term Memory Models (LSTMs)', 'additional features', 'word overlap', 'BM25 scores'], 'Metric': ['good results']}","task: Ranking Short Answer Texts
method: [Attention-Based Neural Matching Model, Convolutional Neural Networks (CNNs), Long Short-Term Memory Models (LSTMs)]
material: [Short answer texts, Questions]
metric: [Word overlap, BM25 scores]"
3842ee1e0fdfeff936b5c49973ff21adfaaf3929,"document : Adversarial Discriminative Domain Adaptation Adversarial learning methods are a promising approach to training robust deep networks , and can generate complex samples across diverse domains . They also can improve recognition despite the presence of domain shift or dataset bias : several adversarial approaches to unsupervised domain adaptation have recently been introduced , which reduce the difference between the training and test domain distributions and thus improve generalization performance . Prior generative approaches show compelling visualizations , but are not optimal on discriminative tasks and can be limited to smaller shifts . Prior discriminative approaches could handle larger domain shifts , but imposed tied weights on the model and did not exploit a GAN - based loss .",5,"['Task', 'Method', 'Material', 'Metric']","document : Adversarial Discriminative Domain Adaptation Adversarial learning methods are a promising approach to training robust deep networks , and can generate complex samples across diverse domains . They also can improve recognition despite the presence of domain shift or dataset bias : several adversarial approaches to unsupervised domain adaptation have recently been introduced , which reduce the difference between the training and test domain distributions and thus improve generalization performance . Prior generative approaches show compelling visualizations , but are not optimal on discriminative tasks and can be limited to smaller shifts . Prior discriminative approaches could handle larger domain shifts , but imposed tied weights on the model and did not exploit a GAN - based loss .","{'Task': ['robust deep networks', 'generalization', 'discriminative tasks', 'unsupervised domain adaptation', 'recognition'], 'Method': ['Adversarial learning methods', 'adversarial approaches', 'Adversarial Discriminative Domain Adaptation', 'GAN', 'generative approaches', 'discriminative approaches'], 'Material': [], 'Metric': []}","{'Task': ['unsupervised domain adaptation', 'improving recognition', 'handling larger domain shifts'],
 'Method': ['adversarial learning methods', 'several adversarial approaches', 'prior generative approaches', 'prior discriminative approaches', 'imposing tied weights', 'not exploiting a GAN-based loss'],
 'Material': [None],
 'Metric': ['generalization performance']}","task: unsupervised domain adaptation, recognition, generalization performance
method: adversarial learning methods, adversarial approaches to unsupervised domain adaptation, generative approaches, discriminative approaches, GAN-based loss
material: deep networks
metric: domain shift, dataset bias, difference between training and test domain distributions"
38cc89399dd6f5aaab1654f27ab3c9eeade12a36,"document : Exploiting temporal information for 3D human pose estimation In this work , we address the problem of 3D human pose estimation from a sequence of 2D human poses . Although the recent success of deep networks has led many state - of - the - art methods for 3D pose estimation to train deep networks end - to - end to predict from images directly , the top - performing approaches have shown the effectiveness of dividing the task of 3D pose estimation into two steps : using a state - of - the - art 2D pose estimator to estimate the 2D pose from images and then mapping them into 3D space . They also showed that a low - dimensional representation like 2D locations of a set of joints can be discriminative enough to estimate 3D pose with high accuracy . However , estimation of 3D pose for individual frames leads to temporally incoherent estimates due to independent error in each frame causing jitter .",5,"['Task', 'Method', 'Material', 'Metric']","document : Exploiting temporal information for 3D human pose estimation In this work , we address the problem of 3D human pose estimation from a sequence of 2D human poses . Although the recent success of deep networks has led many state - of - the - art methods for 3D pose estimation to train deep networks end - to - end to predict from images directly , the top - performing approaches have shown the effectiveness of dividing the task of 3D pose estimation into two steps : using a state - of - the - art 2D pose estimator to estimate the 2D pose from images and then mapping them into 3D space . They also showed that a low - dimensional representation like 2D locations of a set of joints can be discriminative enough to estimate 3D pose with high accuracy . However , estimation of 3D pose for individual frames leads to temporally incoherent estimates due to independent error in each frame causing jitter .","{'Task': ['3D human pose estimation', '3D pose'], 'Method': ['low - dimensional representation', 'deep networks', '2D pose estimator'], 'Material': [], 'Metric': ['accuracy', 'error']}","{'Task': ['3D human pose estimation', 'dividing the task of 3D pose estimation into two steps'],
 'Method': ['using a state-of-the-art 2D pose estimator', 'mapping 2D poses into 3D space'],
 'Material': ['sequence of 2D human poses', 'images', 'low-dimensional representation', '2D locations of a set of joints'],
 'Metric': ['high accuracy']}","Task:
- 3D human pose estimation
- Estimation of 3D pose for individual frames

Method:
- Training deep networks end-to-end to predict 3D pose from images directly
- Dividing the task of 3D pose estimation into two steps: using a state-of-the-art 2D pose estimator to estimate 2D pose from images and then mapping them into 3D space

Material:
- Sequence of 2D human poses
- Images
- State-of-the-art 2D pose estimator
- Low-dimensional representation (2D locations of a set of joints)

Metric:
- Accuracy (of 3D pose estimation)
- Temporal coherence (of 3D pose estimates)"
38e2f851b705faa0d0a698ed9885bd6834440073,"Probabilistic Model - Agnostic Meta - Learning section : Abstract Meta - learning for few - shot learning entails acquiring a prior over previous tasks and experiences , such that new tasks be learned from small amounts of data . However , a critical challenge in few - shot learning is task ambiguity : even when a powerful prior can be meta - learned from a large number of prior tasks , a small dataset for a new task can simply be too ambiguous to acquire a single model ( e.g. , a classifier ) for that task that is accurate . In this paper , we propose a probabilistic meta - learning algorithm that can sample models for a new task from a model distribution .",5,"['Task', 'Method', 'Material', 'Metric']","Probabilistic Model - Agnostic Meta - Learning section : Abstract Meta - learning for few - shot learning entails acquiring a prior over previous tasks and experiences , such that new tasks be learned from small amounts of data . However , a critical challenge in few - shot learning is task ambiguity : even when a powerful prior can be meta - learned from a large number of prior tasks , a small dataset for a new task can simply be too ambiguous to acquire a single model ( e.g. , a classifier ) for that task that is accurate . In this paper , we propose a probabilistic meta - learning algorithm that can sample models for a new task from a model distribution .","{'Task': ['task ambiguity', 'few - shot learning'], 'Method': ['classifier', 'probabilistic meta - learning algorithm', 'Meta - learning', 'Probabilistic Model - Agnostic Meta - Learning'], 'Material': [], 'Metric': []}","{'Task': ['few-shot learning', 'new task'], 'Method': ['meta-learning', 'probabilistic meta-learning', 'acquiring a prior', 'sampling models'], 'Material': ['previous tasks and experiences', 'small datasets'], 'Metric': ['accuracy']}","task:
- name: Few-shot learning
- name: Meta learning for few-shot learning
- name: Acquiring a prior over previous tasks and experiences
- name: Learning new tasks from small amounts of data
method:
- name: Probabilistic meta learning
- name: Sampling models for a new task from a model distribution
material: []
metric:
- name: Task ambiguity
- name: Accuracy of a model for a new task
- name: Size of a dataset for a new task"
3aa21de1a7c97e0458e10ed5730ce160bb436caa,"document : Pixel2Mesh : Generating 3D Mesh Models from Single RGB Images We propose an end - to - end deep learning architecture that produces a 3D shape in triangular mesh from a single color image . Limited by the nature of deep neural network , previous methods usually represent a 3D shape in volume or point cloud , and it is non - trivial to convert them to the more ready - to - use mesh model . Unlike the existing methods , our network represents 3D mesh in a graph - based convolutional neural network and produces correct geometry by progressively deforming an ellipsoid , leveraging perceptual features extracted from the input image . We adopt a coarse - to - fine strategy to make the whole deformation procedure stable , and define various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3D geometry .",5,"['Task', 'Method', 'Material', 'Metric']","document : Pixel2Mesh : Generating 3D Mesh Models from Single RGB Images We propose an end - to - end deep learning architecture that produces a 3D shape in triangular mesh from a single color image . Limited by the nature of deep neural network , previous methods usually represent a 3D shape in volume or point cloud , and it is non - trivial to convert them to the more ready - to - use mesh model . Unlike the existing methods , our network represents 3D mesh in a graph - based convolutional neural network and produces correct geometry by progressively deforming an ellipsoid , leveraging perceptual features extracted from the input image . We adopt a coarse - to - fine strategy to make the whole deformation procedure stable , and define various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3D geometry .","{'Task': [], 'Method': ['Pixel2Mesh', '3D Mesh Models', 'coarse - to - fine strategy', 'graph - based convolutional neural network', 'end deep learning architecture', '3D mesh', 'mesh model', 'deep neural network', 'deformation procedure'], 'Material': ['RGB Images'], 'Metric': []}","{'Task': ['Generating 3D Mesh Models', 'Producing a 3D shape'], 'Method': ['End-to-end deep learning architecture', 'Representing 3D mesh in a graph-based convolutional neural network', 'Progressively deforming an ellipsoid', 'Defining various mesh related losses'], 'Material': [], 'Metric': ['Visually appealing', 'Physically accurate']}","task: Generating 3D Mesh Models from Single RGB Images
method:
- Proposing an end-to-end deep learning architecture
- Representing 3D mesh in a graph-based convolutional neural network
- Progressively deforming an ellipsoid
- Defining various mesh related losses
material: N/A
metric:
- Visually appealing 3D geometry
- Physically accurate 3D geometry
- Stable deformation procedure"
3acc07f7f8951617276cf99483ed02aeb0a6eeac,"document : Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes During the last half decade , convolutional neural networks ( CNNs ) have triumphed over semantic segmentation , which is a core task of various emerging industrial applications such as autonomous driving and medical imaging . However , to train CNNs requires a huge amount of data , which is difficult to collect and laborious to annotate . Recent advances in computer graphics make it possible to train CNN models on photo - realistic synthetic data with computer - generated annotations . Despite this , the domain mismatch between the real images and the synthetic data significantly decreases the models ’ performance .",5,"['Task', 'Method', 'Material', 'Metric']","document : Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes During the last half decade , convolutional neural networks ( CNNs ) have triumphed over semantic segmentation , which is a core task of various emerging industrial applications such as autonomous driving and medical imaging . However , to train CNNs requires a huge amount of data , which is difficult to collect and laborious to annotate . Recent advances in computer graphics make it possible to train CNN models on photo - realistic synthetic data with computer - generated annotations . Despite this , the domain mismatch between the real images and the synthetic data significantly decreases the models ’ performance .","{'Task': ['semantic segmentation', 'autonomous driving', 'medical imaging', 'computer graphics', 'industrial applications', 'Semantic Segmentation of Urban Scenes'], 'Method': ['CNN', 'convolutional neural networks', 'CNNs', 'Curriculum Domain Adaptation'], 'Material': [], 'Metric': []}","{'Task': ['semantic segmentation', 'core task of various emerging industrial applications such as autonomous driving and medical imaging'],
 'Method': ['convolutional neural networks (CNNs)', 'training CNN models'],
 'Material': ['real images', 'photo-realistic synthetic data'],
 'Metric': ['models ’ performance']}","task: [""semantic segmentation"", ""autonomous driving"", ""medical imaging""]
method: [""convolutional neural networks (CNNs)"", ""training CNN models on photo-realistic synthetic data""]
material: [""huge amount of data"", ""real images"", ""synthetic data"", ""computer-generated annotations""]
metric: [""models’ performance"", ""domain mismatch""]"
3ca3993b1f3536b15112f759067f62e999c5d38f,"BB8 : A Scalable , Accurate , Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth section : Abstract We introduce a novel method for 3D object detection and pose estimation from color images only . We first use segmentation to detect the objects of interest in 2D even in presence of partial occlusions and cluttered background .",5,"['Task', 'Method', 'Material', 'Metric']","BB8 : A Scalable , Accurate , Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth section : Abstract We introduce a novel method for 3D object detection and pose estimation from color images only . We first use segmentation to detect the objects of interest in 2D even in presence of partial occlusions and cluttered background .","{'Task': ['3D object detection', 'objects of interest in 2D even', 'Predicting the 3D Poses of Challenging Objects', 'pose estimation'], 'Method': ['segmentation', 'Partial Occlusion Method', 'BB8'], 'Material': [], 'Metric': []}","{'Task': ['3D object detection', 'pose estimation'], 'Method': ['BB8', 'segmentation'], 'Material': ['color images'], 'Metric': ['scalable', 'accurate', 'robust to partial occlusion']}","task: 3D object detection and pose estimation
method:
- Novel method for 3D object detection and pose estimation from color images only
- Segmentation for object detection in 2D
material: Color images
metric:
- Scalability
- Accuracy
- Robustness to partial occlusion"
3cf31ecb2724b5088783d7c96a5fc0d5604cbf41,"document : Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations We present a simple and effective scheme for dependency parsing which is based on bidirectional - LSTMs ( BiLSTMs ) . Each sentence token is associated with a BiLSTM vector representing the token in its sentential context , and feature vectors are constructed by concatenating a few BiLSTM vectors . The BiLSTM is trained jointly with the parser objective , resulting in very effective feature extractors for parsing . We demonstrate the effectiveness of the approach by applying it to a greedy transition - based parser as well as to a globally optimized graph - based parser .",5,"['Task', 'Method', 'Material', 'Metric']","document : Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations We present a simple and effective scheme for dependency parsing which is based on bidirectional - LSTMs ( BiLSTMs ) . Each sentence token is associated with a BiLSTM vector representing the token in its sentential context , and feature vectors are constructed by concatenating a few BiLSTM vectors . The BiLSTM is trained jointly with the parser objective , resulting in very effective feature extractors for parsing . We demonstrate the effectiveness of the approach by applying it to a greedy transition - based parser as well as to a globally optimized graph - based parser .","{'Task': ['Dependency Parsing', 'dependency parsing', 'parsing'], 'Method': ['BiLSTMs', 'greedy transition - based parser', 'feature extractors', 'Bidirectional LSTM Feature Representations', 'BiLSTM', 'bidirectional - LSTMs', 'globally optimized graph - based parser'], 'Material': [], 'Metric': ['parser objective']}","{'Task': ['dependency parsing'], 'Method': ['bidirectional-LSTMs (BiLSTMs)', 'greedy transition-based parser', 'globally optimized graph-based parser'], 'Material': ['sentence token', 'BiLSTM vector', 'parser objective'], 'Metric': ['effectiveness']}","task: Dependency Parsing
method:
- Bidirectional Long Short-Term Memory (BiLSTM)
material:
- Sentence tokens
- BiLSTM vectors
- Few BiLSTM vectors for feature construction
metric:
- Parser objective
- Effectiveness of the approach (demonstrated through application to a greedy transition-based parser and a globally optimized graph-based parser)"
3daa086acd367dc971a2dc1382caba2031294233,"Holistic , Instance - level Human Parsing section : Abstract Object parsing - the task of decomposing an object into its semantic parts - has traditionally been formulated as a category - level segmentation problem . Consequently , when there are multiple objects in an image , current methods can not count the number of objects in the scene , nor can they determine which part belongs to which object . We address this problem by segmenting the parts of objects at an instance - level , such that each pixel in the image is assigned a part label , as well as the identity of the object it belongs to .",5,"['Task', 'Method', 'Material', 'Metric']","Holistic , Instance - level Human Parsing section : Abstract Object parsing - the task of decomposing an object into its semantic parts - has traditionally been formulated as a category - level segmentation problem . Consequently , when there are multiple objects in an image , current methods can not count the number of objects in the scene , nor can they determine which part belongs to which object . We address this problem by segmenting the parts of objects at an instance - level , such that each pixel in the image is assigned a part label , as well as the identity of the object it belongs to .","{'Task': ['category - level segmentation problem', 'Holistic , Instance - level Human Parsing', 'decomposing an object', 'Object parsing'], 'Method': [], 'Material': [], 'Metric': []}","{'Task': ['Abstract Object parsing', 'decomposing an object into its semantic parts', 'segmentation problem', 'counting the number of objects in the scene', 'determining which part belongs to which object'],
 'Method': ['Instance-level segmenting', 'segmenting the parts of objects'],
 'Material': [],
 'Metric': ['pixel in the image', 'part label', 'identity of the object']}","```yaml
Task:
- Holistic Human Parsing
- Instance-level Object Parsing
- Abstract Object Parsing
- Decomposing objects into semantic parts
- Counting the number of objects in a scene
- Determining which part belongs to which object

Method:
- Category-level segmentation
- Instance-level segmentation
- Pixel-level segmentation

Material:
- Image

Metric:
- Part label
- Object identity
```"
408e8eecc14c5cc60bbdfc486ba7a7fc97031788,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training . In this paper we present an approach for training a convolutional neural network using only unlabeled data . We train the network to discriminate between a set of surrogate classes . Each surrogate class is formed by applying a variety of transformations to a randomly sampled ’ seed ’ image patch . We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition .,5,"['Task', 'Method', 'Material', 'Metric']",Discriminative Unsupervised Feature Learning with Convolutional Neural Networks Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training . In this paper we present an approach for training a convolutional neural network using only unlabeled data . We train the network to discriminate between a set of surrogate classes . Each surrogate class is formed by applying a variety of transformations to a randomly sampled ’ seed ’ image patch . We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition .,"{'Task': ['visual object recognition', 'supervised training'], 'Method': ['network', 'convolutional neural networks', 'feature learning algorithm', 'Discriminative Unsupervised Feature Learning with Convolutional Neural Networks'], 'Material': [], 'Metric': []}","{'Task': ['visual object recognition'], 'Method': ['Discriminative Unsupervised Feature Learning', 'training a convolutional neural network using only unlabeled data', 'training the network to discriminate between a set of surrogate classes'], 'Material': ['convolutional neural network', 'surrogate classes', 'seed image patch'], 'Metric': ['successfully']}","task: Visual object recognition
method: Discriminative Unsupervised Feature Learning with Convolutional Neural Networks, Training a convolutional neural network using only unlabeled data, Forming surrogate classes by applying a variety of transformations to a randomly sampled 'seed' image patch
material: Unlabeled data, Image patches
metric: Success of the feature learning algorithm in visual object recognition"
4365eb43a635bc6431dfaf3af1f7bf7bf55522cc,"document : CoupleNet : Coupling Global Structure with Local Parts for Object Detection The region - based Convolutional Neural Network ( CNN ) detectors such as Faster R - CNN or R - FCN have already shown promising results for object detection by combining the region proposal subnetwork and the classification subnetwork together . Although R - FCN has achieved higher detection speed while keeping the detection performance , the global structure information is ignored by the position - sensitive score maps . To fully explore the local and global properties , in this paper , we propose a novel fully convolutional network , named as CoupleNet , to couple the global structure with local parts for object detection . Specifically , the object proposals obtained by the Region Proposal Network ( RPN ) are fed into the the coupling module which consists of two branches .",5,"['Task', 'Method', 'Material', 'Metric']","document : CoupleNet : Coupling Global Structure with Local Parts for Object Detection The region - based Convolutional Neural Network ( CNN ) detectors such as Faster R - CNN or R - FCN have already shown promising results for object detection by combining the region proposal subnetwork and the classification subnetwork together . Although R - FCN has achieved higher detection speed while keeping the detection performance , the global structure information is ignored by the position - sensitive score maps . To fully explore the local and global properties , in this paper , we propose a novel fully convolutional network , named as CoupleNet , to couple the global structure with local parts for object detection . Specifically , the object proposals obtained by the Region Proposal Network ( RPN ) are fed into the the coupling module which consists of two branches .","{'Task': ['Object Detection', 'detection', 'object detection'], 'Method': ['region proposal subnetwork', 'fully convolutional network', 'RPN', 'CNN', 'CoupleNet', 'coupling module', 'Region Proposal Network', 'classification subnetwork', 'R - FCN', 'Convolutional Neural Network'], 'Material': [], 'Metric': ['detection speed']}","{'Task': ['object detection'], 'Method': ['region-based Convolutional Neural Network (CNN)', 'Faster R-CNN', 'R-FCN', 'CoupleNet', 'Region Proposal Network (RPN)', 'coupling module'], 'Material': [], 'Metric': ['detection speed', 'detection performance', 'local properties', 'global structure information']}","task: Object Detection
method:
- Region-based Convolutional Neural Network (CNN)
- Faster R-CNN
- R-FCN
- Proposed method: CoupleNet
material:
- Region proposals
- Object proposals obtained by Region Proposal Network (RPN)
metric:
- Detection speed
- Detection performance
- Local properties
- Global structure information"
436b07bebaa1d1f05ef85415e10374048d25334d,"OUTRAGEOUSLY LARGE NEURAL NETWORKS : THE SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER section : ABSTRACT The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per - example basis , has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation .",5,"['Task', 'Method', 'Material', 'Metric']","OUTRAGEOUSLY LARGE NEURAL NETWORKS : THE SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER section : ABSTRACT The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per - example basis , has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation .","{'Task': ['Conditional computation', 'computation', 'model capacity'], 'Method': ['OUTRAGEOUSLY LARGE NEURAL NETWORKS', 'SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER', 'neural network'], 'Material': [], 'Metric': []}","Based on the given document, here's the Python dictionary with extracted entities:

{'Task': ['absorbing information', 'dramatically increasing model capacity'],
 'Method': ['conditional computation', 'parts of the network are active on a per - example basis'],
 'Material': ['neural network'],
 'Metric': ['number of parameters']}","task: Increasing model capacity in neural networks
method:
- Conditional computation
- Parts of the network are active on a per-example basis
material: Neural networks
metric: Model capacity, Computation"
44078d0daed8b13114cffb15b368acc467f96351,"document : Triplet Probabilistic Embedding for Face Verification and Clustering Despite significant progress made over the past twenty five years , unconstrained face verification remains a challenging problem . This paper proposes an approach that couples a deep CNN - based approach with a low - dimensional discriminative embedding step , learned using triplet probability constraints to address the unconstrained face verification problem . Aside from yielding performance improvements , this embedding provides significant advantages in terms of memory and for post - processing operations like subject specific clustering . Experiments on the challenging IJB - A dataset show that the proposed algorithm performs close to the state of the art methods in verification and identification metrics , while requiring much less training data and training / test time .",5,"['Task', 'Method', 'Material', 'Metric']","document : Triplet Probabilistic Embedding for Face Verification and Clustering Despite significant progress made over the past twenty five years , unconstrained face verification remains a challenging problem . This paper proposes an approach that couples a deep CNN - based approach with a low - dimensional discriminative embedding step , learned using triplet probability constraints to address the unconstrained face verification problem . Aside from yielding performance improvements , this embedding provides significant advantages in terms of memory and for post - processing operations like subject specific clustering . Experiments on the challenging IJB - A dataset show that the proposed algorithm performs close to the state of the art methods in verification and identification metrics , while requiring much less training data and training / test time .","{'Task': ['post - processing operations', 'unconstrained face verification problem', 'Clustering', 'Face Verification', 'unconstrained face verification'], 'Method': ['Triplet Probabilistic Embedding', 'CNN', 'subject specific clustering', 'low - dimensional discriminative embedding step'], 'Material': ['IJB - A dataset'], 'Metric': ['verification and identification metrics', 'training / test time', 'memory']}","{'Task': ['unconstrained face verification', 'subject specific clustering'],
 'Method': ['deep CNN - based approach', 'low-dimensional discriminative embedding step', 'triplet probability constraints'],
 'Material': ['IJB-A dataset'],
 'Metric': ['verification metrics', 'identification metrics']}","task: [""unconstrained face verification"", ""subject specific clustering""]
method: [""deep CNN-based approach"", ""low-dimensional discriminative embedding step"", ""triplet probability constraints""]
material: []
metric: [""performance improvements"", ""memory advantages"", ""verification and identification metrics""]"
45429c281e30f9e87ebcd1ae42e0656d2ead24d1,"document : High - Resolution Image Synthesis and Semantic Manipulation with Conditional GANs We present a new method for synthesizing high - resolution photo - realistic images from semantic label maps using conditional generative adversarial networks ( conditional GANs ) . Conditional GANs have enabled a variety of applications , but the results are often limited to low - resolution and still far from realistic . In this work , we generate visually appealing results with a novel adversarial loss , as well as new multi - scale generator and discriminator architectures . Furthermore , we extend our framework to interactive visual manipulation with two additional features .",5,"['Task', 'Method', 'Material', 'Metric']","document : High - Resolution Image Synthesis and Semantic Manipulation with Conditional GANs We present a new method for synthesizing high - resolution photo - realistic images from semantic label maps using conditional generative adversarial networks ( conditional GANs ) . Conditional GANs have enabled a variety of applications , but the results are often limited to low - resolution and still far from realistic . In this work , we generate visually appealing results with a novel adversarial loss , as well as new multi - scale generator and discriminator architectures . Furthermore , we extend our framework to interactive visual manipulation with two additional features .","{'Task': ['High - Resolution Image Synthesis', 'semantic label maps', 'Semantic Manipulation', 'synthesizing high - resolution photo - realistic images', 'interactive visual manipulation'], 'Method': ['multi - scale generator and discriminator architectures', 'conditional GANs', 'adversarial loss', 'Conditional GANs', 'conditional generative adversarial networks'], 'Material': [], 'Metric': []}","{'Task': ['High-Resolution Image Synthesis', 'Interactive visual manipulation'],
 'Method': ['conditional generative adversarial networks (conditional GANs)', 'novel adversarial loss', 'new multi-scale generator and discriminator architectures'],
 'Material': ['semantic label maps'],
 'Metric': ['visually appealing results']}","task: High-Resolution Image Synthesis from Semantic Label Maps
method: Conditional Generative Adversarial Networks (Conditional GANs)
- Adversarial Loss (novel)
- Multi-Scale Generator Architectures
- Discriminator Architectures
material: Semantic Label Maps
metric: Visual Appeal (generating visually appealing results)"
455da02e5048dffb51fb6ab5eb8aeca5926c9d9a,"document : Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition Existing deep convolutional neural networks ( CNNs ) require a fixed - size ( , 224 224 ) input image . This requirement is “ artificial ” and may reduce the recognition accuracy for the images or sub - images of an arbitrary size / scale . In this work , we equip the networks with another pooling strategy , “ spatial pyramid pooling ” , to eliminate the above requirement .",5,"['Task', 'Method', 'Material', 'Metric']","document : Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition Existing deep convolutional neural networks ( CNNs ) require a fixed - size ( , 224 224 ) input image . This requirement is “ artificial ” and may reduce the recognition accuracy for the images or sub - images of an arbitrary size / scale . In this work , we equip the networks with another pooling strategy , “ spatial pyramid pooling ” , to eliminate the above requirement .","{'Task': ['Visual Recognition'], 'Method': ['Spatial Pyramid Pooling', 'Deep Convolutional Networks', 'pooling strategy', 'CNNs', 'deep convolutional neural networks', 'spatial pyramid pooling'], 'Material': [], 'Metric': ['recognition accuracy']}","{'Task': ['visual recognition'], 'Method': ['spatial pyramid pooling'], 'Material': ['deep convolutional neural networks (CNNs)'], 'Metric': ['recognition accuracy']}","task: Visual Recognition
method: [ ""deep convolutional neural networks (CNNs)"", ""equip the networks with another pooling strategy"", ""spatial pyramid pooling"" ]
material: []
metric: [""recognition accuracy"", ""images or sub-images of an arbitrary size / scale"", ""fixed-size input image (224x224)"" ]"

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbOusBkMpteY",
        "outputId": "4581000a-5b00-4e9a-f943-321c7024b1d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uN-aL4hapxqV",
        "outputId": "c5385aed-03fd-4f4f-aa06-8cf417c93a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.16.2-py3-none-any.whl (267 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/267.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/267.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.16.2\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.3)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.1 responses-0.18.0\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.2)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=2a41ee76dae975dab9380ca9dcc7ca68398a90efa2b7ae3934c680534075d578\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ubWNKulUpzjA"
      },
      "outputs": [],
      "source": [
        "# import glob\n",
        "# import random\n",
        "# import os\n",
        "# import csv\n",
        "\n",
        "# DOC_PATH = \"/content/drive/MyDrive/NUS/PhD/Prof.Kenji/FormatBias/EvalData/SemEval2017/docsutf8/\"\n",
        "# KEYWORD_PATH = \"/content/drive/MyDrive/NUS/PhD/Prof.Kenji/FormatBias/EvalData/SemEval2017/keys/\"\n",
        "\n",
        "# all_files = os.listdir(DOC_PATH)\n",
        "\n",
        "# random.shuffle(all_files)\n",
        "\n",
        "# saved_data = []\n",
        "# for file_name in all_files[:200]:\n",
        "#     document_path = os.path.join(DOC_PATH, file_name)\n",
        "#     with open(document_path, \"r\") as file:\n",
        "#         document = file.read().strip()\n",
        "\n",
        "#     keyword_path = os.path.join(KEYWORD_PATH, file_name[:-4] + \".key\")\n",
        "#     keyphrases = []\n",
        "#     with open(keyword_path) as file:\n",
        "#         for line in file:\n",
        "#             keyphrases.append(line.split(\"\\n\")[0])\n",
        "\n",
        "#     saved_data.append([file_name[:-4], document, keyphrases])\n",
        "\n",
        "# with open(\"/content/drive/MyDrive/NUS/PhD/Prof.Kenji/FormatBias/EvalData/list_keyphrases_SemEval2017_200.csv\", \"w\") as file:\n",
        "#     csvwriter = csv.writer(file)\n",
        "#     csvwriter.writerow([\"id\", \"document\", \"keyphrases\"])\n",
        "#     csvwriter.writerows(saved_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbbX-bpUsTq0",
        "outputId": "6df70b02-e9d9-4168-8cf5-5c9c9ed4a037"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score: 0.5\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "key = \"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\"\n",
        "client = OpenAI(api_key = key)\n",
        "\n",
        "def get_chatgpt_answer(input_prompt, role=\"You are helpful assistant!\", n=1):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0125\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": role},\n",
        "            {\"role\": \"user\", \"content\": input_prompt},\n",
        "        ],\n",
        "        max_tokens=1024,\n",
        "        top_p=1,\n",
        "        n=n,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    if n > 1:\n",
        "        ans = []\n",
        "        for idx in range(n): ans.append(response.choices[idx].message.content)\n",
        "        return ans\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def compute_f1(list1, list2):\n",
        "    # Convert lists to sets for efficient intersection and union operations\n",
        "    set1 = set(list1)\n",
        "    set2 = set(list2)\n",
        "\n",
        "    # Calculate True Positives, False Positives, and False Negatives\n",
        "    true_positives = len(set1.intersection(set2))\n",
        "    false_positives = len(set2.difference(set1))\n",
        "    false_negatives = len(set1.difference(set2))\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    precision = true_positives / (true_positives + false_positives)\n",
        "    recall = true_positives / (true_positives + false_negatives)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "\n",
        "    return f1_score\n",
        "\n",
        "# Example usage\n",
        "list1 = [\"apple\", \"banana\", \"orange\", \"pear\"]\n",
        "list2 = [\"banana\", \"orange\", \"grape\", \"kiwi\"]\n",
        "\n",
        "f1_score = compute_f1(list1, list2)\n",
        "print(\"F1 Score:\", f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eIWhoNZ5qb6G"
      },
      "outputs": [],
      "source": [
        "from posixpath import expandvars\n",
        "import re\n",
        "import yaml\n",
        "import ast\n",
        "\n",
        "def check_python_follow_ranking(answer, cot=False):\n",
        "    if '```python' in answer and '```' in answer: answer = answer.replace('```python', '').replace('```', '')\n",
        "    if cot:\n",
        "        try: answer = answer.split(\"<ANSWER>\")[1].split(\"</ANSWER>\")[0].strip()\n",
        "        except: return False\n",
        "    try:\n",
        "        answer = ast.literal_eval(answer)\n",
        "        return answer\n",
        "    except: return False\n",
        "\n",
        "def extract_python_ranking(answer, cot=False):\n",
        "    if '```python' in answer and '```' in answer: answer = answer.replace('```python', '').replace('```', '')\n",
        "    if cot: answer = answer.split(\"<ANSWER>\")[1].split(\"</ANSWER>\")[0].strip()\n",
        "    return ast.literal_eval(answer)\n",
        "\n",
        "def check_newline_follow_ranking(answer, cot=False):\n",
        "    if cot:\n",
        "        try: answer = answer.split(\"<ANSWER>\")[1].split(\"</ANSWER>\")[0].strip()\n",
        "        except: return False\n",
        "    try:\n",
        "        answer = answer.split(\"\\n\")\n",
        "        return answer\n",
        "    except: return False\n",
        "\n",
        "def extract_newline_ranking(answer, cot=False):\n",
        "    if cot: answer = answer.split(\"<ANSWER>\")[1].split(\"</ANSWER>\")[0].strip()\n",
        "    answer = answer.split(\"\\n\")\n",
        "    answer = [ans for ans in answer if len(ans) >= 2]\n",
        "    answer = [ans[1:].strip() if ans[0] == \"-\" else ans.strip() for ans in answer]\n",
        "    return answer\n",
        "\n",
        "def check_bullet_follow_ranking(answer, cot=False):\n",
        "    if cot:\n",
        "        try: answer = answer.split(\"<ANSWER>\")[1].split(\"</ANSWER>\")[0].strip()\n",
        "        except: return False\n",
        "    try:\n",
        "        answer = yaml.safe_load(answer)\n",
        "        return answer\n",
        "    except: return False\n",
        "\n",
        "def extract_bullet_ranking(answer, cot=False):\n",
        "    if cot: answer = answer.split(\"<ANSWER>\")[1].split(\"</ANSWER>\")[0].strip()\n",
        "    answer = yaml.safe_load(answer)\n",
        "    return answer\n",
        "\n",
        "def check_special_character_follow_ranking(answer, cot=False):\n",
        "    if cot:\n",
        "        try: answer = answer.split(\"<ANSWER>\")[1].split(\"</ANSWER>\")[0].strip()\n",
        "        except: return False\n",
        "    try:\n",
        "        answer = answer.split(\"<SEP>\")\n",
        "        return answer\n",
        "    except: return False\n",
        "\n",
        "def extract_special_character_ranking(answer, cot=False):\n",
        "    if cot: answer = answer.split(\"<ANSWER>\")[1].split(\"</ANSWER>\")[0].strip()\n",
        "    answer = answer.split(\"<SEP>\")\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRuZIpvVzQOq"
      },
      "source": [
        "<h1> 1. Zero-shot performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GPo1Qy_rc_Q",
        "outputId": "120f01a7-df70-4061-9feb-54920c2cccfa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "200it [19:36,  5.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python_fi: 0.935\n",
            "special_fi: 1.0\n",
            "bullet_fi: 0.965\n",
            "newline_fi: 1.0\n",
            "===\n",
            "python_F1: 0.39511906580025724\n",
            "special_F1: 0.08879880378259873\n",
            "bullet_F1: 0.31072992008440936\n",
            "newline_F1: 0.3716077584645687\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Zero-shot performance\n",
        "\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "import yaml\n",
        "import ast\n",
        "\n",
        "python_fi = 0\n",
        "special_fi = 0\n",
        "bullet_fi = 0\n",
        "newline_fi = 0\n",
        "\n",
        "python_F1 = 0\n",
        "special_F1 = 0\n",
        "bullet_F1 = 0\n",
        "newline_F1 = 0\n",
        "\n",
        "all_cnt = 0\n",
        "\n",
        "saved_data = []\n",
        "with open(\"/content/drive/MyDrive/NUS/PhD/Prof.Kenji/FormatBias/EvalData/list_keyphrases_SemEval2017_200.csv\") as file:\n",
        "    csvreader = csv.reader(file)\n",
        "    header = next(csvreader)\n",
        "    for row in tqdm(csvreader):\n",
        "        id = row[list(header).index(\"id\")]\n",
        "        document = row[list(header).index(\"document\")]\n",
        "        keyphrases = eval(row[list(header).index(\"keyphrases\")])\n",
        "\n",
        "        zs_python_prompt = f\"\"\"Extract a list of keyphrases from the following document:\n",
        "Document: {document}\n",
        "Generate your list as a Python list without any explanation.\"\"\"\n",
        "\n",
        "        zs_bullet_prompt = f\"\"\"Extract a list of keyphrases from the following document:\n",
        "Document: {document}\n",
        "Generate your list using bullet points without any explanation.\"\"\"\n",
        "\n",
        "        zs_special_prompt = f\"\"\"Extract a list of keyphrases from the following document:\n",
        "Document: {document}\n",
        "Generate your list using <SEP> to seperate elements without any explanation.\"\"\"\n",
        "\n",
        "        zs_newline_prompt = f\"\"\"Extract a list of keyphrases from the following document:\n",
        "Document: {document}\n",
        "Generate your list such that each element is in a new line without any explanation.\"\"\"\n",
        "\n",
        "        zs_python_answer = get_chatgpt_answer(zs_python_prompt)\n",
        "        zs_special_answer = get_chatgpt_answer(zs_special_prompt)\n",
        "        zs_bullet_answer = get_chatgpt_answer(zs_bullet_prompt)\n",
        "        zs_newline_answer =  get_chatgpt_answer(zs_newline_prompt)\n",
        "\n",
        "        python_follow = check_python_follow_ranking(zs_python_answer)\n",
        "        if python_follow:\n",
        "            python_fi += 1\n",
        "            extract_python = extract_python_ranking(zs_python_answer)\n",
        "            python_F1 += compute_f1(extract_python, keyphrases)\n",
        "\n",
        "        special_follow = check_special_character_follow_ranking(zs_special_answer)\n",
        "        if special_follow:\n",
        "            special_fi += 1\n",
        "            extract_special = extract_special_character_ranking(zs_special_answer)\n",
        "            special_F1 += compute_f1(extract_special, keyphrases)\n",
        "\n",
        "        bullet_follow = check_bullet_follow_ranking(zs_bullet_answer)\n",
        "        if bullet_follow:\n",
        "            bullet_fi += 1\n",
        "            extract_bullet = extract_bullet_ranking(zs_bullet_answer)\n",
        "            bullet_F1 += compute_f1(extract_bullet, keyphrases)\n",
        "\n",
        "        newline_follow = check_newline_follow_ranking(zs_newline_answer)\n",
        "        if newline_follow:\n",
        "            newline_fi += 1\n",
        "            extract_newline = extract_newline_ranking(zs_newline_answer)\n",
        "            newline_F1 += compute_f1(extract_newline, keyphrases)\n",
        "\n",
        "        all_cnt += 1\n",
        "        saved_data.append([id, document, keyphrases, zs_python_answer, zs_special_answer, zs_bullet_answer, zs_newline_prompt])\n",
        "\n",
        "print(f\"python_fi: {python_fi/all_cnt}\")\n",
        "print(f\"special_fi: {special_fi/all_cnt}\")\n",
        "print(f\"bullet_fi: {bullet_fi/all_cnt}\")\n",
        "print(f\"newline_fi: {newline_fi/all_cnt}\")\n",
        "print(\"===\")\n",
        "print(f\"python_F1: {python_F1/all_cnt}\")\n",
        "print(f\"special_F1: {special_F1/all_cnt}\")\n",
        "print(f\"bullet_F1: {bullet_F1/all_cnt}\")\n",
        "print(f\"newline_F1: {newline_F1/all_cnt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vW9iqRaMu7X8"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open(\"/content/drive/MyDrive/NUS/PhD/Prof.Kenji/FormatBias/EvalData/list_output/chatgpt_semeval2017_7Apr.csv\", \"w\") as file:\n",
        "    csvwriter = csv.writer(file)\n",
        "    csvwriter.writerow([\"id\", \"document\", \"keyphrases\", \"zs_python_answer\", \"zs_special_answer\", \"zs_bullet_answer\", \"zs_newline_prompt\"])\n",
        "    csvwriter.writerows(saved_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL4RCWmwzfeE"
      },
      "source": [
        "<h1> 2. CoT performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PHdWF5izSPV",
        "outputId": "a14f2d47-c2c6-49d1-e7c6-e5ae43443082"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "200it [59:32, 17.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python_fi: 0.3939393939393939\n",
            "special_fi: 0.9797979797979798\n",
            "bullet_fi: 0.9494949494949495\n",
            "newline_fi: 0.9949494949494949\n",
            "===\n",
            "python_F1: 0.06044828626254504\n",
            "special_F1: 0.16131334618475082\n",
            "bullet_F1: 0.15513818576317287\n",
            "newline_F1: 0.22757419412293223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Zero-shot performance\n",
        "\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "import yaml\n",
        "import ast\n",
        "\n",
        "python_fi = 0\n",
        "special_fi = 0\n",
        "bullet_fi = 0\n",
        "newline_fi = 0\n",
        "\n",
        "python_F1 = 0\n",
        "special_F1 = 0\n",
        "bullet_F1 = 0\n",
        "newline_F1 = 0\n",
        "\n",
        "all_cnt = 0\n",
        "\n",
        "saved_data = []\n",
        "with open(\"/content/drive/MyDrive/NUS/PhD/Prof.Kenji/FormatBias/EvalData/list_keyphrases_SemEval2017_200.csv\") as file:\n",
        "    csvreader = csv.reader(file)\n",
        "    header = next(csvreader)\n",
        "    for row in tqdm(csvreader):\n",
        "        id = row[list(header).index(\"id\")]\n",
        "        document = row[list(header).index(\"document\")]\n",
        "        keyphrases = eval(row[list(header).index(\"keyphrases\")])\n",
        "\n",
        "        wrapping = \"Wrap your final list by <ANSWER> and </ANSWER>.\"\n",
        "\n",
        "        cot_python_prompt = f\"\"\"Extract a list of keyphrases from the following document:\n",
        "Document: {document}\n",
        "Generate your list in Python format. Generate your list step-by-step. {wrapping}\"\"\"\n",
        "\n",
        "        cot_bullet_prompt = f\"\"\"Extract a list of keyphrases from the following document:\n",
        "Document: {document}\n",
        "Generate your list using bullet points. Generate your list step-by-step. {wrapping}\"\"\"\n",
        "\n",
        "        cot_special_prompt = f\"\"\"Extract a list of keyphrases from the following document:\n",
        "Document: {document}\n",
        "Generate your list using <SEP> to seperate elements. Generate your list step-by-step. {wrapping}\"\"\"\n",
        "\n",
        "        cot_newline_prompt = f\"\"\"Extract a list of keyphrases from the following document:\n",
        "Document: {document}\n",
        "Generate your list such that each element is in a new line. Generate your list step-by-step. {wrapping}\"\"\"\n",
        "\n",
        "        cot_python_answer = get_chatgpt_answer(cot_python_prompt)\n",
        "        cot_special_answer = get_chatgpt_answer(cot_special_prompt)\n",
        "        cot_bullet_answer = get_chatgpt_answer(cot_bullet_prompt)\n",
        "        cot_newline_answer =  get_chatgpt_answer(cot_newline_prompt)\n",
        "\n",
        "        # print(cot_python_answer)\n",
        "        # print(cot_special_answer)\n",
        "        # print(cot_bullet_answer)\n",
        "        # print(cot_newline_answer)\n",
        "        # print(\"===\")\n",
        "\n",
        "        python_follow = check_python_follow_ranking(cot_python_answer, cot=True)\n",
        "        if python_follow:\n",
        "            try:\n",
        "                python_fi += 1\n",
        "                extract_python = extract_python_ranking(cot_python_answer, cot=True)\n",
        "                python_F1 += compute_f1(extract_python, keyphrases)\n",
        "            except: continue\n",
        "\n",
        "        special_follow = check_special_character_follow_ranking(cot_special_answer, cot=True)\n",
        "        if special_follow:\n",
        "            try:\n",
        "                special_fi += 1\n",
        "                extract_special = extract_special_character_ranking(cot_special_answer, cot=True)\n",
        "                special_F1 += compute_f1(extract_special, keyphrases)\n",
        "            except: continue\n",
        "\n",
        "        bullet_follow = check_bullet_follow_ranking(cot_bullet_answer, cot=True)\n",
        "        if bullet_follow:\n",
        "            try:\n",
        "                extract_bullet = extract_bullet_ranking(cot_bullet_answer, cot=True)\n",
        "                bullet_F1 += compute_f1(extract_bullet, keyphrases)\n",
        "                bullet_fi += 1\n",
        "            except: continue\n",
        "\n",
        "        newline_follow = check_newline_follow_ranking(cot_newline_answer, cot=True)\n",
        "        if newline_follow:\n",
        "            try:\n",
        "                newline_fi += 1\n",
        "                extract_newline = extract_newline_ranking(cot_newline_answer, cot=True)\n",
        "                newline_F1 += compute_f1(extract_newline, keyphrases)\n",
        "            except: continue\n",
        "\n",
        "        all_cnt += 1\n",
        "        saved_data.append([id, document, keyphrases, cot_python_answer, cot_special_answer, cot_bullet_answer, cot_newline_prompt])\n",
        "\n",
        "print(f\"python_fi: {python_fi/all_cnt}\")\n",
        "print(f\"special_fi: {special_fi/all_cnt}\")\n",
        "print(f\"bullet_fi: {bullet_fi/all_cnt}\")\n",
        "print(f\"newline_fi: {newline_fi/all_cnt}\")\n",
        "print(\"===\")\n",
        "print(f\"python_F1: {python_F1/all_cnt}\")\n",
        "print(f\"special_F1: {special_F1/all_cnt}\")\n",
        "print(f\"bullet_F1: {bullet_F1/all_cnt}\")\n",
        "print(f\"newline_F1: {newline_F1/all_cnt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeDWLoPK0qBV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
